#!/usr/bin/env python3

from torch_geometric.datasets import TUDataset
from torch_geometric.data import DataLoader
from graphprot2.MyNets import FunnelGNN, MyDataset
from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit
from graphprot2 import model_util
from graphprot2 import gp2lib
import numpy as np
import statistics
import argparse
import random
import shutil
import torch
import copy
import sys
import os
import re

__version__ = "0.1"

"""

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~ OPEN FOR BUSINESS ~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


AuthoR: uhlm [at] informatik [dot] uni-freiburg [dot] de


~~~~~~~~~~~~~~~~~~~~~~~~~~
Check out available modes
~~~~~~~~~~~~~~~~~~~~~~~~~~

graphprot2 -h


"""


################################################################################

def setup_argument_parser():
    """Setup argparse parser."""
    # Tool description text.
    help_description = """

    Modelling RBP binding preferences to predict RPB binding sites.

    """

    # Define argument parser.
    p = argparse.ArgumentParser(#add_help=False,
                                prog="graphprot2",
                                description=help_description)

    # Tool version.
    p.add_argument("-v", "--version", action="version",
                   version="graphprot2 v" + __version__)

    # Add subparsers.
    subparsers = p.add_subparsers(help='Program modes')

    """
    Model training mode.
    """
    p_tr = subparsers.add_parser('train',
                                  help='Train a binding site prediction model')
    p_tr.set_defaults(which='train')

    # Add required arguments group.
    p_trm = p_tr.add_argument_group("required arguments")
    # Required arguments for train.
    p_trm.add_argument("--in",
                   dest="in_folder",
                   type=str,
                   required = True,
                   help = "Input training data folder (output of graphprot2 gt)")
    p_trm.add_argument("--out",
                   dest="out_folder",
                   type=str,
                   required = True,
                   help = "Model training results output folder")

    # Add feature usage arguments group.
    p_trf = p_tr.add_argument_group("feature definition arguments")
    p_trf.add_argument("--only-seq",
                   dest = "only_seq",
                   default = False,
                   action = "store_true",
                   help = "Use only sequence feature. By default all features present in --in are used as node attributes (default: False)")
    p_trf.add_argument("--use-phastcons",
                   dest = "use_pc_con",
                   default = False,
                   action = "store_true",
                   help = "Add phastCons conservation scores. Set --use-x to define which features to add on top of sequence feature (by default all --in features are used)")
    p_trf.add_argument("--use-phylop",
                   dest = "use_pp_con",
                   default = False,
                   action = "store_true",
                   help = "Add phyloP conservation scores. Set --use-x to define which features to add on top of sequence feature (by default all --in features are used)")
    p_trf.add_argument("--use-eia",
                   dest = "use_eia",
                   default = False,
                   action = "store_true",
                   help = "Add exon-intron annotations. Set --use-x to define which features to add on top of sequence feature (by default all --in features are used)")
    p_trf.add_argument("--use-tra",
                   dest = "use_tra",
                   default = False,
                   action = "store_true",
                   help = "Add transcript region annotations. Set --use-x to define which features to add on top of sequence feature (by default all --in features are used)")
    p_trf.add_argument("--use-rra",
                   dest = "use_rra",
                   default = False,
                   action = "store_true",
                   help = "Add repeat region annotations. Set --use-x to define which features to add on top of sequence feature (by default all --in features are used)")
    p_trf.add_argument("--use-str-elem-p",
                   dest = "use_str_elem_p",
                   default = False,
                   action = "store_true",
                   help = "Add structural elements probabilities. Set --use-x to define which features to add on top of sequence feature (by default all --in features are used)")
    p_trf.add_argument("--use-bps",
                   dest = "use_bps",
                   default = False,
                   action = "store_true",
                   help = "Add base pairs to graph. Set --use-x to define which features to add on top of sequence feature (by default all --in features are used)")
    p_trf.add_argument("--bps-mode",
                   dest = "bps_mode",
                   type = int,
                   default = 1,
                   choices = [1,2],
                   help="Defines which base pairs are added to the graphs. --bpp-mode 1 : base pairs with start or end in viewpoint region. --bpp-mode 2 : only base pairs with start+end in viewpoint (default: 1)")
    p_trf.add_argument("--bps-prob-cutoff",
                   dest = "bps_cutoff",
                   type = float,
                   metavar='float',
                   default = 0.5,
                   help = "Base pair probability cutoff for filtering base pairs added to the graph (default: 0.5)")
    p_trf.add_argument("--uc-context",
                   dest = "uc_context",
                   default = False,
                   action = "store_true",
                   help = "Convert lowercase context (if present, added by graphprot2 gt --con-ext) to uppercase (default: False)")

    # Add advanced model definition arguments group.
    p_tra = p_tr.add_argument_group("model definition arguments")
    # Optional arguments for train.
    p_tra.add_argument("--gen-cv",
                   dest = "gen_cv",
                   default = False,
                   action = "store_true",
                   help = "Run cross validation in combination with hyperparameter optimization to evaluate generalization performance (default: False)")
    p_tra.add_argument("--gen-cv-k",
                   dest="gen_cv_k",
                   type = int,
                   default = 10,
                   choices = [5,10],
                   help = "Cross validation k for evaluating generalization performance (default: 10)")
    p_tra.add_argument("--gm-cv",
                   dest = "gm_cv",
                   default = False,
                   action = "store_true",
                   help = "Treat data as generic model data (positive IDs with specific format required). This turns on generic model data cross validation, with every fold leaving one RBP set out for testing (ignoring --gen-cv and --gen-cv-k) (default: False)")
    p_tra.add_argument("--train-cv",
                   dest = "train_cv",
                   default = False,
                   action = "store_true",
                   help = "Run cross validation to train final model, with hyperparameter optimization in each split and selection of best parameters based their on average performance on validation sets. By default final model training is done for one split only (validation set size controlled by --train-vs). Note that --train-cv with many hyperparameter combinations considerably increases run time (default: False)")
    p_tra.add_argument("--train-cv-k",
                   dest="train_cv_k",
                   type = int,
                   default = 5,
                   choices = [5,10],
                   help = "Final model cross validation k. Use in combination with --train-cv (default: 5)")
    p_tra.add_argument("--train-vs",
                   dest = "train_vs",
                   type = float,
                   metavar='float',
                   default = 0.2,
                   help = "Validation set size for training final model as percentage of all training sites. Only effective if --train-cv not set (with --train-cv validation set size controlled by --train-cv-k) (default: 0.2)")
    p_tra.add_argument("--batch-size",
                   dest="list_batch_size",
                   type = int,
                   metavar='int',
                   nargs='+',
                   default=[50],
                   help = "List of gradient descent batch sizes (default: 50)")
    p_tra.add_argument("--epochs",
                   dest="epochs",
                   type = int,
                   metavar='int',
                   default = 200,
                   help = "Number of training epochs (default: 200)")
    p_tra.add_argument("--patience",
                   dest="patience",
                   type = int,
                   metavar='int',
                   default = 30,
                   help = "Number of epochs to wait for further improvement on validation set before stopping (default: 30)")
    p_tra.add_argument("--fc-hidden-dim",
                   dest="fc_hidden_dim",
                   type = int,
                   metavar='int',
                   default = 128,
                   help = "Number of dimensions for fully connected layers (default: 128)")
    p_tra.add_argument("--list-lr",
                   dest="list_lr",
                   type=float,
                   metavar='float',
                   nargs='+',
                   default=[0.0001],
                   help="List of learning rates for hyperparameter optimization (default: 0.0001)")
    p_tra.add_argument("--list-hidden-dim",
                   dest="list_node_hidden_dim",
                   type=int,
                   metavar='int',
                   nargs='+',
                   default=[128],
                   help="List of node feature dimensions in hidden layers for hyperparameter optimization (default: 128)")
    p_tra.add_argument("--list-weight-decay",
                   dest="list_weight_decay",
                   type=float,
                   metavar='float',
                   nargs='+',
                   default=[0.0001],
                   help="List of weight decays for hyperparameter optimization (default: 0.0001)")

    """
    Model evaluation mode.
    """
    p_ev = subparsers.add_parser('eval',
                                  help='Evaluate properties learned from positive sites')
    p_ev.set_defaults(which='eval')

    # Add required arguments group.
    p_evm = p_ev.add_argument_group("required arguments")
    # Required arguments for eval.
    p_evm.add_argument("--in",
                   dest="in_folder",
                   type=str,
                   required = True,
                   help = "Input model training folder (output of graphprot2 train)")
    p_evm.add_argument("--out",
                   dest="out_folder",
                   type=str,
                   required = True,
                   help = "Evaluation results output folder")

    # Optional arguments for eval.
    p_ev.add_argument("--nr-top-sites",
                   dest="list_nr_top_sites",
                   type = int,
                   nargs='+',
                   default = [500],
                   help = "Specify number(s) of top predicted sites used for motif extraction. Provide multiple numbers (e.g. --nr-top-sites 100 500 1000) to extract one motif plot from each site set (default: 500)")
    p_ev.add_argument("--nr-top-profiles",
                   dest="nr_top_profiles",
                   type = int,
                   default = 25,
                   metavar='int',
                   help = "Specify number of top predicted sites to plot profiles for (default: 25)")
    p_ev.add_argument("--motif-size",
                   dest="list_motif_sizes",
                   type = int,
                   nargs='+',
                   default = [7],
                   help = "Motif size(s) (widths) for extracting and plotting motifs. Provide multiple sizes (e.g. --motif-size 5 7 9) to extract a motif for each size (default: 7)")
    p_ev.add_argument("--motif-sc-thr",
                   dest="motif_sc_thr",
                   type = float,
                   metavar='float',
                   default = 0.3,
                   help = "Minimum profile score of position to be included in motif (default: 0.3)")
    p_ev.add_argument("--win-size",
                   dest="list_win_sizes",
                   type = int,
                   nargs='+',
                   default = [7],
                   help = "Windows size(s) for calculating position-wise scoring profiles. Provide multiple sizes (e.g. --win-size 5 7 9) to compute average profiles (default: 7)")
    p_ev.add_argument("--plot-format",
                   dest="plot_format",
                   type = int,
                   default = 1,
                   choices = [1,2],
                   help = "Plotting format. 1: png, 2: pdf (default: 1)")

    """
    Binding site prediction mode.
    """
    p_pr = subparsers.add_parser('predict',
                                  help='Predict binding sites (whole sites or profiles)')
    p_pr.set_defaults(which='predict')

    # Add required arguments group.
    p_prm = p_pr.add_argument_group("required arguments")
    # Required arguments for predict.
    p_prm.add_argument("--in",
                   dest="in_folder",
                   type=str,
                   required = True,
                   help = "Input prediction data folder (output of graphprot2 gp)")
    p_prm.add_argument("--model-in",
                   dest="model_in_folder",
                   type=str,
                   required = True,
                   help = "Input model training folder containing model file and parameters (output of graphprot2 train)")
    p_prm.add_argument("--out",
                   dest="out_folder",
                   type=str,
                   metavar='str',
                   required = True,
                   help = "Prediction results output folder")

    # Optional arguments for predict.
    p_pr.add_argument("--mode",
                   dest="mode",
                   type = int,
                   default = 1,
                   choices = [1,2],
                   help = "Define prediction mode. (1) predict whole sites, (2) predict position-wise scoring profiles and extract top-scoring sites from profiles (default: 1)")
    p_pr.add_argument("--win-size",
                   dest="list_win_sizes",
                   type = int,
                   nargs='+',
                   default = [11],
                   help = "Windows size(s) for calculating position-wise scoring profiles. Provide multiple sizes (e.g. --win-size 5 7 9) to compute average profiles (default: 11)")
    p_pr.add_argument("--peak-ext",
                   dest = "peak_ext",
                   type = int,
                   metavar='int',
                   default = 30,
                   help = "Up- and downstream peak position extension for extracting top-scoring sites from fixed-window profiles (default: 30)")
    p_pr.add_argument("--con-ext",
                   dest = "con_ext",
                   type = int,
                   metavar='int',
                   default = False,
                   help = "Up- and downstream context extension for extracting top-scoring sites from fixed-window profiles. By default uses --con-ext info from --model-in (if set in graphprot2 train), but restricts it to a maximum of 50 (default: False)")
    p_pr.add_argument("--thr",
                   dest="sc_thr",
                   type = float,
                   metavar='float',
                   default = 0.5,
                   help = "Minimum profile position score for extracting peak regions and top-scoring sites. Further increase e.g. in case of too many or too broad peaks (default: 0.5)")
    p_pr.add_argument("--max-merge-dist",
                   dest = "max_merge_dist",
                   type = int,
                   metavar='int',
                   default = 0,
                   help = "Maximum distance between two peaks for merging. Two peakse get merged to one if they are <= --max-merge-dist away from each other (default: 0)")

    """
    Training set generation mode.
    """
    p_gt = subparsers.add_parser('gt',
                                  help='Generate training data set')
    p_gt.set_defaults(which='gt')

    # Add required arguments group.
    p_gtm = p_gt.add_argument_group("required arguments")
    # Required arguments for gt.
    p_gtm.add_argument("--in",
                   dest="in_sites",
                   type=str,
                   metavar='str',
                   required = True,
                   help = "Genomic or transcript RBP binding sites file in BED (6-column format) or FASTA format. If --in FASTA, only --str is supported as additional feature. If --in BED, --gtf and --gen become mandatory")
    p_gtm.add_argument("--out",
                   dest="out_folder",
                   type=str,
                   metavar='str',
                   required = True,
                   help = "Output training data folder (== input folder to graphprot2 train)")

    # Optional arguments for gt.
    p_gt.add_argument("--gtf",
                   dest="in_gtf",
                   type=str,
                   metavar='str',
                   help = "Genomic annotations GTF file (.gtf or .gtf.gz)")
    p_gt.add_argument("--gen",
                   dest="in_2bit",
                   type=str,
                   metavar='str',
                   help = "Genomic sequences .2bit file")
    p_gt.add_argument("--mode",
                   dest="mode",
                   type = int,
                   default = 1,
                   choices = [1,2,3],
                   help = "Define mode for --in BED site extraction. (1) Take the center of each site, (2) Take the complete site, (3) Take the upstream end for each site. Note that --min-len applies only for --mode 2 (default: 1)")
    p_gt.add_argument("--mask-bed",
                   dest="mask_bed",
                   type=str,
                   metavar='str',
                   help = "Additional BED regions file (6-column format) for masking negatives (e.g. all positive RBP CLIP sites)")
    p_gt.add_argument("--seq-ext",
                   dest = "seq_ext",
                   type = int,
                   metavar='int',
                   default = 30,
                   help = "Up- and downstream sequence extension of sites (site definition by --mode) with uppercase characters (default: 30)")
    p_gt.add_argument("--con-ext",
                   dest = "con_ext",
                   type = int,
                   metavar='int',
                   default = False,
                   help = "Up- and downstream context sequence extension of sites (site definition by --mode) with lowercase characters. Note that statistics (--report) are produced only for uppercase sequence parts (defined by --seq-ext) (default: False)")
    p_gt.add_argument("--thr",
                   dest="sc_thr",
                   type = float,
                   metavar='float',
                   default = None,
                   help = "Minimum site score (--in BED column 5) for filtering (assuming higher score == better site) (default: None)")
    p_gt.add_argument("--rev-filter",
                   dest="rev_filter",
                   default = False,
                   action = "store_true",
                   help = "Reverse --thr filtering (i.e. the lower the better, e.g. for p-values) (default: False)")
    p_gt.add_argument("--max-len",
                   dest = "max_len",
                   type = int,
                   metavar='int',
                   default = 300,
                   help = "Maximum length of --in sites (default: 300)")
    p_gt.add_argument("--min-len",
                   dest = "min_len",
                   type = int,
                   metavar='int',
                   default = 21,
                   help = "Minimum length of --in sites (only effective for --mode 2). If length < --min-len, take center and extend to --min-len. Use uneven numbers for equal up- and downstream extension (default: 21)")
    p_gt.add_argument("--keep-ids",
                   dest="keep_ids",
                   default = False,
                   action = "store_true",
                   help = "Keep --in BED column 4 site IDs. Note that site IDs have to be unique (default: False)")
    p_gt.add_argument("--allow-overlaps",
                   dest="allow_overlaps",
                   default = False,
                   action = "store_true",
                   help = "Do not select for highest-scoring sites in case of overlapping sites (default: False)")
    p_gt.add_argument("--no-gene-filter",
                   dest="no_gene_filter",
                   default = False,
                   action = "store_true",
                   help = "Do not filter positives based on gene coverage (gene annotations from --gtf) (default: False)")
    p_gt.add_argument("--con-ext-pre",
                   dest="pre_con_ext_merge",
                   default = False,
                   action = "store_true",
                   help = "Add --con-ext extension before selecting for highest-scoring sites in case of overlaps (not afterwards) (default: False)")
    p_gt.add_argument("--neg-comp-thr",
                   dest="neg_comp_thr",
                   type = float,
                   metavar='float',
                   default = 0.5,
                   help = "Sequence complexity (Shannon entropy) threshold for filtering random negative regions (default: 0.5)")
    p_gt.add_argument("--neg-factor",
                   dest="neg_factor",
                   type = int,
                   default = 2,
                   choices = [2,3,4,5],
                   help = "Determines number of initial random negatives to be extracted (== --neg-factor n times # positives) (default: 2)")
    p_gt.add_argument("--keep-add-neg",
                   dest="keep_add_neg",
                   default = False,
                   action = "store_true",
                   help = "Keep additional negatives (# controlled by --neg-factor) instead of outputting same numbers of positive and negative sites (default: False)")
    p_gt.add_argument("--neg-in",
                   dest="in_neg_sites",
                   type=str,
                   metavar='str',
                   help = "Negative genomic or transcript sites in BED (6-column format) or FASTA format (unique IDs required). Use with --in BED/FASTA. If not set, negatives are generated by shuffling --in sequences (if --in FASTA) or random selection of genomic or transcript sites (if --in BED)")
    p_gt.add_argument("--shuffle-k",
                   dest="shuffle_k",
                   type = int,
                   default = 2,
                   choices = [1,2,3],
                   help = "Supply k for k-nucleotide shuffling of --in sequences to generate negative sequences (if no --neg-fa supplied) (default: 2)")
    p_gt.add_argument("--report",
                   dest="report",
                   default = False,
                   action = "store_true",
                   help = "Output an .html report providing various training set statistics and plots (default: False)")
    p_gt.add_argument("--theme",
                   dest="theme",
                   type = int,
                   default = 1,
                   choices = [1,2],
                   help = "Set theme for .html report (1: default, 2: midnight blue) (default: 1)")

    # Add additional feature arguments group.
    p_gta = p_gt.add_argument_group("additional annotation arguments")
    p_gta.add_argument("--eia",
                   dest="exon_intron_annot",
                   default = False,
                   action = "store_true",
                   help = "Add exon-intron annotations to genomic regions (default: False)")
    p_gta.add_argument("--eia-ib",
                   dest="intron_border_annot",
                   default = False,
                   action = "store_true",
                   help = "Add intron border annotations to genomic regions (in combination with --exon-intron) (default: False)")
    p_gta.add_argument("--eia-n",
                   dest="exon_intron_n",
                   default = False,
                   action = "store_true",
                   help = "Label regions not covered by intron or exon regions as N instead of labelling them as introns (I) (in combination with --exon-intron) (default: False)")
    p_gta.add_argument("--tr-list",
                   dest="tr_list",
                   type=str,
                   metavar='str',
                   help = "Supply file with transcript IDs (one ID per row) for exon intron labeling (using the corresponding exon regions from --gtf). By default, exon regions of the most prominent transcripts (automatically selected from --gtf) are used (default: False)")
    p_gta.add_argument("--phastcons",
                   dest="pc_bw",
                   type=str,
                   metavar='str',
                   help = "Genomic .bigWig file with phastCons conservation scores to add as annotations")
    p_gta.add_argument("--phylop",
                   dest="pp_bw",
                   type=str,
                   metavar='str',
                   help = "Genomic .bigWig file with phyloP conservation scores to add as annotations")
    p_gta.add_argument("--tra",
                   dest="tr_reg_annot",
                   default = False,
                   action = "store_true",
                   help = "Add transcript region annotations (5'UTR, CDS, 3'UTR, None) to genomic and transcript regions (default: False)")
    p_gta.add_argument("--tra-codons",
                   dest="tr_reg_codon_annot",
                   default = False,
                   action = "store_true",
                   help = "Add start and stop codon annotations to genomic or transcript regions (in combination with --tra) (default: False)")
    p_gta.add_argument("--tra-borders",
                   dest="tr_reg_border_annot",
                   default = False,
                   action = "store_true",
                   help = "Add transcript and exon border annotations to transcript regions (in combination with --tra) (default: False)")
    p_gta.add_argument("--rra",
                   dest="rep_reg_annot",
                   default = False,
                   action = "store_true",
                   help = "Add repeat region annotations for genomic or transcript regions retrieved from --gen .2bit (default: False)")
    p_gta.add_argument("--str",
                   dest="add_str",
                   default = False,
                   action = "store_true",
                   help = "Add base pairs and position-wise structural elements probabilities features (calculate with RNAplfold) (default: False)")
    p_gta.add_argument("--bp-in",
                   dest="bp_in",
                   default = False,
                   action = "store_true",
                   help = "Supply a custom base pair annotation file for all --in sites. This disables base pair calculation for the positive set (default: False)")
    p_gta.add_argument("--plfold-u",
                   dest="plfold_u",
                   type = int,
                   metavar='int',
                   default = 3,
                   help = "RNAplfold -u parameter value (default: 3)")
    p_gta.add_argument("--plfold-l",
                   dest="plfold_l",
                   type = int,
                   metavar='int',
                   default = 100,
                   help = "RNAplfold -L parameter value (default: 100)")
    p_gta.add_argument("--plfold-w",
                   dest="plfold_w",
                   type = int,
                   metavar='int',
                   default = 150,
                   help = "RNAplfold -W parameter value (default: 150)")

    """
    Test / prediction set generation mode.
    """
    p_gp = subparsers.add_parser('gp',
                                  help='Generate prediction data set')
    p_gp.set_defaults(which='gp')

    # Add required arguments group.
    p_gpm = p_gp.add_argument_group("required arguments")
    # Required arguments for gp.
    p_gpm.add_argument("--in",
                   dest="in_sites",
                   type=str,
                   metavar='str',
                   required = True,
                   help = "Genomic or transcript RBP binding sites file in BED (6-column format) or FASTA format. If --in FASTA, only --str is supported as additional feature. If --in BED, --gtf and --gen become mandatory")
    p_gpm.add_argument("--out",
                   dest="out_folder",
                   type=str,
                   metavar='str',
                   required = True,
                   help = "Output prediction dataset folder (== input folder to graphprot2 predict)")

    # Optional arguments for gp.
    p_gp.add_argument("--gtf",
                   dest="in_gtf",
                   type=str,
                   metavar='str',
                   help = "Genomic annotations GTF file (.gtf or .gtf.gz)")
    p_gp.add_argument("--gen",
                   dest="in_2bit",
                   type=str,
                   metavar='str',
                   help = "Genomic sequences .2bit file")
    p_gp.add_argument("--keep-ids",
                   dest="keep_ids",
                   default = False,
                   action = "store_true",
                   help = "Keep --in BED column 4 site IDs. Note that site IDs have to be unique (default: False)")
    p_gp.add_argument("--gene-filter",
                   dest="gene_filter",
                   default = False,
                   action = "store_true",
                   help = "Filter --in sites based on gene coverage (gene annotations from --gtf) (default: False)")
    p_gp.add_argument("--con-ext",
                   dest = "con_ext",
                   type = int,
                   metavar='int',
                   default = False,
                   help = "Up- and downstream context sequence extension of --in sites with lowercase characters for whole site prediction (graphprot predict --mode 1). Best use same --con-ext values in gp+gt+train modes. Note that statistics (--report) are produced only for uppercase sequence parts (default: False)")
    p_gp.add_argument("--report",
                   dest="report",
                   default = False,
                   action = "store_true",
                   help = "Output an .html report providing various training set statistics and plots (default: False)")
    p_gp.add_argument("--theme",
                   dest="theme",
                   type = int,
                   default = 1,
                   choices = [1,2],
                   help = "Set theme for .html report (1: default, 2: midnight blue) (default: 1)")

    # Add additional feature arguments group.
    p_gpa = p_gp.add_argument_group("additional annotation arguments")
    p_gpa.add_argument("--eia",
                   dest="exon_intron_annot",
                   default = False,
                   action = "store_true",
                   help = "Add exon-intron annotations to genomic regions (default: False)")
    p_gpa.add_argument("--eia-ib",
                   dest="intron_border_annot",
                   default = False,
                   action = "store_true",
                   help = "Add intron border annotations to genomic regions (in combination with --exon-intron) (default: False)")
    p_gpa.add_argument("--eia-n",
                   dest="exon_intron_n",
                   default = False,
                   action = "store_true",
                   help = "Label regions not covered by intron or exon regions as N instead of labelling them as introns (I) (in combination with --exon-intron) (default: False)")
    p_gpa.add_argument("--tr-list",
                   dest="tr_list",
                   type=str,
                   metavar='str',
                   help = "Supply file with transcript IDs (one ID per row) for exon intron labeling (using the corresponding exon regions from --gtf). By default, exon regions of the most prominent transcripts (automatically selected from --gtf) are used (default: False)")
    p_gpa.add_argument("--phastcons",
                   dest="pc_bw",
                   type=str,
                   metavar='str',
                   help = "Genomic .bigWig file with phastCons conservation scores to add as annotations")
    p_gpa.add_argument("--phylop",
                   dest="pp_bw",
                   type=str,
                   metavar='str',
                   help = "Genomic .bigWig file with phyloP conservation scores to add as annotations")
    p_gpa.add_argument("--tra",
                   dest="tr_reg_annot",
                   default = False,
                   action = "store_true",
                   help = "Add transcript region annotations (5'UTR, CDS, 3'UTR, None) to genomic and transcript regions (default: False)")
    p_gpa.add_argument("--tra-codons",
                   dest="tr_reg_codon_annot",
                   default = False,
                   action = "store_true",
                   help = "Add start and stop codon annotations to genomic or transcript regions (in combination with --tra) (default: False)")
    p_gpa.add_argument("--tra-borders",
                   dest="tr_reg_border_annot",
                   default = False,
                   action = "store_true",
                   help = "Add transcript and exon border annotations to transcript regions (in combination with --tra) (default: False)")
    p_gpa.add_argument("--rra",
                   dest="rep_reg_annot",
                   default = False,
                   action = "store_true",
                   help = "Add repeat region annotations for genomic or transcript regions retrieved from --gen .2bit (default: False)")
    p_gpa.add_argument("--str",
                   dest="add_str",
                   default = False,
                   action = "store_true",
                   help = "Add base pairs and position-wise structural elements probabilities features (calculate with RNAplfold) (default: False)")
    p_gpa.add_argument("--bp-in",
                   dest="bp_in",
                   type=str,
                   metavar='str',
                   help = "Supply a custom base pair annotation file for all --in sites, disabling base pair calculation with RNAplfold (default: False)")
    p_gpa.add_argument("--plfold-u",
                   dest="plfold_u",
                   type = int,
                   metavar='int',
                   default = 3,
                   help = "RNAplfold -u parameter value (default: 3)")
    p_gpa.add_argument("--plfold-l",
                   dest="plfold_l",
                   type = int,
                   metavar='int',
                   default = 100,
                   help = "RNAplfold -L parameter value (default: 100)")
    p_gpa.add_argument("--plfold-w",
                   dest="plfold_w",
                   type = int,
                   metavar='int',
                   default = 150,
                   help = "RNAplfold -W parameter value (default: 150)")
    return p


################################################################################

def main_train(args):
    """
    Train a graphprot2 model.

    """

    print("Running for you in TRAIN mode ... ")

    # Checks.
    assert os.path.exists(args.in_folder), "--in folder does not exist"
    indiv_feat_check = args.use_pc_con + args.use_pp_con + args.use_eia + args.use_tra + args.use_rra + args.use_str_elem_p + args.use_bps
    if indiv_feat_check and args.only_seq:
        assert False, "--only-seq useless in combination with individually selected features (--use-xxx)"
    assert args.train_vs > 0.1 and args.train_vs <= 0.5, "Use reasonable values for --train-vs (> 0.1 and <= 0.5)"

    if args.gm_cv and args.gen_cv:
        assert False, "use either --gen-cv to estimate single model generalization performance, or --gm-cv to estimate generic model generalization performance"

    # Generate results output folder.
    out_folder = args.out_folder
    if not os.path.exists(out_folder):
        os.makedirs(out_folder)

    # PTG internal data ID, also name of data subfolder.
    data_id = "data"
    in_folder = args.in_folder
    data_out_folder = args.out_folder + "/" + data_id
    if not os.path.exists(data_out_folder):
        os.makedirs(data_out_folder)

    raw_out_folder = data_out_folder + "/raw"
    proc_out_folder = data_out_folder + "/processed"
    if not os.path.exists(raw_out_folder):
        os.makedirs(raw_out_folder)
    if not os.path.exists(proc_out_folder):
        os.makedirs(proc_out_folder)

    print("Training set input folder:     %s" %(in_folder))
    print("Model training output folder:  %s" %(out_folder))

    # Output mode settings.
    settings_file = out_folder + "/settings.graphprot2_train.out"
    SETOUT = open(settings_file, "w")
    for arg in vars(args):
        SETOUT.write("%s\t%s\n" %(arg, str(getattr(args, arg))))
    SETOUT.close()

    # Class label to RBP label dictionary.
    li2label_dic = {}

    # Load data and store on disk in PyTorch geometric format.
    gp2lib.load_geo_training_data(args,
                                  data_id=data_id,
                                  li2label_dic=li2label_dic,
                                  add_info_out=True)

    # Use node attributes?
    use_node_attr = True
    ana_file = raw_out_folder + "/" + data_id + "_node_attributes.txt"
    if args.only_seq or not os.path.exists(ana_file):
        use_node_attr = False
    if not use_node_attr:
        print("Node attributes disabled ... ")

    # Check for CUDA support.
    if torch.cuda.is_available():
        print("CUDA: I'm available. Using GPU ... ")
    else:
        print("CUDA: I'm NOT available. Using CPU ... ")

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    #current_path = os.getcwd()
    #geometric_cv_data_path = current_path + "/data/geometric_cv/" + args.dataset
    #processed_file = geometric_cv_data_path + '/processed/data.pt'
    geometric_cv_data_path = data_out_folder

    print("Load PyG data again ... ")
    print("CALEB: your bad day just got worse ... ")

    dataset = TUDataset(geometric_cv_data_path, name=data_id, use_node_attr=use_node_attr)
    assert len(dataset), "read-in dataset empty"
    print("# dataset features:  %i" %(dataset.num_features))
    print("Dataset size:        %i" %(len(dataset)))

    random.seed(1)
    # Get graphs and labels.
    graphs = [dataset[idx] for idx in range(dataset.__len__())]
    random.shuffle(graphs)
    labels = [g.y for g in graphs]

    # Hyperparameters.
    #args.list_lr = [0.001, 0.0001]
    #args.list_node_hidden_dim = [96, 128]
    #args.list_weight_decay = [0.0001, 0.00001]
    #args.list_lr = [0.0001]
    #args.list_node_hidden_dim = [128]
    #args.list_weight_decay = [0.00001]

    # Selected hyperparameters.
    #sel_lr = args.list_lr[0]
    #sel_node_hidden_dim = args.list_node_hidden_dim[0]
    #sel_weight_decay = args.list_weight_decay[0]

    # Get HPO combinations.
    c_lr = len(args.list_lr)
    c_nhd = len(args.list_node_hidden_dim)
    c_wd = len(args.list_weight_decay)
    c_bs = len(args.list_batch_size)

    # Number of HP combinations.
    nr_hp_comb = c_lr*c_nhd*c_wd*c_bs
    run_hpo = True
    print("Number of hyperparameter (HP) combinations:  %i" %(nr_hp_comb))
    if nr_hp_comb == 1:
        run_hpo = False
        print("# HP combinations to test == 1. No HP optimization needed ...")


    """
    Run cross validation (--gen-cv, --gen-cv-k) to estimate generalization
    performance.

    """

    if args.gen_cv:

        print("Running CV to estimate generalization performance (--gen-cv) ... ")

        # Path to store CV models.
        model_folder = out_folder + "/cv_models/"
        if not os.path.exists(model_folder):
            os.makedirs(model_folder)
        # CV results (accuracies) file.
        cv_results_file = out_folder + "/cv_results.out"
        if os.path.exists(cv_results_file):
            os.remove(cv_results_file)

        # Chosen hyperparameters + CV results per fold output file.
        opt_cv_hps_file = out_folder + "/cv_opt_hp.out"
        OPTHP = open(opt_cv_hps_file, "w")
        OPTHP.write("fold\topt_batch_size\topt_node_hidden_dim\topt_weight_decay\topt_lr\tacc\n")

        # Split datasets into folds.
        skf = StratifiedKFold(n_splits=args.gen_cv_k, shuffle=False)
        acc_list = []

        i_cv = 0

        # Run --gen-cv-k fold cross validation with hyperparameter optimization.
        for list_tr_idx, list_te_idx in skf.split(np.zeros(len(labels)), labels):

            i_cv += 1
            print("Starting --gen-cv fold %i ... " %(i_cv))

            test_dataset = MyDataset([graphs[idx] for idx in list_te_idx])
            # Further split up train portion into train + validation.
            # Training set == all list entries except last length(test_set).
            train_dataset = MyDataset([graphs[idx] for idx in list_tr_idx[:-len(list_te_idx)]])
            # Validation set == last length(test_set) entries.
            val_dataset = MyDataset([graphs[idx] for idx in list_tr_idx[-len(list_te_idx):]])

            # Report set sizes.
            c_train = len(list_tr_idx) - len(list_te_idx)
            c_test = len(list_te_idx)
            c_val = len(list_te_idx)
            print("# training instances:    %i" %(c_train))
            print("# validation instances:  %i" %(c_val))
            print("# test instances:        %i" %(c_test))

            # Get optimal parameters for this CV split.
            opt_dic = model_util.select_model(args, dataset.num_features, train_dataset,
                                              val_dataset, model_folder, device)

            opt_batch_size = opt_dic["opt_batch_size"]
            opt_node_hidden_dim = opt_dic["opt_node_hidden_dim"]
            opt_weight_decay = opt_dic["opt_weight_decay"]
            opt_lr = opt_dic["opt_lr"]
            opt_acc = opt_dic["opt_acc"]
            opt_epochs = opt_dic["opt_epochs"]

            model = FunnelGNN(input_dim=dataset.num_features, node_hidden_dim=opt_node_hidden_dim, fc_hidden_dim=args.fc_hidden_dim, out_dim=2).to(device)
            model_path = model_folder + "/" + str(opt_batch_size) + "_" + str(opt_node_hidden_dim) + "_" + str(opt_weight_decay) + "_" + str(opt_lr)
            model.load_state_dict(torch.load(model_path))

            # Get test error of optimized model.
            test_loader = DataLoader(test_dataset, batch_size=opt_batch_size, shuffle=False)
            loss, acc = model_util.test(test_loader, device, model)
            # Store test accuracy.
            acc_list.append(acc)
            with open(cv_results_file, 'a+') as f:
                f.write(str(acc) + '\n')
            print("Selected model accuracy: %f" %(acc))

            # Output model state dictionary and optimized hyperparameters.
            OPTHP.write("%i\t%s\t%s\t%s\t%s\t%s\n" %(i_cv, str(opt_batch_size), str(opt_node_hidden_dim), str(opt_weight_decay), str(opt_lr), str(acc) ))

        OPTHP.close()

        # Report average accuracy with stdev.
        avg_acc = statistics.mean(acc_list)
        stdev_acc = statistics.stdev(acc_list)
        print("Average accuarcy + stdev (--gen-cv): %f (+- %f)" %(avg_acc, stdev_acc))
        avg_acc_out_file = args.out_folder + "/avg_accuracy.gen_cv.out"
        #AAOUT = open(avg_acc_out_file, "a+")
        AAOUT = open(avg_acc_out_file, "w")
        AAOUT.write("%f\t%f\n" %(avg_acc, stdev_acc))
        AAOUT.close()

    """
    Run cross validation to estimate generalzation performance of generic
    model.

    """
    if args.gm_cv:

        print("Run CV to estimate generic model generalization performance (--gm-cv) ... ")

        print("# of distinct RBP sets (== # of splits):  %i" %(len(li2label_dic)))

        # Path to store generic CV models.
        model_folder = out_folder + "/generic_cv_models/"
        if not os.path.exists(model_folder):
            os.makedirs(model_folder)
        # CV results (accuracies) file.
        cv_results_file = out_folder + "/generic_cv_results.out"
        if os.path.exists(cv_results_file):
            os.remove(cv_results_file)

        # Chosen hyperparameters + CV results per fold output file.
        opt_cv_hps_file = out_folder + "/generic_cv_opt_hp.out"
        OPTHP = open(opt_cv_hps_file, "w")
        OPTHP.write("fold\trbp\topt_batch_size\topt_node_hidden_dim\topt_weight_decay\topt_lr\tacc\n")

        acc_list = []
        i_cv = 0

        random.seed(1)
        all_positive_graphs = []
        all_negative_graphs = []
        all_positive_graphs_labels = []
        for g in graphs:
            if g.y.item() == 0:
                all_negative_graphs.append(g)
            else:
                all_positive_graphs.append(g)
                all_positive_graphs_labels.append(g.y)

        random.shuffle(all_negative_graphs)
        list_positive_graphs_labels = [l.item() for l in all_positive_graphs_labels]
        list_unique_positive_graphs_labels = list(set(list_positive_graphs_labels))

        for positive_graph_label in list_unique_positive_graphs_labels:
            all_positive_graphs_temp = copy.deepcopy(all_positive_graphs)

            train_graphs_positive = []
            test_graphs_positive = []
            for g in all_positive_graphs_temp:
                if g.y.item() == positive_graph_label:
                    test_graphs_positive.append(g)
                else:
                    train_graphs_positive.append(g)

            # Get RBP label.
            assert li2label_dic[positive_graph_label], "No RBP label for positive class label %i" %(positive_graph_label)
            rbp_label = li2label_dic[positive_graph_label]

            i_cv += 1
            print("Starting --gm-cv fold %i (test set = %s) ... " %(i_cv, rbp_label))

            n_test_graphs_positive = len(test_graphs_positive)
            for g in train_graphs_positive:
                g.y = torch.tensor([1])
            for g in test_graphs_positive:
                g.y = torch.tensor([1])

            train_graphs_negative = all_negative_graphs[:-n_test_graphs_positive]
            test_graphs_negative = all_negative_graphs[-n_test_graphs_positive:]

            all_train_graphs = train_graphs_positive + train_graphs_negative
            test_graphs = test_graphs_positive + test_graphs_negative

            random.shuffle(all_train_graphs)
            random.shuffle(test_graphs)

            n_test_graphs = len(test_graphs)
            train_graphs = all_train_graphs[:-n_test_graphs]
            val_graphs = all_train_graphs[-n_test_graphs:]

            test_dataset = MyDataset(test_graphs)
            train_dataset = MyDataset(train_graphs)
            val_dataset = MyDataset(val_graphs)

            # Get optimal HPs.
            opt_dic = model_util.select_model(args, dataset.num_features, train_dataset,
                                              val_dataset, model_folder, device)

            opt_batch_size = opt_dic["opt_batch_size"]
            opt_node_hidden_dim = opt_dic["opt_node_hidden_dim"]
            opt_weight_decay = opt_dic["opt_weight_decay"]
            opt_lr = opt_dic["opt_lr"]
            opt_acc = opt_dic["opt_acc"]
            opt_epochs = opt_dic["opt_epochs"]

            # Load optimized model.
            model = FunnelGNN(input_dim=dataset.num_features, node_hidden_dim=opt_node_hidden_dim,
                              fc_hidden_dim=args.fc_hidden_dim, out_dim=2).to(device)
            model_path = model_folder + "/" + str(opt_batch_size) + "_" + str(opt_node_hidden_dim) + "_" + str(opt_weight_decay) + "_" + str(opt_lr)
            model.load_state_dict(torch.load(model_path))

            # Get test error (left out RBP) of optimized model.
            test_loader = DataLoader(test_dataset, batch_size=opt_batch_size, shuffle=True)
            loss, acc = model_util.test(test_loader, device, model)
            # Store test accuracy.
            acc_list.append(acc)
            with open(cv_results_file, 'a+') as f:
                f.write(str(acc) + '\n')
            print("Generic model accuracy:  %f" %(acc))

            # Output model state dictionary and optimized hyperparameters.
            OPTHP.write("%i\t%s\t%s\t%s\t%s\t%s\t%s\n" %(i_cv, rbp_label, str(opt_batch_size), str(opt_node_hidden_dim), str(opt_weight_decay), str(opt_lr), str(acc) ))

        OPTHP.close()

        # Make RBP class labels all 1 == positives again.
        for g in graphs:
            if g.y.item() != 0:
                g.y = torch.tensor([1])

        # Report average accuracy with stdev.
        avg_acc = statistics.mean(acc_list)
        stdev_acc = statistics.stdev(acc_list)
        print("Average generic model accuarcy (+- stdev): %f (+- %f)" %(avg_acc, stdev_acc))
        avg_acc_out_file = args.out_folder + "/avg_accuracy.gm_cv.out"
        #AAOUT = open(avg_acc_out_file, "a+")
        AAOUT = open(avg_acc_out_file, "w")
        AAOUT.write("%f\t%f\n" %(avg_acc, stdev_acc))
        AAOUT.close()

    """
    Train final model.

    Do this by taking the whole training set, then split into train and
    validation set (validation set size can be set by --train-vs).
    Do hyperparameter optimization on the train set, and evaluate each
    combination on the validation set. Select the best combination, and
    output the respective model as final model.
    ALTERNATIVELY, use --train-cv to split train set into --train-cv-k
    folds, and do the described procedure --train-cv-k times. This yields
    an average accuracy for each HP combination. Select the HP combination
    with highest average accuracy. Train a final model with this combination,
    using the whole training set, and median # epochs obtained for this
    HP combination.

    """

    # Path to store final models.
    model_folder = out_folder + "/final_models/"
    if not os.path.exists(model_folder):
        os.makedirs(model_folder)

    final_model_path = out_folder + "/" + "final.model"
    final_params_path = out_folder + "/" + "final.params"

    print("")

    if args.train_cv:

        if run_hpo:
            print("Train final model (HPO enabled, %i splits!) ... " %(args.train_cv_k))
        else:
            print("Train final model (%i splits!)... " %(args.train_cv_k))

        skf = StratifiedKFold(n_splits=args.train_cv_k, shuffle=True)

        print("CALEB: I have something for you ... ")

        # Hyperparameter setting string to accuracy list.
        hps2acc_dic = {}
        # Hyperparameter setting string to used epochs list.
        hps2epo_dic = {}

        i_cv = 0

        for list_tr_idx, list_val_idx in skf.split(np.zeros(len(labels)), labels):

            i_cv += 1
            print("Starting --train-cv fold %i ... " %(i_cv))

            train_dataset = MyDataset([graphs[idx] for idx in list_tr_idx])
            val_dataset = MyDataset([graphs[idx] for idx in list_val_idx])

            # Report set sizes.
            c_train = len(list_tr_idx)
            c_val = len(list_val_idx)
            print("# training instances:    %i" %(c_train))
            print("# validation instances:  %i" %(c_val))

            # Get accuracies and used epochs for all HP combinations.
            opt_dic = model_util.select_model(args, dataset.num_features, train_dataset,
                                              val_dataset, model_folder, device,
                                              hps2acc_dic=hps2acc_dic,
                                              hps2epo_dic=hps2epo_dic)

        assert hps2acc_dic, "no HP accuracies recorded in CV (hps2acc_dic empty)"
        assert hps2epo_dic, "no HP epochs recorded in CV (hps2epo_dic empty)"

        # Calculate average accuracies of HP combinations, select best.
        best_hps = ""
        best_acc = 0
        best_epo = 0
        print("Calculate mean accuracies for each HP setting and select best ... ")
        for hps in hps2acc_dic:
            # Checks.
            assert hps in hps2epo_dic, "HP combination %s not in hps2epo_dic" %(hps)
            assert hps2acc_dic[hps], "HP combination %s accuracies list empty" %(hps)
            assert hps2epo_dic[hps], "HP combination %s epochs list empty" %(hps)
            len_acc = len(hps2acc_dic[hps])
            len_epo = len(hps2epo_dic[hps])
            assert len_acc == args.train_cv_k, "HP combination %s accuracies list length != --train-cv-k (%i != %i)" %(len_acc, args.train_cv_k)
            assert len_epo == args.train_cv_k, "HP combination %s epochs list length != --train-cv-k (%i != %i)" %(len_epo, args.train_cv_k)
            # Mean accuracy.
            mean_acc = statistics.mean(hps2acc_dic[hps])
            # For now use median epochs to train final model.
            median_epo = statistics.mean(hps2epo_dic[hps])
            if mean_acc > best_avg_acc:
                best_hps = hps
                best_acc = mean_acc
                best_epo = median_epo

        assert best_hps, "No HP combination selected (best_hps empty)"

        # Train final model.
        best_hpl = best_hps.split("_")
        best_batch_size = int(best_hpl[0])
        best_node_hidden_dim = int(best_hpl[1])
        best_weight_decay = float(best_hpl[2])
        best_lr = float(best_hpl[3])

        print("Best HP setting over --train-cv %i folds:" %(args.train_cv_k))
        print("Batch size = ", best_batch_size)
        print("Hidden layer node feature dim = ", best_node_hidden_dim)
        print("Weight decay = ", best_weight_decay)
        print("Learning rate = ", best_lr)
        print("Median used epochs = ", best_epo)
        print("Average validation accuracy = ", best_acc)

        print("Use best HP setting to train final model ... ")
        print("CALEB: tickets please ... ")

        train_dataset = MyDataset(graphs)
        c_train = len(graphs)
        print("# of training instances: %i" %(c_train))

        model_util.train_final_model(args, dataset, train_dataset, best_epo, device,
                                     final_model_path=final_model_path,
                                     batch_size=best_batch_size,
                                     lr=best_lr,
                                     node_hidden_dim=best_node_hidden_dim,
                                     weight_decay=best_weight_decay)

        # Write model params file.
        PAROUT = open(final_params_path, "w")
        PAROUT.write("fc_hidden_dim\t%s\n" %(str(args.fc_hidden_dim)))
        PAROUT.write("epochs\t%s\n" %(str(args.epochs)))
        PAROUT.write("patience\t%s\n" %(str(args.patience)))
        PAROUT.write("model_epochs\t%s\n" %(str(best_epo)))
        PAROUT.write("train_cv\t%s\n" %(str(args.train_cv)))
        PAROUT.write("train_cv_k\t%s\n" %(str(args.train_cv_k)))
        PAROUT.write("nr_hp_comb\t%s\n" %(str(nr_hp_comb)))
        PAROUT.write("val_acc\t%s\n" %(str(best_acc)))
        PAROUT.write("batch_size\t%s\n" %(str(best_batch_size)))
        PAROUT.write("lr\t%s\n" %(str(best_lr)))
        PAROUT.write("node_hidden_dim\t%s\n" %(str(best_node_hidden_dim)))
        PAROUT.write("weight_decay\t%s\n" %(str(best_weight_decay)))
        PAROUT.write("data_id\t%s" %(data_id))
        PAROUT.close()

        print("Final model (hyper)parameters file:\n%s" %())
        print(final_params_path)
        print("Final model file:")
        print(final_model_path)
        print("")

    else:

        if run_hpo:
            print("Train final model (HPO enabled, 1 split only) ... ")
        else:
            print("Train final model (1 split only)... ")

        val_set_perc = args.train_vs * 100
        print("Using %.1f" %(val_set_perc) + '% ' + "of training instances for validation ... ")

        # Make one train-validation split.
        sss = StratifiedShuffleSplit(n_splits=1, test_size=args.train_vs)

        for list_tr_idx, list_val_idx in sss.split(np.zeros(len(labels)), labels):

            train_dataset = MyDataset([graphs[idx] for idx in list_tr_idx])
            val_dataset = MyDataset([graphs[idx] for idx in list_val_idx])

            # Report set sizes.
            c_train = len(list_tr_idx)
            c_val = len(list_val_idx)
            print("# training instances:    %i" %(c_train))
            print("# validation instances:  %i" %(c_val))

            # Get optimal HPs.
            opt_dic = model_util.select_model(args, dataset.num_features, train_dataset,
                                              val_dataset, model_folder, device)

            opt_batch_size = opt_dic["opt_batch_size"]
            opt_node_hidden_dim = opt_dic["opt_node_hidden_dim"]
            opt_weight_decay = opt_dic["opt_weight_decay"]
            opt_lr = opt_dic["opt_lr"]
            opt_acc = opt_dic["opt_acc"]
            opt_epochs = opt_dic["opt_epochs"]

            #model = FunnelGNN(input_dim=dataset.num_features, node_hidden_dim=opt_node_hidden_dim, fc_hidden_dim=args.fc_hidden_dim, out_dim=2).to(device)

            # Move final model file.
            model_path = model_folder + "/" + str(opt_batch_size) + "_" + str(opt_node_hidden_dim) + "_" + str(opt_weight_decay) + "_" + str(opt_lr)
            #model.load_state_dict(torch.load(model_path))
            assert os.path.exists(model_path), "missing model file (path: %s)" %(model_path)
            gp2lib.move_rename_file(model_path, final_model_path)

            # Write model params file.
            PAROUT = open(final_params_path, "w")
            PAROUT.write("fc_hidden_dim\t%s\n" %(str(args.fc_hidden_dim)))
            PAROUT.write("epochs\t%s\n" %(str(args.epochs)))
            PAROUT.write("patience\t%s\n" %(str(args.patience)))
            PAROUT.write("model_epochs\t%s\n" %(str(opt_epochs)))
            PAROUT.write("train_cv\t%s\n" %(str(args.train_cv)))
            PAROUT.write("train_vs\t%s\n" %(str(args.train_vs)))
            PAROUT.write("nr_hp_comb\t%s\n" %(str(nr_hp_comb)))
            PAROUT.write("val_acc\t%s\n" %(str(opt_acc)))
            PAROUT.write("batch_size\t%s\n" %(str(opt_batch_size)))
            PAROUT.write("lr\t%s\n" %(str(opt_lr)))
            PAROUT.write("node_hidden_dim\t%s\n" %(str(opt_node_hidden_dim)))
            PAROUT.write("weight_decay\t%s\n" %(str(opt_weight_decay)))
            PAROUT.write("data_id\t%s" %(data_id))
            PAROUT.close()

            # Epochs and accuracy.
            print("# epochs used to train model: ", opt_epochs)
            print("Model validation accuracy:    ", opt_acc)
            print("")

            if run_hpo:
                print("Optimized final model hyperparameters")
                print("=====================================")
                print("batch size:                    ", opt_batch_size)
                print("Hidden layer node feature dim: ", opt_node_hidden_dim)
                print("Weight decay:                  ", opt_weight_decay)
                print("learning rate:                 ", opt_lr)
                print("")
            else:
                print("Final model hyperparameters")
                print("===========================")
                print("batch size:                    ", opt_batch_size)
                print("Hidden layer node feature dim: ", opt_node_hidden_dim)
                print("Weight decay:                  ", opt_weight_decay)
                print("learning rate:                 ", opt_lr)
                print("")
            print("Final model (hyper)parameters file:")
            print(final_params_path)
            print("Final model file:")
            print(final_model_path)
            print("")


################################################################################

def main_eval(args):
    """
    Evaluate properties learned from positive binding sites.

    """

    print("Running for you in EVAL mode ... ")

    assert os.path.isdir(args.in_folder), "--in folder does not exist"
    # Get model parameters.
    params_file = args.in_folder + "/final.params"
    assert os.path.isfile(params_file), "missing model training parameter file %s" %(params_file)
    params_dic = gp2lib.read_settings_into_dic(params_file)
    fc_hidden_dim = 128
    node_hidden_dim	= 128
    data_id = "data"
    if "node_hidden_dim" in params_dic:
        node_hidden_dim = int(params_dic["node_hidden_dim"])
    if "fc_hidden_dim" in params_dic:
        fc_hidden_dim = int(params_dic["fc_hidden_dim"])
    if "data_id" in params_dic:
        data_id = params_dic["data_id"]
    # Data folder (containing PTG model training data).
    data_folder = args.in_folder + "/" + data_id
    # PTG raw data subfolder.
    data_raw_folder = data_folder + "/raw"
    # Plot format.
    plot_format = "png"
    if args.plot_format == 2:
        plot_format = "pdf"

    print("fc_hidden_dim:    %i" %(fc_hidden_dim))
    print("node_hidden_dim:  %i" %(node_hidden_dim))

    model_file = args.in_folder + "/final.model"
    assert os.path.isdir(data_folder), "missing data folder %s" %(data_folder)
    assert os.path.isdir(data_raw_folder), "missing raw data folder %s" %(data_raw_folder)
    assert os.path.isfile(model_file), "missing model file %s" %(model_file)

    assert args.motif_sc_thr <= 1 and args.motif_sc_thr >= -1, "--motif-sc-thr must be within -1 and 1"

    # Get graphprot2 train parameters.
    gp2_train_param_file = args.in_folder + "/settings.graphprot2_train.out"
    assert os.path.isfile(gp2_train_param_file), "missing --in parameter file %s" %(gp2_train_param_file)
    gp2_train_param_dic = {}
    with open(gp2_train_param_file) as f:
        for line in f:
            row = line.strip()
            cols = line.strip().split("\t")
            gp2_train_param_dic[cols[0]] = cols[1]
    f.closed
    if "use_bps" in gp2_train_param_dic:
        assert gp2_train_param_dic["use_bps"] == "False", "graphprot2 eval currently does not support --use-bps in grahprot2 train"
    assert "con_ext" in gp2_train_param_dic, "con_ext info tag missing in %s" %(gp2_train_param_file)
    # Get nucleotide alphabet + con_ext info.
    con_ext = False
    con_ext_str = gp2_train_param_dic["con_ext"]
    if con_ext_str != "False": # if not False, must be INT.
        con_ext = int(con_ext_str)

    # Get graphprot2 train features.
    gp2_train_feat_out_file = args.in_folder + "/features.out"
    assert os.path.isfile(gp2_train_feat_out_file), "missing --in feature file %s" %(gp2_train_feat_out_file)
    gp2_train_feat_dic = {}
    feat_alphabet_dic = {}
    with open(gp2_train_feat_out_file) as f:
        for line in f:
            row = line.strip()
            cols = line.strip().split("\t")
            feat_id = cols[0]
            gp2_train_feat_dic[feat_id] = 1
            feat_cat_list = cols[2].split(",")
            feat_cat_list.sort()
            feat_alphabet_dic[feat_id] = feat_cat_list
    f.closed
    assert gp2_train_feat_dic, "no features found in %s" %(gp2_train_feat_out_file)
    assert "bpp.str" not in gp2_train_feat_dic, "graphprot2 eval currently does not support models trained with base pair information"

    # Check for node attributes data.
    use_node_attr = True
    ana_file = data_raw_folder + "/" + data_id + "_node_attributes.txt"
    if not os.path.exists(ana_file):
        use_node_attr = False

    # Get channel info.
    if use_node_attr:
        channel_info_file = args.in_folder + "/channel_infos.out"
        assert os.path.isfile(channel_info_file), "missing channel information file %s" %(channel_info_file)
        ch2label = {}
        ch2featid = {}
        featid2ch_dic = {} # feature ID to channels list.
        featid2alpha_dic = {}
        with open(channel_info_file) as f:
            for line in f:
                if re.search("^ch", line):
                    continue
                cols = line.strip().split("\t")
                ch = int(cols[0])
                ch_id = cols[1]
                feat_id = cols[2]
                feat_type = cols[3]
                if feat_id in featid2ch_dic:
                    featid2ch_dic[feat_id].append(ch)
                else:
                    featid2ch_dic[feat_id] = [ch]
                if feat_type == "C":
                    m = re.search(".+_(.+)", ch_id)
                    label = m.group(1)
                    ch2label[ch] = label
                    if feat_id in featid2alpha_dic:
                        featid2alpha_dic[feat_id].append(label)
                    else:
                        featid2alpha_dic[feat_id] = [label]
                else:
                    ch2label[ch] = False
                ch2featid[ch] = feat_id
        f.closed
        assert ch2featid, "no channel infos read in"

    # Read in additional infos.
    add_out_folder = data_folder + "/add_info"
    pos_add_ids_out_file = add_out_folder + "/positives.add_info.ids"
    neg_add_ids_out_file = add_out_folder + "/negatives.add_info.ids"
    pos_add_seqs_out_file = add_out_folder + "/positives.add_info.fa"
    neg_add_seqs_out_file = add_out_folder + "/negatives.add_info.fa"
    assert os.path.isfile(pos_add_ids_out_file), "missing additional file pos_add_ids_out_file"
    assert os.path.isfile(neg_add_ids_out_file), "missing additional file neg_add_ids_out_file"
    assert os.path.isfile(pos_add_seqs_out_file), "missing additional file pos_add_seqs_out_file"
    assert os.path.isfile(neg_add_seqs_out_file), "missing additional file neg_add_seqs_out_file"
    pos_seqs_dic = gp2lib.read_fasta_into_dic(pos_add_seqs_out_file)
    pos_ids_dic = {}
    with open(pos_add_ids_out_file) as f:
        for line in f:
            cols = line.strip().split("\t")
            pos_ids_dic[cols[0]] = int(cols[1])
    f.closed
    # Store graph ID -> sequence ID.
    gid2sid_dic = {}
    for seq_id in pos_ids_dic:
        gid2sid_dic[pos_ids_dic[seq_id]] = seq_id

    # Results output folders.
    out_folder = args.out_folder
    if not os.path.exists(out_folder):
        os.makedirs(out_folder)
    data_out_folder = out_folder + "/" + data_id
    raw_out_folder = data_out_folder + "/raw"
    proc_out_folder = data_out_folder + "/processed"
    if not os.path.exists(raw_out_folder):
        os.makedirs(raw_out_folder)
    if not os.path.exists(proc_out_folder):
        os.makedirs(proc_out_folder)

    # Window size.
    assert args.list_win_sizes, "empty --win-sizes list given"
    min_win_size = 666666
    for win_size in args.list_win_sizes:
        if (win_size-1) % 2:
            assert False, "only uneven window size values supported"
        if win_size < 5:
            assert False, "window size should be >= 5"
        if win_size < min_win_size:
            min_win_size = win_size

    # Motif size.
    for motif_size in args.list_motif_sizes:
        if (motif_size-1) % 2:
            assert False, "only uneven motif size values supported"
        if motif_size < 5:
            assert False, "motif sizes should be >= 5"
        if motif_size > 21:
            assert False, "motif sizes should be <= 21"

    # Output mode settings.
    settings_file = args.out_folder + "/" + "/settings.graphprot2_eval.out"
    SETOUT = open(settings_file, "w")
    for arg in vars(args):
        SETOUT.write("%s\t%s\n" %(arg, str(getattr(args, arg))))
    SETOUT.close()

    # Read in PTG raw data.
    dataset = TUDataset(data_folder, name=data_id, use_node_attr=use_node_attr)
    assert len(dataset), "read-in dataset empty"
    print("# dataset features:    %i" %(dataset.num_features))
    print("Dataset size:          %i" %(len(dataset)))
    print("Window sizes:         ", args.list_win_sizes)
    print("Top site numbers:     ", args.list_nr_top_sites)
    print("Motif sizes:          ", args.list_motif_sizes)

    # Sequence alphabet.
    seq_alphabet = ["A","C","G","U"]
    if con_ext:
        seq_alphabet = ["A","C","G","U","a","c","g","u"]
    print("Sequence alphabet:          ", seq_alphabet)
    # Exon intron annotation alphabet.
    eia_alphabet = ["E", "I"]
    if "eia" in feat_alphabet_dic:
        eia_alphabet = feat_alphabet_dic["eia"]
        print("Exon-intron alphabet:       ", eia_alphabet)
    rra_alphabet = ["N", "R"]
    if "rra" in feat_alphabet_dic:
        rra_alphabet = feat_alphabet_dic["rra"]
        print("Repeat region alphabet:     ", rra_alphabet)
    tra_alphabet = ["C", "F", "T"]
    if "tra" in feat_alphabet_dic:
        tra_alphabet = feat_alphabet_dic["tra"]
        print("Transcript region alphabet: ", tra_alphabet)

    # Graphs list.
    graphs = [dataset[idx] for idx in range(dataset.__len__())]
    labels = [g.y for g in graphs]

    # Check number of features.
    assert dataset.num_features >= 4, "# dataset features expected to be >= 4"

    # Get positive graphs.
    print("Selecting positive sites ... ")
    pos_graphs = []
    for g in graphs:
        if g.y == 1:
            pos_graphs.append(g)
    assert pos_graphs, "no positive sites (graph label == 1) in dataset"
    print("# positive sites:      %i" %(len(pos_graphs)))

    # Data loader object.
    pos_set = MyDataset(pos_graphs)
    batch_size = 1
    pos_loader = DataLoader(pos_set, batch_size=batch_size, shuffle=False)

    # CUDA device.
    if torch.cuda.is_available():
        print("CUDA: I'm available. Using GPU ... ")
    else:
        print("CUDA: I'm NOT available. Using CPU ... ")
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    #print("CALEB: sailing sailing, over the bounding main ... ")

    # Define model.
    model = FunnelGNN(input_dim=dataset.num_features, node_hidden_dim=node_hidden_dim,
                      fc_hidden_dim=fc_hidden_dim, out_dim=2).to(device)


    # Load model state dictionary.
    model.load_state_dict(torch.load(model_file))

    # Get site scores.
    print("Get positive whole site scores (class 1 probabilities) ... ")
    pos_site_probs = model_util.get_site_probs(pos_loader, device, model)
    assert pos_site_probs, "pos_site_probs list empty"

    # Normalize class 1 probabilities to -1 .. 1.
    #for i,p in enumerate(pos_site_probs):
    #    pos_site_probs[i] = gp2lib.min_max_normalize(p, 1, 0, borders=[-1, 1])

    """
    Method 1
    ========

    Predict subgraphs. batch_size of 1 needed to execute this code.

    # Get position-wise subgraph scores (-1 .. 1) to compare with second method.
    print("Get positive position-wise subgraph scores ... ")
    subgraph_scores_ll = model_util.get_subgraph_scores(pos_loader, device,
                                                        model, args.list_win_size,
                                                        softmax=False)

    Find out why this nice strategy yields different results ...

    """


    """
    Method 2
    ========

    For each graph, split to windows, output each window in raw format,
    predict on it, get scores and assemble for each graph all_scores list,
    containing a list of scores for each window + at the end the mean scores.

    """
    batch_size = 50
    # 1-hot string to nucleotide label.
    hot2l_dic = {'1000': 1,
                 '0100': 2,
                 '0010': 3,
                 '0001': 4}
    hot2nt_dic = {'1000': "A",
                  '0100': "C",
                  '0010': "G",
                  '0001': "U"}
    # If lowercase context model.
    if con_ext:
        hot2l_dic = {'10000000': 1,
                     '01000000': 2,
                     '00100000': 3,
                     '00010000': 4,
                     '00001000': 5,
                     '00000100': 6,
                     '00000010': 7,
                     '00000001': 8}
        hot2nt_dic = {'10000000': "A",
                      '01000000': "C",
                      '00100000': "G",
                      '00010000': "U",
                      '00001000': "a",
                      '00000100': "c",
                      '00000010': "g",
                      '00000001': "u"}

    # Get character counts for sequences.
    cc_dic = gp2lib.seqs_dic_count_chars(pos_seqs_dic)
    # Check for invalid nucleotides characters.
    allowed_nt_dic = {'A': 1, 'C': 1, 'G': 1, 'U': 1}
    c_nts = 4
    if con_ext:
        allowed_nt_dic = {'A': 1, 'C': 1, 'G': 1, 'U': 1, 'a': 1, 'c': 1, 'g': 1, 'u': 1}
        c_nts = 8
    for nt in cc_dic:
        if nt not in allowed_nt_dic:
            assert False, "positive sequences with invalid character \"%s\" encountered (allowed characters: ACGU(acgu)" %(nt)
    assert len(cc_dic) == c_nts, "# of nucleotide characters in positive set != expected # (%i != %i)" %(len(cc_dic), c_nts)

    profile_scores_ll = []
    seqs_ll = []
    eia_ll = []
    tra_ll = []
    rra_ll = []
    pc_ll = []
    pp_ll = []

    print("Compute profile scores ... ")
    for g in pos_graphs:
        x = g.x
        x = x.tolist()
        g_l = len(x)
        # List of node attributes.
        list_node_attr = []
        # List of node labels.
        list_node_labels = []
        # More lists.
        seqs_list = [] # nucleotide labels.
        eia_list = [] # exon intron labels.
        rra_list = [] # repeat region labels.
        tra_list = [] # transcript region labels.
        pc_list = [] # phastCons scores.
        pp_list = [] # phastCons scores.

        for v in x:
            if use_node_attr:
                nav = v[:-c_nts]
                # Exon intron labels list.
                if "eia" in featid2ch_dic:
                    for ch in featid2ch_dic["eia"]:
                        v_i = ch - 1
                        if v[v_i]:
                            eia_list.append(ch2label[ch])
                            break
                # Repeat region labels list.
                if "rra" in featid2ch_dic:
                    for ch in featid2ch_dic["rra"]:
                        v_i = ch - 1
                        if v[v_i]:
                            rra_list.append(ch2label[ch])
                            break
                # Transcript region labels list.
                if "tra" in featid2ch_dic:
                    for ch in featid2ch_dic["tra"]:
                        v_i = ch - 1
                        if v[v_i]:
                            tra_list.append(ch2label[ch])
                            break
                # Conservation scores.
                if "pc.con" in featid2ch_dic:
                    v_i = featid2ch_dic["pc.con"][0] - 1
                    pc_list.append(v[v_i])
                if "pp.con" in featid2ch_dic:
                    v_i = featid2ch_dic["pp.con"][0] - 1
                    pp_list.append(v[v_i])

                # assert len(nav) > c_dist_nts, "invalid number of node attributes (expects > 4, got %i)" %(len(nav))
                new_nav = [str(e) for e in v[:-c_nts]]
                list_node_attr.append(",".join(new_nav))

            seq_1h = [int(e) for e in v[-c_nts:]]
            seq_1h_s = [str(e) for e in seq_1h]
            seq_1h_s = "".join(seq_1h_s)
            assert seq_1h_s in hot2l_dic, "seq_1h_s %s not in hot2l_dic" %(seq_1h_s)
            n_i = hot2l_dic[seq_1h_s]
            list_node_labels.append(n_i)
            # Get sequence nucleotides list.
            nt = hot2nt_dic[seq_1h_s]
            seqs_list.append(nt)

        # Tickets please.
        assert list_node_labels, "list_node_labels empty"
        if use_node_attr:
            if "eia" in featid2ch_dic:
                assert len(list_node_labels) == len(eia_list), "differing list lengths for list_node_labels and eia_list"
            if "rra" in featid2ch_dic:
                assert len(list_node_labels) == len(rra_list), "differing list lengths for list_node_labels and rra_list"
            if "tra" in featid2ch_dic:
                assert len(list_node_labels) == len(tra_list), "differing list lengths for list_node_labels and tra_list"
            if "pc.con" in featid2ch_dic:
                assert len(list_node_labels) == len(pc_list), "differing list lengths for list_node_labels and pc_list"
            if "pp.con" in featid2ch_dic:
                assert len(list_node_labels) == len(pp_list), "differing list lengths for list_node_labels and pp_list"

        seqs_ll.append(seqs_list)
        if eia_list:
            eia_ll.append(eia_list)
        if rra_list:
            rra_ll.append(rra_list)
        if tra_list:
            tra_ll.append(tra_list)
        if pc_list:
            pc_ll.append(pc_list)
        if pp_list:
            pp_ll.append(pp_list)

        # Skip sequences containing < 4 different nucleotides.
        #if len(set(list_node_labels)) != 4:
        #    profile_scores_ll.append([])
        #    continue

        # Get position-wise profile scores for graph.
        profile_scores = model_util.get_window_scores(args.list_win_sizes,
                                          dataset_name=data_id,
                                          list_node_attr=list_node_attr,
                                          list_node_labels=list_node_labels,
                                          use_node_attr=use_node_attr,
                                          model=model,
                                          device=device,
                                          lc_context=con_ext,
                                          con_ext=False,
                                          batch_size=batch_size,
                                          geometric_folder=out_folder)

        profile_scores_ll.append(profile_scores)


    assert len(profile_scores_ll) == len(pos_site_probs), "differing list lengths for profile_scores_ll and pos_site_probs (%i != %i)" %(len(profile_scores_ll), len(pos_site_probs))

    #print("profile_scores_ll[0]:", profile_scores_ll[0])
    #print("seqs_ll[0]:", seqs_ll[0])
    #print("eia_ll[0]:", eia_ll[0])
    #print("pc_ll[0]:", pc_ll[0])

    gi2sc_dic = {}
    for i, pr in enumerate(pos_site_probs):
        gi2sc_dic[i] = pr

    """
    Plot profiles and motifs
    ========================

    - Sort graphs by score and evaluate top sites.
    - Get profile plot for each site.
    - Get motifs inside the set by selecting positions with max score,
      and take sequence around them (defined by motif size).

    """
    profile_plots_folder = args.out_folder + "/" + "profile_plots_out"
    if os.path.exists(profile_plots_folder):
        shutil.rmtree(profile_plots_folder)
    os.makedirs(profile_plots_folder)
    win_info_str = ""
    for win_size in args.list_win_sizes:
        win_info_str += "w%i" %(win_size)

    # Sort graphs by score to evaluate top sites.
    rank_scores_out = args.out_folder + "/rank_score_sites.out"
    RSOUT = open(rank_scores_out, "w")
    site_count = 0
    id_i = 10 ** len(str(args.nr_top_profiles))
    c_thr_skipped = 0

    print("Plot top %i profiles ... " %(args.nr_top_profiles))
    for gi, sc in sorted(gi2sc_dic.items(), key=lambda item: item[1], reverse=True):
        # Profile scores.
        profile_scores = profile_scores_ll[gi]
        if not profile_scores:
            continue
        site_count += 1
        id_i += 1

        # Store ranke + score info.
        RSOUT.write("%i\t%f\n" %(site_count, sc))

        # Sequence nucleotides list.
        seq_list = seqs_ll[gi]
        # Site sequence.
        seq = "".join(seqs_ll[gi])
        # Label and score lists.
        exon_intron_labels = False
        repeat_region_labels = False
        transcript_region_labels = False
        phastcons_scores = False
        phylop_scores = False
        if use_node_attr:
            if "eia" in featid2ch_dic:
                exon_intron_labels = eia_ll[gi]
            if "rra" in featid2ch_dic:
                repeat_region_labels = rra_ll[gi]
            if "tra" in featid2ch_dic:
                transcript_region_labels = tra_ll[gi]
            if "pc.con" in featid2ch_dic:
                phastcons_scores = pc_ll[gi]
            if "pp.con" in featid2ch_dic:
                phylop_scores = pp_ll[gi]

        if site_count <= args.nr_top_profiles:

            plot_file_name = str(id_i)[1:] + "_" + win_info_str + "_sc" + str(sc) + "." + plot_format
            plot_out_file = profile_plots_folder + "/" + plot_file_name

            gp2lib.make_feature_attribution_plot(seq, profile_scores, plot_out_file,
                                          seq_alphabet=seq_alphabet,
                                          eia_alphabet=eia_alphabet,
                                          tra_alphabet=tra_alphabet,
                                          rra_alphabet=rra_alphabet,
                                          seq_label_plot=False,
                                          exon_intron_labels=exon_intron_labels,
                                          transcript_region_labels=transcript_region_labels,
                                          repeat_region_labels=repeat_region_labels,
                                          phastcons_scores=phastcons_scores,
                                          phylop_scores=phylop_scores)
    RSOUT.close()

    print("Create motifs ... ")

    motif_file_info_dic = {}

    # For each motif size.
    for motif_size in args.list_motif_sizes:

        # Motif center position extension.
        motif_extlr = int(motif_size / 2)

        # For each number of top sites.
        for nr_top_sites in args.list_nr_top_sites:

            print("Create motif (size %i) from top %i sites ... " %(motif_size, nr_top_sites))

            motif_seqs_ll = []
            motif_eia_ll = []
            motif_rra_ll = []
            motif_tra_ll = []
            motif_pc_ll = []
            motif_pp_ll = []
            site_count = 0

            for gi, sc in sorted(gi2sc_dic.items(), key=lambda item: item[1], reverse=True):
                # Profile scores.
                profile_scores = profile_scores_ll[gi]
                if not profile_scores:
                    continue

                # Sequence nucleotides list.
                seq_list = seqs_ll[gi]
                # Label and score lists.
                exon_intron_labels = False
                repeat_region_labels = False
                transcript_region_labels = False
                phastcons_scores = False
                phylop_scores = False
                if use_node_attr:
                    if "eia" in featid2ch_dic:
                        exon_intron_labels = eia_ll[gi]
                    if "rra" in featid2ch_dic:
                        repeat_region_labels = rra_ll[gi]
                    if "tra" in featid2ch_dic:
                        transcript_region_labels = tra_ll[gi]
                    if "pc.con" in featid2ch_dic:
                        phastcons_scores = pc_ll[gi]
                    if "pp.con" in featid2ch_dic:
                        phylop_scores = pp_ll[gi]

                # Extract motifs.
                site_l = len(profile_scores)
                # Minimum sequence length for extracting motifs.
                if site_l < (motif_extlr*2 + 1):
                    continue

                tr_prof_sc = profile_scores[motif_extlr:-motif_extlr]
                max_sc = -1
                max_i = 0
                for i,sc in enumerate(tr_prof_sc):
                    if sc > max_sc:
                        max_sc = sc
                        max_i = i+motif_extlr
                if max_sc < args.motif_sc_thr:
                    print("max_sc < thr: %f" %(max_sc))
                    c_thr_skipped += 1
                    continue

                site_count += 1

                # Evaluate only nr_top_sites
                if site_count > nr_top_sites:
                    break

                # Motif list start and end index.
                motif_s = max_i - motif_extlr
                motif_e = max_i + motif_extlr + 1
                # Store motifs.
                seq_motif_list = []
                eia_motif_list = []
                rra_motif_list = []
                tra_motif_list = []
                pc_motif_list = []
                pp_motif_list = []
                # Extract motif indices from lists.
                for i in range(motif_s, motif_e):
                    seq_motif_list.append(seq_list[i])
                    if use_node_attr:
                        if "eia" in featid2ch_dic:
                            eia_motif_list.append(exon_intron_labels[i])
                        if "rra" in featid2ch_dic:
                            rra_motif_list.append(repeat_region_labels[i])
                        if "tra" in featid2ch_dic:
                            tra_motif_list.append(transcript_region_labels[i])
                        if "pc.con" in featid2ch_dic:
                            pc_motif_list.append(phastcons_scores[i])
                        if "pp.con" in featid2ch_dic:
                            pp_motif_list.append(phylop_scores[i])
                assert seq_motif_list, "seq_motif_list empty"
                motif_seqs_ll.append(seq_motif_list)
                if use_node_attr:
                    if "eia" in featid2ch_dic:
                        motif_eia_ll.append(eia_motif_list)
                    if "rra" in featid2ch_dic:
                        motif_rra_ll.append(rra_motif_list)
                    if "tra" in featid2ch_dic:
                        motif_tra_ll.append(tra_motif_list)
                    if "pc.con" in featid2ch_dic:
                        motif_pc_ll.append(pc_motif_list)
                    if "pp.con" in featid2ch_dic:
                        motif_pp_ll.append(pp_motif_list)

            # Plot motifs.
            c_motifs = len(motif_seqs_ll)
            assert c_motifs, "no motif sequences extracted"
            print("# motif sequences extracted:  %i" %(c_motifs))
            print("# motif sequences skipped:    %i" %(c_thr_skipped))

            motif_plots_folder = args.out_folder
            #motif_plots_folder = args.out_folder + "/" + "motif_plots_out"
            #if os.path.exists(motif_plots_folder):
            #    shutil.rmtree(motif_plots_folder)
            #os.makedirs(motif_plots_folder)
            motif_out_file = motif_plots_folder + "/" + "motif_l" + str(motif_size) + "_" + win_info_str + "_top" + str(nr_top_sites) + "." + plot_format

            print("Plot motifs ... ")
            gp2lib.make_motif_plot(motif_seqs_ll, motif_out_file,
                                   motif_eia_ll=motif_eia_ll,
                                   motif_rra_ll=motif_rra_ll,
                                   motif_tra_ll=motif_tra_ll,
                                   motif_pc_ll=motif_pc_ll,
                                   motif_pp_ll=motif_pp_ll,
                                   tra_alphabet=tra_alphabet,
                                   rra_alphabet=rra_alphabet,
                                   seq_alphabet=seq_alphabet,
                                   eia_alphabet=eia_alphabet)

            motif_file_info_dic[motif_out_file] = 1

    print("")
    print("EVALUATION OUTPUT FILES")
    print("=======================")
    for motif_out_file in motif_file_info_dic:
        print("Motif plot:\n%s" %(motif_out_file))
    print("Profile plots folder:\n%s" %(profile_plots_folder))
    print("")


################################################################################

def main_predict(args):
    """
    Predict binding sites (whole site or binding profiles).

    """

    print("Running for you in PREDICT mode ... ")
    if args.mode == 1:
        print("Predicting whole sites ... ")
    if args.mode == 2:
        print("Predicting profiles and top-scoring sites ... ")

    assert os.path.isdir(args.in_folder), "--in folder does not exist"
    assert os.path.isdir(args.model_in_folder), "--model-in folder does not exist"

    # Get model parameters.
    params_file = args.model_in_folder + "/final.params"
    model_file = args.model_in_folder + "/final.model"
    assert os.path.isfile(params_file), "missing model training parameter file %s" %(params_file)
    assert os.path.isfile(params_file), "missing model file %s" %(model_file)
    params_dic = gp2lib.read_settings_into_dic(params_file)
    fc_hidden_dim = 128
    node_hidden_dim	= 128
    batch_size = 50
    if "node_hidden_dim" in params_dic:
        node_hidden_dim = int(params_dic["node_hidden_dim"])
    if "fc_hidden_dim" in params_dic:
        fc_hidden_dim = int(params_dic["fc_hidden_dim"])
    if "batch_size" in params_dic:
        batch_size = int(params_dic["batch_size"])

    # Generate results output folder.
    out_folder = args.out_folder
    if not os.path.exists(out_folder):
        os.makedirs(out_folder)
    model_folder = args.model_in_folder

    # Get graphprot2 train parameters.
    gp2_train_param_file = model_folder + "/settings.graphprot2_train.out"
    assert os.path.isfile(gp2_train_param_file), "missing graphprot2 train parameter file %s" %(gp2_train_param_file)
    gp2_train_param_dic = {}
    with open(gp2_train_param_file) as f:
        for line in f:
            row = line.strip()
            cols = line.strip().split("\t")
            gp2_train_param_dic[cols[0]] = cols[1]
    f.closed
    #if "use_bps" in gp2_train_param_dic:
    #    assert gp2_train_param_dic["use_bps"] == "False", "graphprot2 eval currently does not support --use-bps in grahprot2 train"
    assert "con_ext" in gp2_train_param_dic, "con_ext info tag missing in %s" %(gp2_train_param_file)
    # Get nucleotide alphabet + con_ext info.
    con_ext = False
    con_ext_str = gp2_train_param_dic["con_ext"]
    if con_ext_str != "False": # if not False, must be INT.
        con_ext = int(con_ext_str)
    if args.con_ext:
        assert con_ext, "--con-ext set but --model-in model does not support lowercase context"

    # Output files.
    ws_scores_out = out_folder + "/" + "whole_site_scores.out"
    score_profiles_out = out_folder + "/" + "profiles.out"
    peak_regions_bed = out_folder + "/" + "profiles_peak_regions.bed"
    peak_regions_ref_bed = out_folder + "/" + "profiles_peak_regions.ref.bed"
    top_prof_sites_bed = out_folder + "/" + "wide_profiles_top_sites.bed"
    top_prof_sites_ref_bed = out_folder + "/" + "wide_profiles_top_sites.ref.bed"
    top_prof_sites_merged_bed = out_folder + "/" + "wide_profiles_top_sites.merged.bed"
    top_prof_sites_merged_ref_bed = out_folder + "/" + "wide_profiles_top_sites.merged.ref.bed"
    # Profile window scores output file.
    prof_win_sc_out_file = out_folder + "/" + "profiles_window_scores.out"
    # Wide (fixed) window scores output file.
    pe_prof_win_sc_out_file = out_folder + "/" + "wide_profiles_window_scores.out"
    if os.path.exists(prof_win_sc_out_file):
        os.remove(prof_win_sc_out_file)
    if os.path.exists(pe_prof_win_sc_out_file):
        os.remove(pe_prof_win_sc_out_file)

    # PyG internal data ID, also name of data subfolder.
    data_id = "data"
    in_folder = args.in_folder
    data_out_folder = args.out_folder + "/" + data_id
    if not os.path.exists(data_out_folder):
        os.makedirs(data_out_folder)

    raw_out_folder = data_out_folder + "/raw"
    proc_out_folder = data_out_folder + "/processed"
    if not os.path.exists(raw_out_folder):
        os.makedirs(raw_out_folder)
    if not os.path.exists(proc_out_folder):
        os.makedirs(proc_out_folder)

    print("Model training input folder:       %s" %(model_folder))
    print("Prediction set input folder:       %s" %(in_folder))
    print("Prediction results output folder:  %s" %(out_folder))
    if args.mode == 2:
        print("Profile window size(s):           ", args.list_win_sizes)
        print("Top-scoring sites --peak-ext:      %i" %(args.peak_ext))
    if con_ext:
        print("--model-in model with --con-ext %i ... " %(con_ext))

    # Output mode settings.
    settings_file = out_folder + "/settings.graphprot2_predict.out"
    SETOUT = open(settings_file, "w")
    for arg in vars(args):
        SETOUT.write("%s\t%s\n" %(arg, str(getattr(args, arg))))
    SETOUT.close()

    # Check for CUDA support.
    if torch.cuda.is_available():
        print("CUDA: I'm available. Using GPU ... ")
    else:
        print("CUDA: I'm NOT available. Using CPU ... ")

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # Read in prediction sequences.
    test_fa_in = args.in_folder + "/" + "test.fa"
    assert os.path.exists(test_fa_in), "--in folder does not contain %s"  %(test_fa_in)
    test_seq_uc = False
    # If profile prediction, make input sequences all uppercase.
    if args.mode == 2:
        test_seq_uc = True
    test_seqs_dic = gp2lib.read_fasta_into_dic(test_fa_in, all_uc=test_seq_uc)
    i = 0
    idx2id_dic = {}
    for seq_id, seq in sorted(test_seqs_dic.items()):
        idx2id_dic[i] = seq_id
        i += 1

    # Read in region information.
    id2chr_dic = {}
    id2s_dic = {}
    id2e_dic = {}
    id2pol_dic = {}
    test_bed_in = args.in_folder + "/" + "test.bed"
    only_seq = True
    if os.path.exists(test_bed_in):
        only_seq = False
        with open(test_bed_in) as f:
            for line in f:
                row = line.strip()
                cols = line.strip().split("\t")
                chr_id = cols[0]
                site_s = int(cols[1])
                site_e = int(cols[2])
                site_id = cols[3]
                site_pol = cols[5]
                id2chr_dic[site_id] = chr_id
                id2s_dic[site_id] = site_s
                id2e_dic[site_id] = site_e
                id2pol_dic[site_id] = site_pol
        f.closed

    # Load whole site data and store in PyG format.
    gp2lib.load_ws_predict_data(args,
                                data_id=data_id,
                                predict_con_ext=args.con_ext,
                                add_info_out=True)

    # Use node attributes?
    use_node_attr = True
    ana_file = raw_out_folder + "/" + data_id + "_node_attributes.txt"
    if not os.path.exists(ana_file):
        use_node_attr = False
    if not use_node_attr:
        print("Node attributes disabled ... ")

    geometric_cv_data_path = data_out_folder

    print("Load PyG data again ... ")
    print("CALEB: your bad day just got worse ... ")

    dataset = TUDataset(geometric_cv_data_path, name=data_id, use_node_attr=use_node_attr)
    l_ds = len(dataset)
    assert l_ds, "read-in dataset empty"
    c_seqs = len(test_seqs_dic)
    # If fake graph was added.
    if args.mode == 2 and con_ext:
        assert l_ds == (c_seqs+1), "fake graph expected in dataset, but lengths differ (l_ds (%i) != c_seqs (%i) + 1)" %(l_ds, c_seqs)
        dataset = dataset[:-1]

    print("# dataset features:  %i" %(dataset.num_features))
    print("Dataset size:        %i" %(len(dataset)))

    # Load model.
    print("Load model ... ")
    model = FunnelGNN(input_dim=dataset.num_features, node_hidden_dim=node_hidden_dim,
                      fc_hidden_dim=fc_hidden_dim, out_dim=2).to(device)
    model.load_state_dict(torch.load(model_file))

    """
    CASE 1: whole site predictions.

    - Load prediction data from graphprot2 predict and store in PyG format.
    - Run whole site predictions on generated graph data.
    - Output whole site scores.

    """

    # Whole site predictions.
    if args.mode == 1:

        #predict_dataset = MyDataset(dataset)
        #predict_loader = DataLoader(predict_dataset, batch_size=batch_size, shuffle=False)
        predict_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)

        # Get whole site scores.
        print("Do the whole site predictions ... ")
        ws_scores = model_util.get_scores(predict_loader, device, model,
                                          min_max_norm=True)

        # Checks.
        assert len(ws_scores) == len(test_seqs_dic), "# test sequences != # whole site scores (%i != %i)" %(len(ws_scores), len(test_seqs_dic))
        # Output whole site scores.
        WSOUT = open(ws_scores_out, "w")
        for i,sc in enumerate(ws_scores):
            seq_id = idx2id_dic[i]
            WSOUT.write("%s\t%f\n" %(seq_id, sc))

        WSOUT.close()

        # Get site scores.
        #print("Get positive whole site scores (class 1 probabilities) ... ")
        #pos_site_probs = model_util.get_site_probs(predict_loader, device, model)
        #assert pos_site_probs, "pos_site_probs list empty"

    """
    CASE 2: profile + peak predictions.

    - predict batch_sizeposition-wise scoring profiles using one ore more
      windows (args.win_size)
    - Extract peaks from profiles, predict on peaks extended by peak_ext
      and con_ext, and report best peaks (choose best scoring in case of
      overlapping peaks)

Profile predictions done already, use code from eval
Then extract from scoring lists peak regions (above thr).
Extend them again, make subgraphs, predict scores for these,
select best overlapping site (or two lists, one without best selection, one with)

    """
    peaks_found = False
    wide_peaks_found = False

    # Profile and peak predictions.
    if args.mode == 2:

        print("CALEB: I have something for you ... ")

        # 1-hot string to nucleotide label.
        hot2l_dic = {'1000': 1,
                     '0100': 2,
                     '0010': 3,
                     '0001': 4}
        hot2nt_dic = {'1000': "A",
                      '0100': "C",
                      '0010': "G",
                      '0001': "U"}
        # If lowercase context model.
        if con_ext:
            hot2l_dic = {'10000000': 1,
                         '01000000': 2,
                         '00100000': 3,
                         '00010000': 4,
                         '00001000': 5,
                         '00000100': 6,
                         '00000010': 7,
                         '00000001': 8}
            hot2nt_dic = {'10000000': "A",
                          '01000000': "C",
                          '00100000': "G",
                          '00010000': "U",
                          '00001000': "a",
                          '00000100': "c",
                          '00000010': "g",
                          '00000001': "u"}

        # Get character counts for sequences.
        cc_dic = gp2lib.seqs_dic_count_chars(test_seqs_dic)
        # Check for invalid nucleotides characters.
        allowed_nt_dic = {'A': 1, 'C': 1, 'G': 1, 'U': 1}
        c_nts = 4
        if con_ext:
            allowed_nt_dic = {'A': 1, 'C': 1, 'G': 1, 'U': 1, 'a': 1, 'c': 1, 'g': 1, 'u': 1}
            c_nts = 8
        for nt in cc_dic:
            if nt not in allowed_nt_dic:
                assert False, "test sequences with invalid character \"%s\" encountered (allowed characters: ACGU(acgu)" %(nt)
        # assert len(cc_dic) == c_nts, "# of nucleotide characters in test set != expected # (%i != %i)" %(len(cc_dic), c_nts)

        PROFOUT = open(score_profiles_out, "w")
        PEAKOUT = open(peak_regions_bed, "w")
        PEAKROUT = open(peak_regions_ref_bed, "w")
        TOPOUT = open(top_prof_sites_bed, "w")
        TOPROUT = open(top_prof_sites_ref_bed, "w")
        # Peak count.
        peak_i = 0
        # List of lists with profile scores.
        profile_scores_ll = []

        print("Compute profile scores and top sites ... ")

        for i,g in enumerate(dataset):
            x = g.x
            x = x.tolist()
            g_l = len(x)
            # Sequence ID.
            seq_id = idx2id_dic[i]
            # List of node attributes.
            list_node_attr = []
            # List of node labels.
            list_node_labels = []
            # List of nt labels.
            seqs_list = []

            for v in x:
                if use_node_attr:
                    nav = [str(e) for e in v[:-c_nts]]
                    list_node_attr.append(",".join(nav))
                # Nucleotides.
                seq_1h = [int(e) for e in v[-c_nts:]]
                seq_1h_s = [str(e) for e in seq_1h]
                seq_1h_s = "".join(seq_1h_s)
                assert seq_1h_s in hot2l_dic, "seq_1h_s %s not in hot2l_dic" %(seq_1h_s)
                n_i = hot2l_dic[seq_1h_s]
                list_node_labels.append(n_i)
                # Get sequence nucleotides list.
                nt = hot2nt_dic[seq_1h_s]
                seqs_list.append(nt)

            # Tickets please.
            assert list_node_labels, "list_node_labels empty"
            len_nodes = len(list_node_labels)

            # Skip sequences containing < 4 different nucleotides.
            #if len(set(list_node_labels)) != 4:
            #    print("Sequence %s contains < 4 individual nucleotides. Skip this sequence ... " %(seq_id))
            #    continue

            # Predict sliding window profiles.
            window_scores = model_util.get_window_scores(args.list_win_sizes,
                                              dataset_name=data_id,
                                              list_node_attr=list_node_attr,
                                              list_node_labels=list_node_labels,
                                              use_node_attr=use_node_attr,
                                              model=model,
                                              con_ext=False,
                                              lc_context=con_ext,
                                              device=device,
                                              batch_size=batch_size,
                                              seq_id=seq_id,
                                              win_sc_out_file=prof_win_sc_out_file,
                                              geometric_folder=out_folder)
            assert len_nodes == len(window_scores), "# nodes != # window scores"

            # Get peak regions + positions.
            peak_list = gp2lib.list_extract_peaks(window_scores,
                                                  max_merge_dist=args.max_merge_dist,
                                                  coords="bed",
                                                  sc_thr=args.sc_thr)

            # If not peaks found (above --thr score).
            if not peak_list:
                print("WARNING: no profile (--win-size) peaks for sequence %s ... " %(seq_id))
                continue
            else:
                peaks_found = True

            # Output profile.
            PROFOUT.write(">%s\n" %(seq_id))
            for i,sc in enumerate(window_scores):
                nt = seqs_list[i]
                pos = i + 1
                PROFOUT.write("%i\t%s\t%f\n" %(pos, nt, sc))

            if not only_seq:
                chr_id = id2chr_dic[seq_id]
                site_pol = id2pol_dic[seq_id]
                ref_s = id2s_dic[site_id]
                ref_e = id2e_dic[site_id]

            # Output peak regions.
            for peak_reg in peak_list:
                peak_s = peak_reg[0]
                peak_e = peak_reg[1]
                top_pos = peak_reg[2]
                top_sc = peak_reg[3]
                peak_i += 1
                peak_id = "peak_%i" %(peak_i)
                PEAKOUT.write("%s\t%i\t%i\t%s,%i,%f\t0\t+\n" %(seq_id, peak_s, peak_e, peak_id, top_pos, top_sc))
                if not only_seq:
                    new_s, new_e = gp2lib.bed_convert_coords(peak_s, peak_e, ref_s, ref_e, site_pol)
                    new_top_s, new_top_e = gp2lib.bed_convert_coords(top_pos-1, top_pos, ref_s, ref_e, site_pol)
                    PEAKROUT.write("%s\t%i\t%i\t%s,%i,%f\t0\t%s\n" %(chr_id, new_s, new_e, peak_id, new_top_e, top_sc, site_pol))

            # Predict fixed (args.peak_ext) window profiles.
            peak_ext_win_size=args.peak_ext*2
            if args.con_ext:
                print("Set context extension to --con-ext %i ... " %(args.con_ext))
                con_ext = args.con_ext
            fixed_win_scores = model_util.get_window_scores([peak_ext_win_size],
                                              dataset_name=data_id,
                                              list_node_attr=list_node_attr,
                                              list_node_labels=list_node_labels,
                                              use_node_attr=use_node_attr,
                                              model=model,
                                              lc_context=con_ext,
                                              con_ext=con_ext,
                                              device=device,
                                              batch_size=batch_size,
                                              seq_id=seq_id,
                                              win_sc_out_file=pe_prof_win_sc_out_file,
                                              geometric_folder=out_folder)
            assert len_nodes == len(fixed_win_scores), "# nodes != # fixed window scores"

            # Get peak regions + positions.
            wide_win_peak_list = gp2lib.list_extract_peaks(fixed_win_scores,
                                                  max_merge_dist=args.max_merge_dist,
                                                  coords="bed",
                                                  sc_thr=args.sc_thr)

            # If not peaks found (above --thr score).
            if not wide_win_peak_list:
                print("WARNING: no wide profile peaks (--peak-ext, --con-ext) for sequence %s ... " %(seq_id))
                continue
            else:
                wide_peaks_found = True

            # Output peak regions.
            for peak_reg in wide_win_peak_list:
                top_pos = peak_reg[2]
                top_sc = peak_reg[3]
                win_s = top_pos - args.peak_ext - 1
                win_e = top_pos + args.peak_ext
                if con_ext:
                    win_s -= con_ext
                    win_e += con_ext
                if win_e > len_nodes:
                    win_e = len_nodes
                if win_s < 0:
                    win_s = 0
                TOPOUT.write("%s\t%i\t%i\t%s,%i\t%f\t+\n" %(seq_id, win_s, win_e, seq_id, top_pos, top_sc))
                if not only_seq:
                    new_s, new_e = gp2lib.bed_convert_coords(win_s, win_e, ref_s, ref_e, site_pol)
                    new_top_s, new_top_e = gp2lib.bed_convert_coords(top_pos-1, top_pos, ref_s, ref_e, site_pol)
                    TOPROUT.write("%s\t%i\t%i\t%s,%i\t%f\t+\n" %(chr_id, new_s, new_e, seq_id, new_top_e, top_sc))

        PROFOUT.close()
        PEAKOUT.close()
        PEAKROUT.close()
        TOPOUT.close()
        TOPROUT.close()

        # Select best sites from overlapping sites.
        if wide_peaks_found:
            gp2lib.bed_sort_merge_output_top_entries(top_prof_sites_bed, top_prof_sites_merged_bed)
        else:
            print("WARNING: no wide profile peaks (--peak-ext, --con-ext) for any input sequence!")
        if only_seq:
            if os.path.exists(peak_regions_ref_bed):
                os.remove(peak_regions_ref_bed)
            if os.path.exists(top_prof_sites_ref_bed):
                os.remove(top_prof_sites_ref_bed)
            if os.path.exists(top_prof_sites_merged_ref_bed):
                os.remove(top_prof_sites_merged_ref_bed)
        else:
            if wide_peaks_found:
                gp2lib.bed_sort_merge_output_top_entries(top_prof_sites_ref_bed, top_prof_sites_merged_ref_bed)

        if not peaks_found:
            print("WARNING: no profile (--win-size) peaks for any input sequence!")

    """
    Output file summary.

    """

    if args.mode == 1:
        print("")
        print("PREDICTION OUTPUT FILES")
        print("=======================")
        print("Whole site predictions output file:\n%s" %(ws_scores_out))
    if args.mode == 2:
        if peaks_found:
            print("")
            print("PREDICTION OUTPUT FILES")
            print("=======================")
            print("Profiles (--win-size) output file:\n%s" %(score_profiles_out))
            print("Profiles (--win-size) window scores:\n%s" %(prof_win_sc_out_file))
            print("Profiles (--win-size) peak regions  .bed:\n%s" %(peak_regions_bed))
            if not only_seq:
                print("Profiles peak regions on reference .bed:\n%s" %(peak_regions_ref_bed))
        if wide_peaks_found:
            if not peaks_found:
                print("")
                print("PREDICTION OUTPUT FILES")
                print("=======================")
            print("Wide profiles (--peak-ext, --con-ext) window scores:\n%s" %(pe_prof_win_sc_out_file))
            print("Wide profiles (--peak-ext, --con-ext) top-scoring sites .bed:\n%s" %(top_prof_sites_bed))
            print("Wide profiles top select-best-in-case-of-overlaps sites .bed:\n%s" %(top_prof_sites_merged_bed))
            if not only_seq:
                print("Wide profiles top-scoring sites on reference .bed:\n%s" %(top_prof_sites_ref_bed))
                print("Wide profiles select-best-in-case-of-overlaps sites on reference .bed:\n%s" %(top_prof_sites_merged_ref_bed))
    print("")


################################################################################

def main_gt(args):
    """
    Generate a training set.

    """

    print("Running for you in GT mode ... ")

    # Generate results output folder.
    if not os.path.exists(args.out_folder):
        os.makedirs(args.out_folder)

    """
    Output files.

    """
    # BED sites files.
    pos_bed_out = args.out_folder + "/" + "positives.bed"
    neg_bed_out = args.out_folder + "/" + "negatives.bed"
    # FASTA sequences of sites files.
    pos_fasta_out = args.out_folder + "/" + "positives.fa"
    neg_fasta_out = args.out_folder + "/" + "negatives.fa"
    # Conservation annotation files.
    pos_pc_con_out = args.out_folder + "/" + "positives.pc.con"
    neg_pc_con_out = args.out_folder + "/" + "negatives.pc.con"
    pos_pp_con_out = args.out_folder + "/" + "positives.pp.con"
    neg_pp_con_out = args.out_folder + "/" + "negatives.pp.con"
    # Transcript region annotation files.
    pos_tra_out = args.out_folder + "/" + "positives.tra"
    neg_tra_out = args.out_folder + "/" + "negatives.tra"
    # Repeat region annotation files.
    pos_rra_out = args.out_folder + "/" + "positives.rra"
    neg_rra_out = args.out_folder + "/" + "negatives.rra"
    # Exon-intron annotation files.
    pos_eia_out = args.out_folder + "/" + "positives.eia"
    neg_eia_out = args.out_folder + "/" + "negatives.eia"
    # Secondary structure annotation files.
    pos_str_elem_p_out = args.out_folder + "/" + "positives.elem_p.str"
    neg_str_elem_p_out = args.out_folder + "/" + "negatives.elem_p.str"
    pos_bpp_out = args.out_folder + "/" + "positives.bpp.str"
    neg_bpp_out = args.out_folder + "/" + "negatives.bpp.str"
    # Mode settings file.
    settings_file = args.out_folder + "/" + "/settings.graphprot2_gt.out"
    # Chromosome or transcript lengths file.
    chr_lengths_file = args.out_folder + "/" + "reference_lengths.out"
    # Genes or transcript regions containing positives.
    regions_with_positives_bed = args.out_folder + "/" + "regions_with_positives.bed"
    # Feature table.
    feat_table_out = args.out_folder + "/" + "features.out"

    """
    Temporary output files.

    """
    # tmp files for processing --in sites.
    tmp1_bed = args.out_folder + "/" + "sites1.tmp.bed"
    tmp2_bed = args.out_folder + "/" + "sites2.tmp.bed"
    tmp3_bed = args.out_folder + "/" + "sites3.tmp.bed"
    # tmp files for processing --neg-in sites.
    neg_tmp1_bed = args.out_folder + "/" + "neg_sites1.tmp.bed"
    neg_tmp2_bed = args.out_folder + "/" + "neg_sites2.tmp.bed"
    neg_tmp3_bed = args.out_folder + "/" + "neg_sites3.tmp.bed"
    # Gene regions from --gtf.
    gene_regions_gtf_tmp_bed = args.out_folder + "/" + "gene_regions.gtf.tmp.bed"
    # Positives with context +col5 score = 0.
    positives_with_context_bed = args.out_folder + "/" + "positives_with_context.tmp.bed"
    # Negatives with context +col5 score = 0.
    negatives_with_context_bed = args.out_folder + "/" + "negatives_with_context.tmp.bed"
    # Regions for masking (no negatives from these regions).
    mask_negatives_bed = args.out_folder + "/" + "mask_negatives.tmp.bed"
    # Positive sites to shuffle / generate negatives from using bedtools shuffle.
    shuffle_in_bed = args.out_folder + "/" + "shuffle_in.tmp.bed"
    # Random negative sites returned from bedtools shuffle.
    random_negatives_bed = args.out_folder + "/" + "random_negatives.tmp.bed"
    # Random negative sites with unique IDs.
    random_negatives_unique_ids_bed = args.out_folder + "/" + "random_negatives_unique_ids.tmp.bed"
    # Extracted sequences for pos/neg sites.
    pos_tmp_fasta = args.out_folder + "/" + "positives.tmp.fa"
    neg_tmp_fasta = args.out_folder + "/" + "negatives.tmp.fa"
    # Most prominent transcripts BED regions.
    most_prominent_transcripts_bed = args.out_folder + "/" + "most_prominent_transcripts.tmp.bed"
    # Most prominent genes BED regions.
    most_prominent_genes_bed = args.out_folder + "/" + "most_prominent_genes.tmp.bed"
    # Transcript sites mapped to genome.
    pos_transcript_to_genome_bed = args.out_folder + "/" + "positives.transcript_to_genome.bed"
    neg_transcript_to_genome_bed = args.out_folder + "/" + "negatives.transcript_to_genome.bed"

    # Custom base pairs check.
    if args.bp_in:
        assert args.add_str, "--bp-in requires --str to be set for structure calculation"

    # Write generated feature infos to table.
    FEATOUT = open(feat_table_out, "w")

    """
    CASE 1: sequences provided as input (--in FASTA)
    ================================================

    --in FASTA and optionally --neg-fa are set.

    If --in FASTA is set, create negative sequences by shuffling
    Use --neg-fa FASTA to provide negative sequences (instead of
    shuffling positives to generate negatives)
    Supported additional features if --in FASTA:
    --str (secondary structure features)

    """

    # Check if --in file is FASTA file.
    if gp2lib.fasta_check_fasta_file(args.in_sites):
        # Read in sequences (this also checks for unique headers + converts to RNA).
        pos_seqs_dic = gp2lib.read_fasta_into_dic(args.in_sites)
        # Get viewpoint region start+end dictionaries.
        id2vpse_dic = gp2lib.extract_viewpoint_regions_from_fasta(pos_seqs_dic,
                                                                  get_se_dic=True)
        # Looking for lowercase context.
        lc_context = False
        for seq_id in id2vpse_dic:
            seq_l = len(pos_seqs_dic[seq_id])
            if id2vpse_dic[seq_id][1] != seq_l or id2vpse_dic[seq_id][0] != 1:
                lc_context = True
                break
        # Sequence character counts.
        seq_cc_dic = gp2lib.seqs_dic_count_chars(pos_seqs_dic)
        if lc_context:
            print("Lowercase context regions found inside --in sequences ... ")
            allowed_nt_dic = {'A': 1, 'C': 1, 'G': 1, 'U': 1, 'a': 1, 'c': 1, 'g': 1, 'u': 1}
            for nt in seq_cc_dic:
                if nt not in allowed_nt_dic:
                    assert False, "--in sequences contain \"%s\" character (allowed characters: A C G U a c g u)" %(nt)
            FEATOUT.write("fa\tC\tA,C,G,U,a,c,g,u\t-\n")
            # Get most frequenet lowercase extension.
            major_lcl = gp2lib.get_major_lc_len_from_seqs_dic(pos_seqs_dic)
            assert major_lcl, "no lowercase regions extracted (major_lcl evaluated to False)"
            print("Most frequent lowercase region length found:  %i" %(major_lcl))
            print("Set --con-ext to this length ... ")
            # Use major_lcl as con_ext.
            args.con_ext = major_lcl
        else:
            allowed_nt_dic = {'A': 1, 'C': 1, 'G': 1, 'U': 1}
            for nt in seq_cc_dic:
                if nt not in allowed_nt_dic:
                    assert False, "--in sequences contain \"%s\" character (allowed characters: A C G U)" %(nt)
            FEATOUT.write("fa\tC\tA,C,G,U\t-\n")

        # If negative sequences are given, no shuffling of positive sequences necessary to produce negatives.
        if args.in_neg_sites:
            assert os.path.exists(args.in_neg_sites), "--neg-in file \"%s\" not found" %(args.in_neg_sites)
            assert gp2lib.fasta_check_fasta_file(args.in_neg_sites), "--in FASTA requires --neg-in in same format (FASTA)"
            print("Generating training set from --in and --neg-in ... ")
            neg_seqs_dic = gp2lib.read_fasta_into_dic(args.in_neg_sites)
            id2vpse_dic = gp2lib.extract_viewpoint_regions_from_fasta(neg_seqs_dic,
                                                                      id2se_dic=id2vpse_dic,
                                                                      get_se_dic=True)

        else:
            print("Generating training set from --in (shuffling positives to get negatives) ... ")
            print("Set shuffling method: %i-nucleotide shuffling" %(args.shuffle_k))
            # Shuffle positives to generate negatives.
            neg_seqs_dic = gp2lib.ushuffle_sequences(pos_seqs_dic,
                                                     new_ids=True,
                                                     id2vpse_dic=id2vpse_dic,
                                                     id_prefix="shuff_neg",
                                                     ushuffle_k=args.shuffle_k)
        # Output processed sequences.
        gp2lib.fasta_output_dic(pos_seqs_dic, pos_fasta_out)
        gp2lib.fasta_output_dic(neg_seqs_dic, neg_fasta_out)

        # Secondary structure stats dictionaries.
        pos_str_stats_dic = None
        neg_str_stats_dic = None

        # If structure features should be computed.
        if args.add_str:
            FEATOUT.write("bpp.str\tN\tbp_prob\tprob\n")
            FEATOUT.write("elem_p.str\tN\tp_u,p_e,p_h,p_i,p_m,p_s\tprob\n")
            # If report=True, collect statistics.
            if args.report:
                pos_str_stats_dic = {}
                neg_str_stats_dic = {}
            # Calculate structure features for positives.
            print("Get secondary structure features for positive set ... ")
            gp2lib.calc_str_elem_up_bpp(pos_fasta_out, pos_bpp_out, pos_str_elem_p_out,
                                        stats_dic=pos_str_stats_dic,
                                        plfold_u=args.plfold_u,
                                        plfold_l=args.plfold_l,
                                        plfold_w=args.plfold_w)
            # If custom base pair information given.
            if args.bp_in:
                # Sanity check, process, and output custom base pair information file.
                print("--bp-in file supplied. Check, process + overwrite positive base pairs ... ")
                gp2lib.process_custom_bp_file(args.bp_in, pos_bpp_out, pos_seqs_dic,
                                              stats_dic=pos_str_stats_dic)
            # Calculate structure features for negatives.
            print("Get secondary structure features for negative set ... ")
            gp2lib.calc_str_elem_up_bpp(neg_fasta_out, neg_bpp_out, neg_str_elem_p_out,
                                        stats_dic=neg_str_stats_dic,
                                        plfold_u=args.plfold_u,
                                        plfold_l=args.plfold_l,
                                        plfold_w=args.plfold_w)

        # Sequence set stats.
        c_pos_out = len(pos_seqs_dic)
        c_neg_out = len(neg_seqs_dic)
        if args.report:
            # HTML report output file.
            html_report_out = args.out_folder + "/" + "report.graphprot2_gt.html"
            # Plots subfolder.
            plots_subfolder = "plots_graphprot2_gt"
            # Get library and logo path.
            gp2lib_path = os.path.dirname(gp2lib.__file__)
            # Generate report.
            gp2lib.gp2_gt_generate_html_report(pos_seqs_dic, neg_seqs_dic, args.out_folder,
                                               "s", gp2lib_path,
                                               id2ucr_dic=id2vpse_dic,
                                               plots_subfolder=plots_subfolder,
                                               html_report_out=html_report_out,
                                               pos_str_stats_dic=pos_str_stats_dic,
                                               neg_str_stats_dic=neg_str_stats_dic,
                                               theme=args.theme,
                                               kmer_top=10,
                                               rna=True,
                                               uc_entropy=True)

        # Mode settings output file.
        SETOUT = open(settings_file, "w")
        for arg in vars(args):
            SETOUT.write("%s\t%s\n" %(arg, str(getattr(args, arg))))
        SETOUT.close()

        print("# positive sequences output: %i" %(c_pos_out))
        print("# negative sequences output: %i" %(c_neg_out))
        print("")
        print("TRAINING SET OUTPUT FILES")
        print("=========================")
        print("Positives sequences .fa:\n%s" %(pos_fasta_out))
        print("Negatives sequences .fa:\n%s" %(neg_fasta_out))
        if args.add_str:
            print("Positives base pair probabilities .bpp.str:\n%s" %(pos_bpp_out))
            print("Negatives base pair probabilities .bpp.str:\n%s" %(neg_bpp_out))
            print("Positives position-wise structural elements probabilities .elem_p.str:\n%s" %(pos_str_elem_p_out))
            print("Negatives position-wise structural elements probabilities .elem_p.str:\n%s" %(neg_str_elem_p_out))
        if args.report:
            print("Training set generation report .html:\n%s" %(html_report_out))
        print("")
        FEATOUT.close()
        sys.exit()

    """
    CASE 2: genomic or transcript sites given (--in BED)
    ====================================================

    This means:
        - Negatives have to be generated by random sampling,
          unless --neg-in BED is set (== no random negative generation).
        - Additional features can be added

    For genomic sites, the features are:
        - Position-wise exon-intron annotation (with options)
        - Position-wise transcript region annotation (with options)
        - Position-wise repeat region annotation
        - Position-wise conservation scores (phasCons and phyloP)
        - Structure features (base pairs and structural elements probabilities)

    For transcript sites, the features are:
        - Position-wise transcript region annotation (with options)
        - Position-wise repeat region annotation
        - Position-wise conservation scores (phasCons and phyloP)
        - Structure features (base pairs and structural elements probabilities)

    NOTE that exon-intron features make no sense for transcript sites, as each
    position would get the exon label (thus omitted for transcript sites)

    Negatives generation differs between genomic + transcript sites:
    For genomic sites, by default negative sites are selected randomly
    from gene regions containing positive sites.
    For transcript sites, by default negatives are selected randomly
    from transcripts containing positive sites.

    In case not enough random negatives can be extracted from the described
    regions, the search space is expanded:
    - For genomic sites, all gene regions with TSL1-5 transcripts will
      be extracted from the --gtf, and these regions will then be used
      for negatives extraction.
    - For transcript sites, all TSL1-5 transcripts (selecting the transcript
      with highest experimental evidence for each gene) will be used for
      negatives extraction.

    Regarding negative region lengths:
    Currently the region lengths correspond to the positive region lengths
    as bedtools shuffle keeps the lengths for each shuffled region.
    Future implementations could additionally include e.g.:
    The median positive length or expected positive length (after
    --seq-ext and --con-ext, if --mode 1)

    Regarding --con-ext:
    --con-ext site extension with lower-case context sequence is by default
    done after merging overlapping sites.
    If --con-ext-pre is set, --con-ext extension will be done before
    merging of overlapping sites.

    """
    # If --in are BED, require --gtf and --gen.
    if not args.in_gtf:
        print("ERROR: --gtf GTF file is required if --in receives BED file")
        sys.exit()
    if not args.in_2bit:
        print("ERROR: --gen .2bit file is required if --in receives BED file")
        sys.exit()
    # Some sanity checks regarding set values.
    if args.mode == 2:
        if (args.min_len-1) % 2: # if number even.
            assert False, "currently only uneven --min-len values supported"
        if args.min_len < 11 or args.min_len > 201:
            assert False, "use reasonable --min-len values (>= 11, <= 201)"
    min_len_ext = int( (args.min_len - 1) / 2)
    # Maximum site length after applying --seq-ext and --con-ext.
    max_site_length = 500
    if args.max_len > max_site_length:
        assert False, "use reasonable --max-len values for limiting input site lengths (<= %i)" %(max_site_length)
    # Expected site length after --seq-ext.
    exp_site_length = args.seq_ext * 2 + 1
    if exp_site_length > max_site_length:
        assert False, "use reasonable site extension values (--seq-ext) (<= --max-len)"
    if args.con_ext:
        exp_site_length = exp_site_length + 2 * args.con_ext
        if exp_site_length > max_site_length:
            assert False, "use reasonable site extension values (--seq-ext + --con-ext) (<= --max-len)"
        if args.add_str:
            assert args.con_ext <= args.plfold_l, "--con-ext needs to be <= --plfold-l"
    if args.add_str:
        assert args.plfold_l < args.plfold_w, "--plfold-l must be smaller than --plfold-w"

    # Input checks.
    assert os.path.exists(args.in_sites), "--in BED file \"%s\" not found" %(args.in_sites)
    assert os.path.exists(args.in_gtf), "--gtf GTF file \"%s\" not found" %(args.in_gtf)
    assert os.path.exists(args.in_2bit), "--gen .2bit file \"%s\" not found" %(args.in_2bit)
    assert gp2lib.bed_check_six_col_format(args.in_sites), "--in BED file appears to be not in 6-column BED format"
    if args.mask_bed:
        assert os.path.exists(args.mask_bed), "--mask-bed BED file \"%s\" not found" %(args.mask_bed)
        assert gp2lib.bed_check_six_col_format(args.mask_bed), "--mask-bed BED file appears to be not in 6-column BED format"
    if args.pc_bw:
        assert os.path.exists(args.pc_bw), "--phastcons file \"%s\" not found" %(args.pc_bw)
    if args.pp_bw:
        assert os.path.exists(args.pp_bw), "--phylop file \"%s\" not found" %(args.pp_bw)
    if args.tr_reg_codon_annot:
        assert args.tr_reg_annot, "--tra-codons requires --tra to be effective"
    if args.tr_reg_border_annot:
        assert args.tr_reg_annot, "--tra-borders requires --tra to be effective"
    if args.intron_border_annot:
        assert args.exon_intron_annot, "--eia-ib requires --eia to be effective"
    if args.exon_intron_n:
        assert args.exon_intron_annot, "--eia-n requires --eia to be effective"

    # If --in column 4 IDs should be kept, check their uniqueness.
    if args.keep_ids:
        assert gp2lib.bed_check_unique_ids(args.in_sites), "--in BED file \"%s\" column 4 IDs not unique. Disable --keep-ids or provide unique IDs" %(args.in_sites)
        if args.in_neg_sites:
            assert gp2lib.bed_check_unique_ids(args.in_neg_sites), "--neg-in BED file \"%s\" column 4 IDs not unique. Disable --keep-ids or provide unique IDs" %(args.in_neg_sites)
            assert gp2lib.bed_check_unique_ids_two_files(args.in_sites, args.in_neg_sites), "combined column 4 IDs of --neg-in BED and --in BED not unique. Disable --keep-ids or provide unique IDs"

    # Quick check what remains after applying score threshold.
    if args.sc_thr is not None:
        c_rem = gp2lib.bed_get_score_filtered_count(args.in_sites, args.sc_thr,
                                                    rev_filter=args.rev_filter)
        if not c_rem:
            print("ERROR: no remaining sites after filtering by set --thr score. Use lower score threshold or disable --thr")
            sys.exit()

    # Initial --neg-in checks.
    if args.in_neg_sites:
        assert os.path.exists(args.in_neg_sites), "--neg-in file \"%s\" not found" %(args.in_neg_sites)
        assert not gp2lib.fasta_check_fasta_file(args.in_neg_sites), "--neg-in seems to be in FASTA format, but BED format expected"
        assert gp2lib.bed_check_six_col_format(args.in_neg_sites), "--neg-in BED file appears to be not in 6-column BED format"

    # Mode settings output file.
    SETOUT = open(settings_file, "w")
    for arg in vars(args):
        SETOUT.write("%s\t%s\n" %(arg, str(getattr(args, arg))))
    SETOUT.close()

    # Get chromsome lengths from --gen.
    print("Get chromosome lengths from --gen ... ")
    chr_len_dic = gp2lib.get_chromosome_lengths_from_2bit(args.in_2bit, chr_lengths_file)
    # Get chromosome IDs.
    print("Get chromosome IDs from --in ... ")
    chr_ids_dic = gp2lib.bed_get_chromosome_ids(args.in_sites)

    # Check whether IDs are genomic or not.
    transcript_regions = False
    for chr_id in chr_ids_dic:
        if chr_id not in chr_len_dic:
            transcript_regions = True
            break
    # Demand pure transcript or genomic sites --in BED.
    if transcript_regions:
        for chr_id in chr_ids_dic:
            assert chr_id not in chr_len_dic, "chromosome and non-chromosome IDs encountered in --in BED column 1. --in column 1 must contain either chromsome or transcript IDs (conflicting IDs: \"%s\", \"%s\")" %(transcript_regions, chr_id)
        print("No chromosome IDs found in --in, interpret column 1 IDs as transcript IDs ... ")
    # Demand same thing for negative sites.
    if args.in_neg_sites:
        neg_chr_ids_dic = gp2lib.bed_get_chromosome_ids(args.in_neg_sites)
        for chr_id in neg_chr_ids_dic:
            if transcript_regions:
                assert chr_id not in chr_len_dic, "--in BED contains transcript sites, whereas --neg-in also contains chromosome IDs in BED column 1. Use either transcript or genomic sites for both --in and --neg-in"
            else:
                assert chr_id in chr_len_dic, "--in BED contains genomic sites, whereas --neg-in also contains non-chromosome IDs in BED column 1. Use either transcript or genomic sites for both --in and --neg-in"

    # If transcript_regions, there should be no genomic col1 IDs.
    tr_seqs_dic = {}
    if transcript_regions:
        # Check for set --exon-intron.
        if args.exon_intron_annot:
            print("ERROR: setting --eia is useless for transcript sites. Please disable or provide genomic input sites.")
            sys.exit()
        if args.intron_border_annot:
            print("ERROR: setting --eia-ib is useless for transcript sites. Please disable or provide genomic input sites.")
            sys.exit()
        if args.exon_intron_n:
            print("ERROR: setting --eia-ib is useless for transcript sites. Please disable or provide genomic input sites.")
            sys.exit()
        # Read in transcript lengths from --gtf.
        print("Get transcript lengths from --gtf ... ")
        tr_len_dic = gp2lib.gtf_get_transcript_lengths(args.in_gtf)
        print("Verifying transcript IDs by checking their presence in --gtf ... ")
        for tr_id in chr_ids_dic:
            assert tr_id in tr_len_dic, "--in column 1 transcript ID \"%s\" not found in --gtf" %(tr_id)
        # Make a transcript regions .bed file.
        print("Output transcripts containing positive sites to BED ... ")
        gp2lib.bed_sequence_lengths_to_bed(tr_len_dic, regions_with_positives_bed,
                                           ids_dic=chr_ids_dic)
        if args.in_neg_sites:
            for tr_id in neg_chr_ids_dic:
                assert tr_id in tr_len_dic, "--neg-in column 1 transcript ID \"%s\" not found in --gtf" %(tr_id)
            chr_ids_dic.update(neg_chr_ids_dic)
        # Get spliced transcript sequences from gtf+2bit.
        print("Get transcript sequences from --gtf and --gen ... ")
        tr_seqs_dic = gp2lib.get_transcript_sequences_from_gtf(args.in_gtf, args.in_2bit,
                                                               lc_repeats=args.rep_reg_annot,
                                                               tr_ids_dic=chr_ids_dic)
        # Output transcript lengths file.
        print("Output transcript lengths file ... ")
        gp2lib.output_chromosome_lengths_file(tr_len_dic, chr_lengths_file)
        # Make a new chr_len_dic, only with transcript IDs from --in and their lengths.
        chr_len_dic = {}
        for tr_id in chr_ids_dic:
            tr_len = tr_len_dic[tr_id]
            chr_len_dic[tr_id] = tr_len
        # Update chromosome lengths dictionary.
        # chr_len_dic.update(tr_len_dic)
    else:
        # Genomic sites checks.
        if args.tr_reg_border_annot:
            print("ERROR: setting --tra-borders only supported for transcript sites. Please disable or provide transcript input sites.")
            sys.exit()

    """
    Process positives
    =================

    - Filter and extend positives
    - if --neg-in, filter and extend negatives as well
    - select best overlapping sites for --in, not for --neg-in
    - Disable overlap selection by --allow-overlaps

    """

    # Process --in positives.
    ps_count_dic = {}
    id2pl_dic = gp2lib.process_in_sites(args.in_sites, tmp1_bed, chr_len_dic, args,
                                        transcript_regions=transcript_regions,
                                        count_dic=ps_count_dic,
                                        id_prefix="CLIP")
    print("# positive sites found in --in:             %i" %(ps_count_dic['c_in']))
    if ps_count_dic['c_filt_max_len']:
        print("# positive sites filtered by --max-len:     %i" %(ps_count_dic['c_filt_max_len']))
    if ps_count_dic['c_filt_ref']:
        print("# positive sites removed (reference ID):    %i" %(ps_count_dic['c_filt_ref']))
    if ps_count_dic['c_filt_thr']:
        print("# positive sites removed (--thr):           %i" %(ps_count_dic['c_filt_thr']))
    assert ps_count_dic['c_out'], "no positive sites remaining after filtering and extension ... "
    print("# positive sites read in (post-filtering):  %i" %(ps_count_dic['c_out']))

    # If --neg-in negative sites given, process same as --in.
    if args.in_neg_sites:
        ns_count_dic = {}
        id2pl_dic = gp2lib.process_in_sites(args.in_neg_sites, neg_tmp1_bed, chr_len_dic, args,
                                            transcript_regions=transcript_regions,
                                            count_dic=ns_count_dic,
                                            id2pl_dic=id2pl_dic,
                                            id_prefix="neg")

        print("# negative sites found in --neg-in:         %i" %(ps_count_dic['c_in']))

        if ns_count_dic['c_filt_max_len']:
            print("# negative sites filtered by --max-len:     %i" %(ns_count_dic['c_filt_max_len']))
        if ps_count_dic['c_filt_thr']:
            print("# negative sites filtered by --thr:         %i" %(ns_count_dic['c_filt_thr']))
        assert ns_count_dic['c_out'], "no negative sites remaining after filtering and extension ... "
        print("# negative sites read in (post-filtering):  %i" %(ns_count_dic['c_out']))

    # If overlaps between sites allowed (should work with transcript + genomic sites).
    if args.allow_overlaps:
        gp2lib.make_file_copy(tmp1_bed, tmp2_bed)
    else:
        print("Select highest-scoring sites in case of overlaps ... ")
        # Check if scores are there.
        sc2c_dic = gp2lib.bed_get_score_to_count_dic(tmp1_bed)
        # Throw this error in case no col5 scores are present (e.g. each row with score 0).
        if len(sc2c_dic) == 1:
            print("WARNING: --in BED regions all contain the same column 5 score!")
            print("         To select best sites in case of overlaps, please add")
            print("            distinct BED column 5 scores to --in BED file.")
            print("                    Enabling --allow-overlaps ... ")
            args.allow_overlaps = True
        #assert len(sc2c_dic) != 1, "removing overlaps by selecting entries with best BED column 5 scores useless since --in BED file contains only one distinct column 5 score. Set --allow-overlaps to skip removal of overlaps or assign column 5 scores to --in BED regions"
        # Merge overlapping sites, selecting best scoring site for each overlap group.
        gp2lib.bed_sort_merge_output_top_entries(tmp1_bed, tmp2_bed,
                                                 rev_filter=args.rev_filter)

    # For genomic sites, extract gene regions containing positives.
    if not transcript_regions:
        # Get gene regions from .gtf.
        print("Extract gene regions from --gtf ... ")
        gp2lib.gtf_extract_gene_bed(args.in_gtf, gene_regions_gtf_tmp_bed)
        # Get gene regions containing positives.
        params = "-u -s"
        print("Get gene regions containing positive sites ... ")
        gp2lib.intersect_bed_files(gene_regions_gtf_tmp_bed, args.in_sites, params, regions_with_positives_bed)
        # Filter positives by gene coverage.
        if args.no_gene_filter:
            gp2lib.make_file_copy(tmp2_bed, tmp3_bed)
        else:
            # Accept only --in sites that overlap with gene regions.
            print("Filter positive sites by gene overlap ... ")
            params = "-u -s"
            gp2lib.intersect_bed_files(tmp2_bed, gene_regions_gtf_tmp_bed, params, tmp3_bed)
            c_out = gp2lib.count_file_rows(tmp3_bed)
            assert c_out, "no remaining positive sites after filtering by gene overlap"
            print("# positive sites after filtering by gene coverage:  %i" %(c_out))
    else:
        gp2lib.make_file_copy(tmp2_bed, tmp3_bed)


    """
    Revise positives
    ================

    - Store rows with original scores in dictionary id2row_dic
    - Make BED col5 scores = 0 for sequence extraction.
    - Apply --con-ext extension (if set) unless --con-ext-pre was enabled.
    - if --neg-in, revise --neg-in same way.

    """

    id2row_dic = gp2lib.revise_in_sites(tmp3_bed, positives_with_context_bed,
                                        chr_len_dic, id2pl_dic, args,
                                        transcript_regions=transcript_regions)
    assert id2row_dic, "no positive sites remaining after revision"
    if args.in_neg_sites:
        id2row_neg_dic = gp2lib.revise_in_sites(neg_tmp1_bed, negatives_with_context_bed,
                                                chr_len_dic, id2pl_dic, args,
                                                transcript_regions=transcript_regions)
        assert id2row_neg_dic, "no negative sites (--neg-in) remaining after revision"
        print("# selected positive sites:  %i" %(len(id2row_dic)))
        print("# selected negative sites:  %i" %(len(id2row_neg_dic)))
    else:
        print("# selected positive sites to generate negatives for:  %i" %(len(id2row_dic)))

    """
    Generate negatives
    ==================

    - Collect files with regions for masking and merge them.
    - Generate a file with sites to shuffle (use args.neg_factor
      times input sites) (shuffle_in_bed)
    - Use bedtools shuffle to generate random negatives in the
      regions of interest, without overlap with masked regions.
      In case # of random negatives != requested number, retry
      with all most prominent transcripts (transcript_regions=True),
      or all genes with TSL1 transcripts as search space
      (if --in sites are genomic).
    - Make random negative IDs unique.
    - Read in positive + negative region lengths.

    """
    # If requested number of negatives could not be extracted.
    round_two = False
    if not args.in_neg_sites:
        # Collect masking files and merge them to BED.
        print("Collect regions for masking (== no negatives from these regions) ... ")
        mask_files_list = []
        mask_files_list.append(args.in_sites)
        mask_files_list.append(positives_with_context_bed)
        if args.mask_bed:
            mask_files_list.append(mask_bed)
        gp2lib.merge_files(mask_files_list, mask_negatives_bed)
        # Generate shuffle file.
        shuffle_list = []
        # Generate random negatives, neg_factor times number of positives.
        for i in range(args.neg_factor):
            shuffle_list.append(positives_with_context_bed)
        gp2lib.merge_files(shuffle_list, shuffle_in_bed)

        # Generate negatives.
        print("Extract random negative regions ... ")
        check = gp2lib.bed_generate_random_negatives(shuffle_in_bed, chr_lengths_file, random_negatives_bed,
                                                     incl_bed=regions_with_positives_bed,
                                                     excl_bed=mask_negatives_bed)
        if not check:
            print("# random negatives extracted < # requested. Expand extraction space ... ")
            round_two = True
            if transcript_regions:
                # Get most prominent transcript regions.
                print("Get most prominent transcripts (TSL1-5) from --gtf ... ")
                mp_ids_dic = {}
                gp2lib.gtf_extract_most_prominent_transcripts(args.in_gtf, "dummy",
                                                              return_ids_dic=mp_ids_dic,
                                                              set_ids_dic=chr_ids_dic,
                                                              strict=True)
                assert mp_ids_dic, "no transcript IDs read into dictionary"
                # Output most prominent transcripts to BED.
                gp2lib.bed_sequence_lengths_to_bed(mp_ids_dic, most_prominent_transcripts_bed)
                # Rerun.
                print("Extract random negatives from most prominent transcripts ... ")
                check = gp2lib.bed_generate_random_negatives(shuffle_in_bed, chr_lengths_file, random_negatives_bed,
                                                     incl_bed=most_prominent_transcripts_bed,
                                                     excl_bed=mask_negatives_bed)
                # If still not enough, terminate.
                assert check, "still not enough random negatives after second round with most prominent transcripts (try with lower --neg-factor)"
                # Get list of transcript IDs with negative sites.
                neg_ids_dic = gp2lib.bed_get_chromosome_ids(random_negatives_bed)
                # Update transcript sequences dictionary.
                print("Get transcript sequences containing negative sites ... ")
                neg_tr_seqs_dic = gp2lib.get_transcript_sequences_from_gtf(args.in_gtf, args.in_2bit,
                                                                           lc_repeats=args.rep_reg_annot,
                                                                           tr_ids_dic=neg_ids_dic)
            else:
                # Get TSL gene regions.
                print("Get gene regions with TSL transcript support from --gtf ... ")
                gp2lib.gtf_extract_tsl_gene_bed(args.in_gtf, most_prominent_genes_bed,
                                                strict=True)
                # Rerun.
                print("Extract random negatives from gene regions with TSL transcript support ... ")
                check = gp2lib.bed_generate_random_negatives(shuffle_in_bed, chr_lengths_file, random_negatives_bed,
                                                     incl_bed=most_prominent_genes_bed,
                                                     excl_bed=mask_negatives_bed)
                # If still not enough, terminate.
                assert check, "still not enough random negatives after second round with most prominent genes (try with lower --neg-factor)"

        # Make unique IDs for extracted random negatives.
        new2oldid_dic = {}
        print("Make random negative IDs unique ... ")
        gp2lib.bed_process_bed_file(random_negatives_bed, random_negatives_unique_ids_bed,
                                    new2oldid_dic=new2oldid_dic,
                                    id_prefix="rand_neg",
                                    generate_unique_ids=True)

        # Get negative .bed rows.
        id2row_neg_dic = gp2lib.bed_read_rows_into_dic(random_negatives_unique_ids_bed)

        # Check if negative regions were indeed selected based on incl_bed excl_bed.
        check_shuffle=True
        if check_shuffle:
            print("Checking bedtools shuffle output ... ")
            print("Look out for negative regions not fully overlapping with -incl or reference ...")
            # Get IDs for these regions.
            trouble_ids_dic = {}
            check_incl_bed = regions_with_positives_bed
            if round_two:
                if transcript_regions:
                    check_incl_bed = most_prominent_transcripts_bed
                else:
                    check_incl_bed = most_prominent_genes_bed
            warning = gp2lib.check_random_negatives(random_negatives_unique_ids_bed, check_incl_bed,
                                                    mask_negatives_bed, chr_lengths_file,
                                                    trouble_ids_dic=trouble_ids_dic,
                                                    report=False)
            if warning:
                print("WARNING: bedtools shuffle output contains such regions. Removing ...")
                assert trouble_ids_dic, "no IDs in trouble_ids_dic to remove negative regions"
                c_del = 0
                for neg_id in trouble_ids_dic:
                    del id2row_neg_dic[neg_id]
                    c_del += 1
                print("# regions removed due to bedtools shuffle behaving badly:  %i" %(c_del))
                print("CALEB: you behave now ... ")
                # Overwrite random_negatives_unique_ids_bed.
                gp2lib.bed_write_row_dic_into_file(id2row_neg_dic, random_negatives_unique_ids_bed)

    """
    Extract positive and negative sequences
    =======================================

    - For transcript sites, extract from sequence dictionary.
    - For genomic sites, extract from --gen in_2bit.
    - For genomic sites, delete sites with lengths != extracted sequence lengths
    - Remove N containing sequences / sites in both sets
    - Remove low complexity negatives (unless negatives given by --neg-in)

    """

    # Get region lengths.
    pos_bed_len_dic = gp2lib.bed_get_region_lengths(positives_with_context_bed)
    if not args.in_neg_sites:
        negatives_with_context_bed = random_negatives_unique_ids_bed
    neg_bed_len_dic = gp2lib.bed_get_region_lengths(negatives_with_context_bed)

    if transcript_regions:
        # Get sequences for pos/neg transcript sites.
        print("Extract sequences for transcript sites ... ")
        pos_context_seqs_dic = gp2lib.extract_transcript_sequences(id2row_dic, tr_seqs_dic)
        if round_two:
            neg_context_seqs_dic = gp2lib.extract_transcript_sequences(id2row_neg_dic, neg_tr_seqs_dic)
        else:
            neg_context_seqs_dic = gp2lib.extract_transcript_sequences(id2row_neg_dic, tr_seqs_dic)
    else:
        # Get sequences for pos/neg genomic sites.
        # Read in positive region lengths.
        print("Extract sequences for genomic sites ... ")
        # Extract positive sequences from .2bit.
        gp2lib.bed_extract_sequences_from_2bit(positives_with_context_bed, pos_tmp_fasta, args.in_2bit,
                                               lc_repeats=args.rep_reg_annot)
        # Extract negative sequences from .2bit.
        gp2lib.bed_extract_sequences_from_2bit(negatives_with_context_bed, neg_tmp_fasta, args.in_2bit,
                                               lc_repeats=args.rep_reg_annot)
        # Read in sequences.
        pos_context_seqs_dic = gp2lib.read_fasta_into_dic(pos_tmp_fasta,
                                                          skip_data_id="positive",
                                                          report=2)
        neg_context_seqs_dic = gp2lib.read_fasta_into_dic(neg_tmp_fasta,
                                                          skip_data_id="negative",
                                                          report=2)

    # Compare region lengths with sequence lengths, delete mismatches.
    print("Compare extracted sequence lengths to site lengths ... ")

    # IDs to delete lists.
    pos_del_list = []
    neg_del_list = []
    for neg_id in neg_context_seqs_dic:
        nseq_l = len(neg_context_seqs_dic[neg_id])
        nreg_l = neg_bed_len_dic[neg_id]
        if nseq_l != nreg_l:
            neg_del_list.append(neg_id)

    for pos_id in pos_context_seqs_dic:
        pseq_l = len(pos_context_seqs_dic[pos_id])
        preg_l = pos_bed_len_dic[pos_id]
        if pseq_l != preg_l:
            pos_del_list.append(pos_id)
    # Delete mismatches.
    for reg_id in pos_del_list:
        del pos_context_seqs_dic[reg_id]
    for reg_id in neg_del_list:
        del neg_context_seqs_dic[reg_id]

    c_pos_del = len(pos_del_list)
    c_neg_del = len(neg_del_list)
    c_pos_rem = len(pos_context_seqs_dic)
    c_neg_rem = len(neg_context_seqs_dic)

    assert neg_context_seqs_dic, "no negatives remaining after sequence vs region length comparison"
    assert pos_context_seqs_dic, "no positives remaining after sequence vs region length comparison"
    if args.in_neg_sites:
        if c_neg_rem < c_pos_rem:
            print("WARNING: less negatives than positives for dataset construction (#neg: %i, #pos: %i)" %(c_neg_rem, c_pos_rem))
    else:
        assert c_neg_rem > c_pos_rem, "not enough random negatives for dataset construction (#neg: %i, #pos: %i)" %(c_neg_rem, c_pos_rem)
    if c_pos_del:
        print("# positives removed due to region vs sequence length difference:  %i" %(c_pos_del))
    if c_neg_del:
        print("# negatives removed due to region vs sequence length difference:  %i" %(c_neg_del))

    # Remove N containing sequences.
    print("Remove N containing sequences ... ")
    pos_del_list = []
    neg_del_list = []
    for neg_id in neg_context_seqs_dic:
        if re.search("N", neg_context_seqs_dic[neg_id], re.I):
            neg_del_list.append(neg_id)
    for pos_id in pos_context_seqs_dic:
        if re.search("N", pos_context_seqs_dic[pos_id], re.I):
            pos_del_list.append(pos_id)
    # Delete mismatches.
    for reg_id in pos_del_list:
        del pos_context_seqs_dic[reg_id]
    for reg_id in neg_del_list:
        del neg_context_seqs_dic[reg_id]
    # Check / summarize.
    c_pos_del = len(pos_del_list)
    c_neg_del = len(neg_del_list)
    if c_pos_del:
        print("# N containing positives removed:  %i" %(c_pos_del))
    if c_neg_del:
        print("# N containing negatives removed:  %i" %(c_neg_del))
    assert neg_context_seqs_dic, "no negatives remaining after removing N containing ones"
    assert pos_context_seqs_dic, "no positives remaining after removing N containing ones"

    # Remove low complexity negatives.
    if args.in_neg_sites:
        print("Do not remove low complexity negative sequences if --neg-in ... ")
    else:
        neg_del_list = []
        print("Remove low complexity negative sequences (--neg-comp-thr %f) ... " %(args.neg_comp_thr))
        for neg_id in neg_context_seqs_dic:
            check_seq = neg_context_seqs_dic[neg_id].upper()
            if args.con_ext:
                pos_id = new2oldid_dic[neg_id]
                uslc_len = 0
                dslc_len = 0
                # Compare lengths.
                pos_l = len(pos_context_seqs_dic[pos_id])
                neg_l = len(neg_context_seqs_dic[neg_id])
                # For now throw an error if lengths not the same.
                assert pos_l == neg_l, "positive site length != corresponding negative site length (%i != %i)" %(pos_l, neg_l)
                neg_seq = neg_context_seqs_dic[neg_id]
                assert pos_id in id2pl_dic, "missing site ID \"%s\" in uppercase region length dictionary" %(pos_id)
                uslc_len = id2pl_dic[pos_id][0]
                uc_len = id2pl_dic[pos_id][1]
                dslc_len = id2pl_dic[pos_id][2]
                uc_s = uslc_len
                uc_e = uslc_len + uc_len
                check_seq = neg_seq[uc_s:uc_e].upper()
            # Get nt frequencies.
            count_dic = gp2lib.seq_count_nt_freqs(check_seq, rna=True)
            # Calculate sequence entropy.
            seq_entr = gp2lib.calc_seq_entropy(len(check_seq), count_dic)
            if seq_entr < args.neg_comp_thr:
                #print("Removing %s: %s (%f)" %(neg_id, check_seq, seq_entr))
                neg_del_list.append(neg_id)
        # Remove.
        for neg_id in neg_del_list:
            del neg_context_seqs_dic[neg_id]
        c_neg_del = len(neg_del_list)
        if c_neg_del:
            print("# low complexity negatives removed:  %i" %(c_neg_del))
        assert neg_context_seqs_dic, "no negatives remaining after removing low complexity ones"

        # Remaining negatives should still be > remaining positives.
        c_pos_rem = len(pos_context_seqs_dic)
        c_neg_rem = len(neg_context_seqs_dic)
        assert c_neg_rem > c_pos_rem, "not enough random negatives for dataset construction (#neg: %i, #pos: %i). Try with higher --neg-factor" %(c_neg_rem, c_pos_rem)

    """
    Select negatives
    ================

    - If --keep-add-neg is set, take all extracted negatives
    - If not, select same number as positives (random selection)

    """

    # Select same number of negative sites.
    if not args.keep_add_neg and not args.in_neg_sites:
        print("Randomly select subset of negatives (# == # positives) from extracted negatives ... ")
        # Randomly select negative IDs and store in list.
        rand_ids_list = gp2lib.random_order_dic_keys_into_list(neg_context_seqs_dic)
        # Number of positives = number of negatives.
        c_pos = len(pos_context_seqs_dic)
        # Go over remaining negative IDs (all - c_pos) and delete them.
        for neg_id in rand_ids_list[c_pos:]:
            del neg_context_seqs_dic[neg_id]

    """
    Create lowercase sequence context
    =================================

    - Make upstream + downstream sequence context lowercase.
      Upstream lowercase, uppercase, and downstream lowercase region lengths
      for each site ID are stored in id2pl_dic.
      For negatives, use new2oldid_dic for new random negative ID mapping
      to old associated positive ID, from which to get uppercase+lowercase
      lengths.
    - Output sequences + sites to finish FASTA+BED extraction part.

    """

    # Dictionaries for storing original lowercase+uppercase information (if --rep-reg).
    pos_context_seqs_rep_lc_dic = {}
    neg_context_seqs_rep_lc_dic = {}
    # Store uppercase start and ends for sequence IDs.
    id2ucr_dic = {}
    if args.rep_reg_annot:
        # Store lowercase info in dics, then make sequences uppercase again.
        for pos_id in pos_context_seqs_dic:
            seq = pos_context_seqs_dic[pos_id]
            pos_context_seqs_rep_lc_dic[pos_id] = seq
            pos_context_seqs_dic[pos_id] = seq.upper()
            l_seq = len(seq)
        for neg_id in neg_context_seqs_dic:
            seq = neg_context_seqs_dic[neg_id]
            neg_context_seqs_rep_lc_dic[neg_id] = seq
            neg_context_seqs_dic[neg_id] = seq.upper()
            l_seq = len(seq)

    if args.con_ext:
        print("Make site sequence context (--con-ext) lowercase ... ")
        # Process positive sequences.
        for pos_id in pos_context_seqs_dic:
            assert pos_id in id2pl_dic, "positive site ID \"%s\" missing in id2pl_dic" %(pos_id)
            uslc_len = id2pl_dic[pos_id][0]
            uc_len = id2pl_dic[pos_id][1]
            dslc_len = id2pl_dic[pos_id][2]
            seq = pos_context_seqs_dic[pos_id]
            uc_s = uslc_len + 1
            uc_e = uslc_len + uc_len
            new_seq = gp2lib.add_lowercase_context_to_sequences(seq, uc_s, uc_e,
                                                      convert_to_rna=True)
            pos_context_seqs_dic[pos_id] = new_seq
            id2ucr_dic[pos_id] = [uc_s, uc_e]

        # Process negative sequences.
        for neg_id in neg_context_seqs_dic:
            if args.in_neg_sites:
                assert neg_id in id2pl_dic, "negative site ID \"%s\" missing in id2pl_dic" %(neg_id)
                uslc_len = id2pl_dic[neg_id][0]
                uc_len = id2pl_dic[neg_id][1]
                dslc_len = id2pl_dic[neg_id][2]
                seq = neg_context_seqs_dic[neg_id]
                uc_s = uslc_len + 1
                uc_e = uslc_len + uc_len
                new_seq = gp2lib.add_lowercase_context_to_sequences(seq, uc_s, uc_e,
                                                          convert_to_rna=True)
                neg_context_seqs_dic[neg_id] = new_seq
                id2ucr_dic[neg_id] = [uc_s, uc_e]
            else:
                pos_id = new2oldid_dic[neg_id]
                assert pos_id in id2pl_dic, "positive site ID \"%s\" missing in id2pl_dic" %(pos_id)
                uslc_len = id2pl_dic[pos_id][0]
                uc_len = id2pl_dic[pos_id][1]
                dslc_len = id2pl_dic[pos_id][2]
                # Compare lengths.
                pos_l = len(pos_context_seqs_dic[pos_id])
                neg_l = len(neg_context_seqs_dic[neg_id])
                # For now throw an error if lengths not the same.
                assert pos_l == neg_l, "positive site length != corresponding negative site length (%i != %i)" %(pos_l, neg_l)
                seq = neg_context_seqs_dic[neg_id]
                uc_s = uslc_len + 1
                uc_e = uslc_len + uc_len
                new_seq = gp2lib.add_lowercase_context_to_sequences(seq, uc_s, uc_e,
                                                          convert_to_rna=True)
                neg_context_seqs_dic[neg_id] = new_seq
                id2ucr_dic[neg_id] = [uc_s, uc_e]

    # Length statistics for final positives + negatives.
    final_pos_len_list = []
    final_neg_len_list = []
    final_pos_len_list = gp2lib.get_seq_len_list_from_dic(pos_context_seqs_dic)
    final_neg_len_list = gp2lib.get_seq_len_list_from_dic(neg_context_seqs_dic)
    c_final_pos = len(final_pos_len_list)
    c_final_neg = len(final_neg_len_list)

    # Output full-length (+ context) sequences.
    print("Output selected positives + negatives to BED and FASTA ... ")
    gp2lib.fasta_output_dic(pos_context_seqs_dic, pos_fasta_out)
    gp2lib.fasta_output_dic(neg_context_seqs_dic, neg_fasta_out)
    # Output BED regions.
    gp2lib.bed_write_row_dic_into_file(id2row_dic, pos_bed_out,
                                       id2out_dic=pos_context_seqs_dic)
    gp2lib.bed_write_row_dic_into_file(id2row_neg_dic, neg_bed_out,
                                       id2out_dic=neg_context_seqs_dic)
    # Delete negative entries not selected for final set.
    del_ids = []
    for neg_id in id2row_neg_dic:
        if neg_id not in neg_context_seqs_dic:
            del_ids.append(neg_id)
    for neg_id in del_ids:
        del id2row_neg_dic[neg_id]

    # Report numbers.
    neg_pos_ratio = c_final_neg / c_final_pos
    print("Obtained negatives to positives ratio:  %.3f" %(neg_pos_ratio))
    print("# positive training sites output:       %i" %(c_final_pos))
    print("# negative training sites output:       %i" %(c_final_neg))

    """
    Calculate additional features
    =============================

    Currently these are:
    structure (for sequence, genomic regions, and transcript regions)
    transcript region annotation (for genomic and transcript regions)
    exon-intron annotation (for genomic regions)
    conservation scores (for genomic and transcript regions)
    repeat region information (for genomic and transcript regions)

    1) structure (--str):
    To calculate probabilities of base pairs + structural elements.
    .bpp.str: base pairs + probabilities file
    .str_elem.str: unpaired + structural elements probabilities
    Note that structure calculation yields same results for uppercase
    and lowercase sequences (mixed ones too), so no separate upper-
    lowercase sequence files necessary.

    2) conservation scores (--phastcons, --phylop):
    Position-wise phyloP and / or phastCons scores to add for each
    site position.
    .pc.con : phastCons position-wise conservation scores file
    .pp.con : phyloP position-wise conservation scores file

    3) transcript region annotation (--tra, --tra-codons, --tra-borders)
    Add transcript region label annotations from --gtf to genomic or
    transcript sites.
    --tra: add 5'UTR (F), CDS (C), and 3'UTR (T) annotation to transcript
    or genomic sites.
    --tra-codons: add start (S) and stop (E) codon annotation to transcript
    or genomic sites.
    --tra-borders: add transcript start (A), transcript end (Z) nt, and
    exon border (B) annotation to transcript sites.

    4) exon-intron annotation (--eia, --eia-ib, --eia-n):
    For genomic sites only.
    Position-wise exon + intron annotation, for each genomic site position
    .eia : stores E or I labels for each site position
    If --eia-n, additional N label for regions not overlapping with
    transcripts.
    If --eia-ib, intron border labels (F: 5' intron start, T: 3' intron end)

    5) repeat region annotation (--rra)
    Add position-wise R (repeat) or N (no repeat) annotations to label
    repeat and non-repeat region sequences.
    Repeat region information is stored in --gen .2bit, with lower-case
    nucleotides belonging to repeat regions annotated by RepeatMasker and
    Tandem Repeats Finder (with period of 12 or less).
    """

    # Additional feature stats dictionaries.
    pos_str_stats_dic = None
    neg_str_stats_dic = None
    pos_phastcons_stats_dic = None
    neg_phastcons_stats_dic = None
    pos_phylop_stats_dic = None
    neg_phylop_stats_dic = None
    pos_eia_stats_dic = None
    neg_eia_stats_dic = None
    pos_tra_stats_dic = None
    neg_tra_stats_dic = None
    pos_rra_stats_dic = None
    neg_rra_stats_dic = None
    if args.report and args.add_str:
        pos_str_stats_dic = {}
        neg_str_stats_dic = {}
    if args.report and args.pc_bw:
        pos_phastcons_stats_dic = {}
        neg_phastcons_stats_dic = {}
    if args.report and args.pp_bw:
        pos_phylop_stats_dic = {}
        neg_phylop_stats_dic = {}
    if args.report and args.exon_intron_annot:
        pos_eia_stats_dic = {}
        neg_eia_stats_dic = {}
    if args.report and args.tr_reg_annot:
        pos_tra_stats_dic = {}
        neg_tra_stats_dic = {}
    if args.report and args.rep_reg_annot:
        pos_rra_stats_dic = {}
        neg_rra_stats_dic = {}

    # Store feature infos.
    if args.con_ext:
        FEATOUT.write("fa\tC\tA,C,G,U,a,c,g,u\t-\n")
    else:
        FEATOUT.write("fa\tC\tA,C,G,U\t-\n")
    if args.add_str:
        FEATOUT.write("bpp.str\tN\tbp_prob\tprob\n")
        FEATOUT.write("elem_p.str\tN\tp_u,p_e,p_h,p_i,p_m,p_s\tprob\n")
    if args.pc_bw:
        FEATOUT.write("pc.con\tN\tphastcons_score\tprob\n")
    if args.pp_bw:
        FEATOUT.write("pp.con\tN\tphylop_score\tminmax2\n")
    if args.tr_reg_annot:
        ll = ["F", "C", "N", "T"] # label list.
        if args.tr_reg_border_annot:
            ll.append("A")
            ll.append("Z")
            ll.append("B")
        if args.tr_reg_codon_annot:
            ll.append("S")
            ll.append("E")
        ll.sort()
        lls = ",".join(ll)
        FEATOUT.write("tra\tC\t%s\t-\n" %(lls))
    if args.exon_intron_annot:
        ll = ["E", "I"]
        if args.intron_border_annot:
            ll.append("T")
            ll.append("F")
        if args.exon_intron_n:
            ll.append("N")
        ll.sort()
        lls = ",".join(ll)
        FEATOUT.write("eia\tC\t%s\t-\n" %(lls))
    if args.rep_reg_annot:
        FEATOUT.write("rra\tC\tN,R\t-\n")
    FEATOUT.close()

    """
    1) Secondary structure features.

    """
    # Secondary structure stats dictionaries.

    if args.add_str:
        """
        # Calculate structure features for positives.
        print("Get secondary structure information for positives ... ")
        gp2lib.calc_str_elem_up_bpp(pos_fasta_out, pos_bpp_out, pos_str_elem_p_out,
                                    stats_dic=pos_str_stats_dic,
                                    id2ucr_dic=id2ucr_dic,
                                    plfold_u=args.plfold_u,
                                    plfold_l=args.plfold_l,
                                    plfold_w=args.plfold_w)
        # Calculate structure features for negatives.
        print("Get secondary structure information for negatives ... ")
        gp2lib.calc_str_elem_up_bpp(neg_fasta_out, neg_bpp_out, neg_str_elem_p_out,
                                    stats_dic=neg_str_stats_dic,
                                    id2ucr_dic=id2ucr_dic,
                                    plfold_u=args.plfold_u,
                                    plfold_l=args.plfold_l,
                                    plfold_w=args.plfold_w)
        """
        # Calculate structure features for positives.
        print("Get secondary structure information for positives ... ")
        gp2lib.calc_ext_str_features(id2row_dic, chr_len_dic,
                                     pos_bpp_out, pos_str_elem_p_out, args,
                                     id2ucr_dic=id2ucr_dic,
                                     stats_dic=pos_str_stats_dic,
                                     bp_check_seqs_dic=pos_context_seqs_dic,
                                     tr_regions=transcript_regions,
                                     tr_seqs_dic=tr_seqs_dic)
        # If custom base pair information given.
        if args.bp_in:
            # Sanity check, process, and output custom base pair information file.
            print("--bp-in file supplied. Check, process + overwrite positive base pairs ... ")
            gp2lib.process_custom_bp_file(args.bp_in, pos_bpp_out, pos_context_seqs_dic,
                                          stats_dic=pos_str_stats_dic)
        # Calculate structure features for negatives.
        if transcript_regions and round_two:
            tr_seqs_dic = neg_tr_seqs_dic
        print("Get secondary structure information for negatives ... ")
        gp2lib.calc_ext_str_features(id2row_neg_dic, chr_len_dic,
                                     neg_bpp_out, neg_str_elem_p_out, args,
                                     id2ucr_dic=id2ucr_dic,
                                     stats_dic=neg_str_stats_dic,
                                     bp_check_seqs_dic=neg_context_seqs_dic,
                                     tr_regions=transcript_regions,
                                     tr_seqs_dic=tr_seqs_dic)

    """
    2) Conservation scores.

    """
    # More preprocessing if transcript_regions.
    if transcript_regions:
        if args.pc_bw or args.pp_bw or args.tr_reg_annot:
            print("Additional annotations for transcript sites require mapping to genome ... ")
            print("Mapping transcript sites to genome ... ")
            posid2hitc_dic = {}
            negid2hitc_dic = {}
            gp2lib.bed_convert_transcript_to_genomic_sites(pos_bed_out, args.in_gtf,
                                                           pos_transcript_to_genome_bed,
                                                           site2hitc_dic=posid2hitc_dic)
            gp2lib.bed_convert_transcript_to_genomic_sites(neg_bed_out, args.in_gtf,
                                                           neg_transcript_to_genome_bed,
                                                           site2hitc_dic=negid2hitc_dic)

    # If phastCons conservation scores (--pc-bw) are given.
    if args.pc_bw:
        if transcript_regions:
            print("Extracting phastCons conservation scores for transcript sites ... ")
            # For transcript sites.
            gp2lib.extract_conservation_scores(pos_transcript_to_genome_bed,
                                               pos_pc_con_out, args.pc_bw,
                                               stats_dic=pos_phastcons_stats_dic,
                                               id2ucr_dic=id2ucr_dic,
                                               merge_split_regions=True,
                                               report=False)
            gp2lib.extract_conservation_scores(neg_transcript_to_genome_bed,
                                               neg_pc_con_out, args.pc_bw,
                                               stats_dic=neg_phastcons_stats_dic,
                                               id2ucr_dic=id2ucr_dic,
                                               merge_split_regions=True,
                                               report=False)
        else:
            print("Extracting phastCons conservation scores for genomic sites ... ")
            # For genomic sites.
            gp2lib.extract_conservation_scores(pos_bed_out,
                                               pos_pc_con_out, args.pc_bw,
                                               stats_dic=pos_phastcons_stats_dic,
                                               id2ucr_dic=id2ucr_dic,
                                               merge_split_regions=False,
                                               report=False)
            gp2lib.extract_conservation_scores(neg_bed_out,
                                               neg_pc_con_out, args.pc_bw,
                                               stats_dic=neg_phastcons_stats_dic,
                                               id2ucr_dic=id2ucr_dic,
                                               merge_split_regions=False,
                                               report=False)

    # If phastCons conservation scores (--pp-bw) are given.
    if args.pp_bw:
        if transcript_regions:
            print("Extracting phyloP conservation scores for transcript sites ... ")
            # For transcript sites.
            gp2lib.extract_conservation_scores(pos_transcript_to_genome_bed,
                                               pos_pp_con_out, args.pp_bw,
                                               stats_dic=pos_phylop_stats_dic,
                                               id2ucr_dic=id2ucr_dic,
                                               merge_split_regions=True,
                                               report=False)
            gp2lib.extract_conservation_scores(neg_transcript_to_genome_bed,
                                               neg_pp_con_out, args.pp_bw,
                                               stats_dic=neg_phylop_stats_dic,
                                               id2ucr_dic=id2ucr_dic,
                                               merge_split_regions=True,
                                               report=False)
        else:
            print("Extracting phyloP conservation scores for genomic sites ... ")
            # For genomic sites.
            gp2lib.extract_conservation_scores(pos_bed_out,
                                               pos_pp_con_out, args.pp_bw,
                                               stats_dic=pos_phylop_stats_dic,
                                               id2ucr_dic=id2ucr_dic,
                                               merge_split_regions=False,
                                               report=False)
            gp2lib.extract_conservation_scores(neg_bed_out,
                                               neg_pp_con_out, args.pp_bw,
                                               stats_dic=neg_phylop_stats_dic,
                                               id2ucr_dic=id2ucr_dic,
                                               merge_split_regions=False,
                                               report=False)
        print("Normalizing phyloP scores ... ")
        gp2lib.phylop_norm_train_scores(pos_pp_con_out, neg_pp_con_out)

    """
    3) Transcript region annotations for transcript sites.

    """
    if args.tr_reg_annot and transcript_regions:
        print("Extracting transcript region annotations for transcript sites ... ")
        if args.tr_reg_codon_annot:
            print("Start + stop codon annotations enabled ... ")
        if args.tr_reg_border_annot:
            print("Transcript + exon border annotations enabled ... ")
        tr_ids_dic = chr_len_dic
        if round_two:
            tr_ids_dic = mp_ids_dic
        gp2lib.bed_get_transcript_annotations_from_gtf(tr_ids_dic,
                                            pos_transcript_to_genome_bed,
                                            args.in_gtf, pos_tra_out,
                                            stats_dic=pos_tra_stats_dic,
                                            id2ucr_dic=id2ucr_dic,
                                            codon_annot=args.tr_reg_codon_annot,
                                            border_annot=args.tr_reg_border_annot,
                                            merge_split_regions=True)
        gp2lib.bed_get_transcript_annotations_from_gtf(tr_ids_dic,
                                            neg_transcript_to_genome_bed,
                                            args.in_gtf, neg_tra_out,
                                            stats_dic=neg_tra_stats_dic,
                                            id2ucr_dic=id2ucr_dic,
                                            codon_annot=args.tr_reg_codon_annot,
                                            border_annot=args.tr_reg_border_annot,
                                            merge_split_regions=True)

    """
    4) Exon-intron + transcript region annotations for genomic sites.

    """
    if (args.exon_intron_annot or args.tr_reg_annot) and not transcript_regions:
        print("Get transcript annotations for genomic sites ... ")
        # Get transcript IDs to use for exon-intron or region annotation.
        tr_ids_dic = {}
        if args.tr_list:
            print("Using transcript list from --tr-list to define transcript regions ... ")
            # Read in transcript IDs from --tr-list.
            tr_ids_dic = gp2lib.read_ids_into_dic(args.tr_list)
            # Read in all transcript IDs in --gtf.
            all_tr_ids_dic = gp2lib.gtf_get_transcript_ids(args.in_gtf)
            # Check if given --tr list IDs are in --gtf.
            for tr_id in tr_ids_dic:
                if tr_id not in all_tr_ids_dic:
                    print("WARNING: transcript ID \"%s\" from --tr-list not found in --gtf. Ignoring transcript ... " %(tr_id))
        else:
            # Create exon BED file from most prominent transcripts.
            strict=False
            if strict:
                print("Get most prominent transcript for each gene from --gtf (TSL1-5 only)... ")
            else:
                print("Get most prominent transcript for each gene from --gtf ... ")
            # Don't be strict (!= only TSL1-5 allowed).
            tr_ids_dic = gp2lib.gtf_extract_most_prominent_transcripts(args.in_gtf,
                                            "dummy",
                                            strict=strict,
                                            return_ids_dic=tr_ids_dic)

        # Exon-intron annotations for genomic sites.
        if args.exon_intron_annot:
            print("Extract exon-intron annotations for genomic sites ... ")
            if args.tr_list:
                print("Use exon regions of --tr-list transcripts from --gtf ... ")
            else:
                print("Use exon regions of most prominent transcripts from --gtf ... ")
            if args.intron_border_annot:
                print("Intron border annotation enabled ... ")
            if args.exon_intron_n:
                print("Non-intron non-exon region annotation enabled ... ")
            # Positives.
            gp2lib.bed_get_exon_intron_annotations_from_gtf(tr_ids_dic, pos_bed_out,
                                        args.in_gtf, pos_eia_out,
                                        stats_dic=pos_eia_stats_dic,
                                        id2ucr_dic=id2ucr_dic,
                                        n_labels=args.exon_intron_n,
                                        intron_border_labels=args.intron_border_annot)
            # Negatives.
            gp2lib.bed_get_exon_intron_annotations_from_gtf(tr_ids_dic, neg_bed_out,
                                        args.in_gtf, neg_eia_out,
                                        stats_dic=neg_eia_stats_dic,
                                        id2ucr_dic=id2ucr_dic,
                                        n_labels=args.exon_intron_n,
                                        intron_border_labels=args.intron_border_annot)

        # Transcript region annotations for genomic sites.
        if args.tr_reg_annot:
            print("Extracting transcript region annotations for genomic sites ... ")
            if args.tr_reg_codon_annot:
                print("Start + stop codon annotations enabled ... ")
            # Positives.
            gp2lib.bed_get_transcript_annotations_from_gtf(tr_ids_dic,
                                                pos_bed_out,
                                                args.in_gtf, pos_tra_out,
                                                stats_dic=pos_tra_stats_dic,
                                                id2ucr_dic=id2ucr_dic,
                                                codon_annot=args.tr_reg_codon_annot,
                                                border_annot=args.tr_reg_border_annot,
                                                merge_split_regions=False)
            # Negatives.
            gp2lib.bed_get_transcript_annotations_from_gtf(tr_ids_dic,
                                                neg_bed_out,
                                                args.in_gtf, neg_tra_out,
                                                stats_dic=neg_tra_stats_dic,
                                                id2ucr_dic=id2ucr_dic,
                                                codon_annot=args.tr_reg_codon_annot,
                                                border_annot=args.tr_reg_border_annot,
                                                merge_split_regions=False)

    """
    5) Repeat region annotations for genomic and transcript sites.

    """
    if args.rep_reg_annot:
        print("Extracting repeat region annotations for positives ... ")
        gp2lib.fasta_get_repeat_region_annotations(pos_context_seqs_rep_lc_dic, pos_rra_out,
                                                   stats_dic=pos_rra_stats_dic,
                                                   id2ucr_dic=id2ucr_dic)
        print("Extracting repeat region annotations for negatives ... ")
        gp2lib.fasta_get_repeat_region_annotations(neg_context_seqs_rep_lc_dic, neg_rra_out,
                                                   stats_dic=neg_rra_stats_dic,
                                                   id2ucr_dic=id2ucr_dic)


    """
    Generate HTML report including various training dataset statistics.

    """
    dataset_type =  "g"
    if transcript_regions:
        dataset_type =  "t"

    if args.report:
        # Gene biotype statistics for target genes.
        gbt_stats = True
        # All gene biotype counts.
        all_gbtc_dic = {}
        # Target gene biotype counts.
        target_gbtc_dic = {}
        # Target region to hit count (transcript or gene ID -> hit count).
        t2hc_dic = {}
        # Target region to info.
        t2i_dic = {}
        if gbt_stats:
            # Calculate additional stats for report.
            print("Extract additional statistics for report ... ")
            # Get count stats for gene biotypes.
            region_ids_dic = gp2lib.bed_get_region_ids(regions_with_positives_bed)
            # Get gene biotype counts for target genes.
            if transcript_regions:
                target_gbtc_dic = gp2lib.gtf_get_gene_biotypes_from_transcript_ids(
                                            region_ids_dic, args.in_gtf,
                                            all_gbtc_dic=all_gbtc_dic)

                # Get transcript ID to hit count dictionary.
                t2hc_dic = gp2lib.bed_get_chromosome_ids(pos_bed_out)
                # Get transcript infos.
                t2i_dic = gp2lib.gtf_get_transcript_infos(region_ids_dic, args.in_gtf)
            else:
                target_gbtc_dic = gp2lib.gtf_get_gene_biotypes(
                                            region_ids_dic, args.in_gtf,
                                            all_gbtc_dic=all_gbtc_dic)
                # Get overlapping site counts for each gene.
                t2hc_dic = gp2lib.bed_intersect_count_region_overlaps(gene_regions_gtf_tmp_bed, pos_bed_out)
                # Get gene infos.
                t2i_dic = gp2lib.gtf_get_gene_infos(region_ids_dic, args.in_gtf)

        # HTML report output file.
        html_report_out = args.out_folder + "/" + "report.graphprot2_gt.html"
        # Plots subfolder.
        plots_subfolder = "plots_graphprot2_gt"
        # Get library and logo path.
        gp2lib_path = os.path.dirname(gp2lib.__file__)
        # Generate report.
        gp2lib.gp2_gt_generate_html_report(pos_context_seqs_dic, neg_context_seqs_dic,
                                           args.out_folder, dataset_type, gp2lib_path,
                                           html_report_out=html_report_out,
                                           id2ucr_dic=id2ucr_dic,
                                           plots_subfolder=plots_subfolder,
                                           pos_str_stats_dic=pos_str_stats_dic,
                                           neg_str_stats_dic=neg_str_stats_dic,
                                           pos_phastcons_stats_dic=pos_phastcons_stats_dic,
                                           neg_phastcons_stats_dic=neg_phastcons_stats_dic,
                                           pos_phylop_stats_dic=pos_phylop_stats_dic,
                                           neg_phylop_stats_dic=neg_phylop_stats_dic,
                                           pos_eia_stats_dic=pos_eia_stats_dic,
                                           neg_eia_stats_dic=neg_eia_stats_dic,
                                           pos_tra_stats_dic=pos_tra_stats_dic,
                                           neg_tra_stats_dic=neg_tra_stats_dic,
                                           pos_rra_stats_dic=pos_rra_stats_dic,
                                           neg_rra_stats_dic=neg_rra_stats_dic,
                                           target_gbtc_dic=target_gbtc_dic,
                                           all_gbtc_dic=all_gbtc_dic,
                                           t2hc_dic=t2hc_dic,
                                           t2i_dic=t2i_dic,
                                           kmer_top=10,
                                           target_top=20,
                                           theme=args.theme,
                                           rna=True,
                                           uc_entropy=True)

    """
    Take out the trash.

    """
    clean_up = True
    if clean_up:
        print("Removing temporary files ... ")
        # Remove tmp files.
        if os.path.exists(tmp1_bed):
            os.remove(tmp1_bed)
        if os.path.exists(tmp2_bed):
            os.remove(tmp2_bed)
        if os.path.exists(tmp3_bed):
            os.remove(tmp3_bed)
        if os.path.exists(neg_tmp1_bed):
            os.remove(neg_tmp1_bed)
        if os.path.exists(neg_tmp2_bed):
            os.remove(neg_tmp2_bed)
        if os.path.exists(neg_tmp3_bed):
            os.remove(neg_tmp3_bed)
        if os.path.exists(gene_regions_gtf_tmp_bed):
            os.remove(gene_regions_gtf_tmp_bed)
        if os.path.exists(positives_with_context_bed):
            os.remove(positives_with_context_bed)
        if os.path.exists(negatives_with_context_bed):
            os.remove(negatives_with_context_bed)
        if os.path.exists(mask_negatives_bed):
            os.remove(mask_negatives_bed)
        if os.path.exists(shuffle_in_bed):
            os.remove(shuffle_in_bed)
        if os.path.exists(random_negatives_bed):
            os.remove(random_negatives_bed)
        if os.path.exists(random_negatives_unique_ids_bed):
            os.remove(random_negatives_unique_ids_bed)
        if os.path.exists(pos_tmp_fasta):
            os.remove(pos_tmp_fasta)
        if os.path.exists(neg_tmp_fasta):
            os.remove(neg_tmp_fasta)
        if os.path.exists(most_prominent_transcripts_bed):
            os.remove(most_prominent_transcripts_bed)
        if os.path.exists(most_prominent_genes_bed):
            os.remove(most_prominent_genes_bed)
        if os.path.exists(pos_transcript_to_genome_bed):
            os.remove(pos_transcript_to_genome_bed)
        if os.path.exists(neg_transcript_to_genome_bed):
            os.remove(neg_transcript_to_genome_bed)

    """
    Output file summary.

    """
    print("")
    print("TRAINING SET OUTPUT FILES")
    print("=========================")
    print("Positives sequences .fa:\n%s" %(pos_fasta_out))
    print("Negatives sequences .fa:\n%s" %(neg_fasta_out))
    print("Positives sites .bed:\n%s" %(pos_bed_out))
    print("Negatives sites .bed:\n%s" %(neg_bed_out))
    if args.add_str:
        print("Positives base pair probabilities .bpp.str:\n%s" %(pos_bpp_out))
        print("Negatives base pair probabilities .bpp.str:\n%s" %(neg_bpp_out))
        print("Positives position-wise structural elements probabilities .elem_p.str:\n%s" %(pos_str_elem_p_out))
        print("Negatives position-wise structural elements probabilities .elem_p.str:\n%s" %(neg_str_elem_p_out))
    if args.pc_bw:
        print("Positives phastCons conservation scores .pc.con:\n%s" %(pos_pc_con_out))
        print("Negatives phastCons conservation scores .pc.con:\n%s" %(neg_pc_con_out))
    if args.pp_bw:
        print("Positives phyloP conservation scores .pp.con:\n%s" %(pos_pp_con_out))
        print("Negatives phyloP conservation scores .pp.con:\n%s" %(neg_pp_con_out))
    if args.exon_intron_annot:
        print("Positives exon intron region annotations .eia:\n%s" %(pos_eia_out))
        print("Negatives exon intron region annotations .eia:\n%s" %(neg_eia_out))
    if args.tr_reg_annot:
        print("Positives transcript region annotations .tra:\n%s" %(pos_tra_out))
        print("Negatives transcript region annotations .tra:\n%s" %(neg_tra_out))
    if args.rep_reg_annot:
        print("Positives repeat region annotations .rra:\n%s" %(pos_rra_out))
        print("Negatives repeat region annotations .rra:\n%s" %(neg_rra_out))
    if args.report:
        print("Training set generation report .html:\n%s" %(html_report_out))
    print("")


################################################################################

def main_gp(args):
    """
    Generate a prediction (or test) set.

    """

    print("Running for you in GP mode ... ")

    # Generate results output folder.
    if not os.path.exists(args.out_folder):
        os.makedirs(args.out_folder)

    """
    Output files.

    """
    test_bed_out = args.out_folder + "/" + "test.bed"
    test_fasta_out = args.out_folder + "/" + "test.fa"
    # Additional feature files.
    test_pc_con_out = args.out_folder + "/" + "test.pc.con"
    test_pp_con_out = args.out_folder + "/" + "test.pp.con"
    test_tra_out = args.out_folder + "/" + "test.tra"
    test_rra_out = args.out_folder + "/" + "test.rra"
    test_eia_out = args.out_folder + "/" + "test.eia"
    test_str_elem_p_out = args.out_folder + "/" + "test.elem_p.str"
    test_bpp_out = args.out_folder + "/" + "test.bpp.str"
    # Mode settings file.
    settings_file = args.out_folder + "/" + "/settings.graphprot2_gp.out"
    # Chromosome or transcript lengths file.
    chr_lengths_file = args.out_folder + "/" + "reference_lengths.out"
    # Genes or transcript regions containing --in sites.
    regions_with_sites_bed = args.out_folder + "/" + "regions_with_sites.bed"
    # Feature table.
    feat_table_out = args.out_folder + "/" + "features.out"
    # Transcript sites mapped to genome.
    test_transcript_to_genome_bed = args.out_folder + "/" + "test_transcript_to_genome.bed"


    """
    Temporary output files.

    """
    # tmp files for processing --in sites.
    tmp_bed = args.out_folder + "/" + "test.tmp.bed"
    # Gene regions from --gtf.
    gene_regions_gtf_tmp_bed = args.out_folder + "/" + "gene_regions.gtf.tmp.bed"
    # Processed --in sites +col5 score = 0.
    processed_in_sites_bed = args.out_folder + "/" + "processed_in_sites_sc0.bed"
    # Extracted site sequences (twoBitToFa output).
    tmp_fasta = args.out_folder + "/" + "test.tmp.fa"

    # Write generated feature infos to table.
    FEATOUT = open(feat_table_out, "w")

    """
    CASE 1: sequences provided as input (--in FASTA)
    ================================================

    --in FASTA given.

    If --in FASTA is set, possible features beside sequence:
    --str and optionally --bp-in (custom base pairs)

    """

    # Check if --in file is FASTA file.
    if gp2lib.fasta_check_fasta_file(args.in_sites):
        # Read in sequences (this also checks for unique headers + converts to RNA).
        print("Processing --in FASTA sequences ... ")
        test_seqs_dic = gp2lib.read_fasta_into_dic(args.in_sites)
        id2vpse_dic = gp2lib.extract_viewpoint_regions_from_fasta(test_seqs_dic,
                                                                  get_se_dic=True)
        # Output sequences.
        gp2lib.fasta_output_dic(test_seqs_dic, test_fasta_out)

        # Looking for lowercase context.
        lc_context = False
        for seq_id in id2vpse_dic:
            seq_l = len(test_seqs_dic[seq_id])
            if id2vpse_dic[seq_id][1] != seq_l or id2vpse_dic[seq_id][0] != 1:
                lc_context = True
                break

        # Sequence character counts.
        seq_cc_dic = gp2lib.seqs_dic_count_chars(test_seqs_dic)
        if lc_context:
            print("Lowercase context regions found inside --in sequences ... ")
            allowed_nt_dic = {'A': 1, 'C': 1, 'G': 1, 'U': 1, 'a': 1, 'c': 1, 'g': 1, 'u': 1}
            for nt in seq_cc_dic:
                if nt not in allowed_nt_dic:
                    assert False, "--in sequences contain \"%s\" character (allowed characters: A C G U a c g u)" %(nt)
            FEATOUT.write("fa\tC\tA,C,G,U,a,c,g,u\t-\n")
            # Get most frequenet lowercase extension.
            major_lcl = gp2lib.get_major_lc_len_from_seqs_dic(test_seqs_dic)
            assert major_lcl, "no lowercase regions extracted (major_lcl evaluated to False)"
            print("Most frequent lowercase region length found:  %i" %(major_lcl))
            print("Set --con-ext to this length ... ")
            # Use major_lcl as con_ext.
            args.con_ext = major_lcl
        else:
            allowed_nt_dic = {'A': 1, 'C': 1, 'G': 1, 'U': 1}
            for nt in seq_cc_dic:
                if nt not in allowed_nt_dic:
                    assert False, "--in sequences contain \"%s\" character (allowed characters: A C G U)" %(nt)
            FEATOUT.write("fa\tC\tA,C,G,U\t-\n")

        # Secondary structure stats dictionaries.
        test_str_stats_dic = None
        if args.add_str or args.bp_in:
            FEATOUT.write("bpp.str\tN\tbp_prob\tprob\n")
            if args.report:
                test_str_stats_dic = {}

        # If structure features should be computed.
        if args.add_str:
            FEATOUT.write("elem_p.str\tN\tp_u,p_e,p_h,p_i,p_m,p_s\tprob\n")
            # Calculate structure features for positives.
            print("Get secondary structure features ... ")
            gp2lib.calc_str_elem_up_bpp(test_fasta_out, test_bpp_out, test_str_elem_p_out,
                                        stats_dic=test_str_stats_dic,
                                        plfold_u=args.plfold_u,
                                        plfold_l=args.plfold_l,
                                        plfold_w=args.plfold_w)
        # If custom base pair information given.
        if args.bp_in:
            # Sanity check, process, and output custom base pair information file.
            print("--bp-in file supplied. Check, process + overwrite calculated base pairs ... ")
            gp2lib.process_custom_bp_file(args.bp_in, test_bpp_out, test_seqs_dic,
                                          stats_dic=test_str_stats_dic)

        # Sequence set stats.
        c_test_out = len(test_seqs_dic)
        if args.report:
            # HTML report output file.
            html_report_out = args.out_folder + "/" + "report.graphprot2_gp.html"
            # Plots subfolder.
            plots_subfolder = "plots_graphprot2_gp"
            # Get library and logo path.
            gp2lib_path = os.path.dirname(gp2lib.__file__)
            # Generate report.
            gp2lib.gp2_gp_generate_html_report(test_seqs_dic, args.out_folder,
                                               "s", gp2lib_path,
                                               id2ucr_dic=id2vpse_dic,
                                               plots_subfolder=plots_subfolder,
                                               html_report_out=html_report_out,
                                               test_str_stats_dic=test_str_stats_dic,
                                               theme=args.theme,
                                               kmer_top=10,
                                               rna=True)

        # Mode settings output file.
        SETOUT = open(settings_file, "w")
        for arg in vars(args):
            SETOUT.write("%s\t%s\n" %(arg, str(getattr(args, arg))))
        SETOUT.close()

        print("# prediction set sequences output: %i" %(c_test_out))
        print("")
        print("PREDICTION SET OUTPUT FILES")
        print("===========================")
        print("Sequences .fa:\n%s" %(test_fasta_out))
        if args.add_str:
            print("Base pair probabilities .bpp.str:\n%s" %(test_bpp_out))
            print("Position-wise structural elements probabilities .elem_p.str:\n%s" %(test_str_elem_p_out))
        if args.bp_in and not args.add_str:
            print("Base pair probabilities .bpp.str (from --bp-in):\n%s" %(test_bpp_out))
        if args.report:
            print("Prediction set generation report .html:\n%s" %(html_report_out))
        print("")
        FEATOUT.close()
        sys.exit()

    """
    CASE 2: genomic or transcript sites given (--in BED)
    ====================================================

    This means:
        - Additional features can be added

    For genomic sites, the features are:
        - Position-wise exon-intron annotation (with options)
        - Position-wise transcript region annotation (with options)
        - Position-wise repeat region annotation
        - Position-wise conservation scores (phasCons and phyloP)
        - Structure features (base pairs and structural elements probabilities)

    For transcript sites, the features are:
        - Position-wise transcript region annotation (with options)
        - Position-wise repeat region annotation
        - Position-wise conservation scores (phasCons and phyloP)
        - Structure features (base pairs and structural elements probabilities)

    NOTE that exon-intron features make no sense for transcript sites, as each
    position would get the exon label (thus omitted for transcript sites)

    """
    # If --in are BED, require --gtf and --gen.
    if not args.in_gtf:
        print("ERROR: --gtf GTF file is required if --in receives BED file")
        sys.exit()
    if not args.in_2bit:
        print("ERROR: --gen .2bit file is required if --in receives BED file")
        sys.exit()
    # Input checks.
    assert os.path.exists(args.in_sites), "--in BED file \"%s\" not found" %(args.in_sites)
    assert os.path.exists(args.in_gtf), "--gtf GTF file \"%s\" not found" %(args.in_gtf)
    assert os.path.exists(args.in_2bit), "--gen .2bit file \"%s\" not found" %(args.in_2bit)
    assert gp2lib.bed_check_six_col_format(args.in_sites), "--in BED file appears to be not in 6-column BED format"
    if args.pc_bw:
        assert os.path.exists(args.pc_bw), "--phastcons file \"%s\" not found" %(args.pc_bw)
    if args.pp_bw:
        assert os.path.exists(args.pp_bw), "--phylop file \"%s\" not found" %(args.pp_bw)
    if args.tr_reg_codon_annot:
        assert args.tr_reg_annot, "--tra-codons requires --tra to be effective"
    if args.tr_reg_border_annot:
        assert args.tr_reg_annot, "--tra-borders requires --tra to be effective"
    if args.intron_border_annot:
        assert args.exon_intron_annot, "--eia-ib requires --eia to be effective"
    if args.exon_intron_n:
        assert args.exon_intron_annot, "--eia-n requires --eia to be effective"

    # Check whether split regions are given and --con-ext.
    if args.con_ext:
        if gp2lib.bed_check_for_part_ids(args.in_sites):
            assert False, "--in BED file column 4 contains part IDs (_e, _p endings, used to merge exons or in general two genomic regions). Please disable --con-ext or change IDs"

    # If --in column 4 IDs should be kept, check their uniqueness.
    if args.keep_ids:
        assert gp2lib.bed_check_unique_ids(args.in_sites), "--in BED file \"%s\" column 4 IDs not unique. Disable --keep-ids or provide unique IDs" %(args.in_sites)

    # Mode settings output file.
    SETOUT = open(settings_file, "w")
    for arg in vars(args):
        SETOUT.write("%s\t%s\n" %(arg, str(getattr(args, arg))))
    SETOUT.close()

    # Get chromsome lengths from --gen.
    print("Get chromosome lengths from --gen ... ")
    chr_len_dic = gp2lib.get_chromosome_lengths_from_2bit(args.in_2bit, chr_lengths_file)
    # Get chromosome IDs.
    print("Get chromosome IDs from --in ... ")
    chr_ids_dic = gp2lib.bed_get_chromosome_ids(args.in_sites)

    # Check whether IDs are genomic or not.
    transcript_regions = False
    for chr_id in chr_ids_dic:
        if chr_id not in chr_len_dic:
            transcript_regions = True
            break
    # Demand pure transcript or genomic sites --in BED.
    if transcript_regions:
        for chr_id in chr_ids_dic:
            assert chr_id not in chr_len_dic, "chromosome and non-chromosome IDs encountered in --in BED column 1. --in column 1 must contain either chromsome or transcript IDs (conflicting IDs: \"%s\", \"%s\")" %(transcript_regions, chr_id)
        print("No chromosome IDs found in --in, interpret column 1 IDs as transcript IDs ... ")

    # If transcript_regions, there should be no genomic col1 IDs.
    tr_seqs_dic = {}
    if transcript_regions:
        # Check for set --exon-intron.
        if args.exon_intron_annot:
            print("ERROR: setting --eia is useless for transcript sites. Please disable or provide genomic input sites.")
            sys.exit()
        if args.intron_border_annot:
            print("ERROR: setting --eia-ib is useless for transcript sites. Please disable or provide genomic input sites.")
            sys.exit()
        if args.exon_intron_n:
            print("ERROR: setting --eia-ib is useless for transcript sites. Please disable or provide genomic input sites.")
            sys.exit()
        # Read in transcript lengths from --gtf.
        print("Get transcript lengths from --gtf ... ")
        tr_len_dic = gp2lib.gtf_get_transcript_lengths(args.in_gtf)
        print("Verifying transcript IDs by checking their presence in --gtf ... ")
        for tr_id in chr_ids_dic:
            assert tr_id in tr_len_dic, "--in column 1 transcript ID \"%s\" not found in --gtf" %(tr_id)
        # Make a transcript regions .bed file.
        print("Output transcripts containing test sites to BED ... ")
        gp2lib.bed_sequence_lengths_to_bed(tr_len_dic, regions_with_sites_bed,
                                           ids_dic=chr_ids_dic)

        # Get spliced transcript sequences from gtf+2bit.
        print("Get transcript sequences from --gtf and --gen ... ")
        tr_seqs_dic = gp2lib.get_transcript_sequences_from_gtf(args.in_gtf, args.in_2bit,
                                                               lc_repeats=args.rep_reg_annot,
                                                               tr_ids_dic=chr_ids_dic)
        # Output transcript lengths file.
        print("Output transcript lengths file ... ")
        gp2lib.output_chromosome_lengths_file(tr_len_dic, chr_lengths_file)
        # Make a new chr_len_dic, only with transcript IDs from --in and their lengths.
        chr_len_dic = {}
        for tr_id in chr_ids_dic:
            tr_len = tr_len_dic[tr_id]
            chr_len_dic[tr_id] = tr_len
    else:
        # Genomic sites checks.
        if args.tr_reg_border_annot:
            print("ERROR: setting --tra-borders only supported for transcript sites. Please disable or provide transcript input sites.")
            sys.exit()
        # Check for genomic region _e or _p (split) IDs (exon regions).
        if args.exon_intron_annot:
            print("Genomic regions and --eia enabled: check for split IDs ... ")
            in_reg_ids_dic = gp2lib.bed_get_region_ids(args.in_sites)
            core_ids_dic = gp2lib.get_core_id_to_part_counts_dic(in_reg_ids_dic)
            for core_id in core_ids_dic:
                assert core_ids_dic[core_id] == 1, "--eia enabled but genomic split IDs (== exon regions) found. Disable --eia or use transcript regions"
    """
    Process --in sites
    ==================

    Filter --in sites based on:
    Reference ID (only standard chromosome or transcript IDs allowed)
    Sites with gene overlap (--gene-filter)

    """

    # Process --in positives.
    count_stats_dic = {}
    # Lowercase uppercase lowercase part lengths.
    id2pl_dic = {}

    id2row_dic = gp2lib.process_test_sites(args.in_sites, tmp_bed, chr_len_dic,
                                           id2pl_dic, args,
                                           transcript_regions=transcript_regions,
                                           count_dic=count_stats_dic,
                                           id_prefix="TEST")
    print("# sites found in --in:               %i" %(count_stats_dic['c_in']))
    if count_stats_dic['c_filt_ref']:
        print("# sites removed (reference ID):      %i" %(count_stats_dic['c_filt_ref']))

    assert count_stats_dic['c_out'], "no sites remaining after filtering ... "
    print("# sites remaining (post-filtering):  %i" %(count_stats_dic['c_out']))

    # For genomic sites, extract gene regions containing positives.
    if not transcript_regions:
        # Get gene regions from .gtf.
        print("Extract gene regions from --gtf ... ")
        gp2lib.gtf_extract_gene_bed(args.in_gtf, gene_regions_gtf_tmp_bed)
        # Get gene regions containing positives.
        params = "-u -s"
        print("Get gene regions containing --in sites ... ")
        gp2lib.intersect_bed_files(gene_regions_gtf_tmp_bed, args.in_sites, params, regions_with_sites_bed)
        # Filter positives by gene coverage.
        if not args.gene_filter:
            gp2lib.make_file_copy(tmp_bed, processed_in_sites_bed)
        else:
            # Accept only --in sites that overlap with gene regions.
            print("Filter --in sites by gene overlap ... ")
            params = "-u -s"
            gp2lib.intersect_bed_files(tmp_bed, gene_regions_gtf_tmp_bed, params, processed_in_sites_bed)
            c_out = gp2lib.count_file_rows(processed_in_sites_bed)
            assert c_out, "no remaining --in sites after filtering by gene overlap"
            print("# --in sites after filtering by gene coverage:  %i" %(c_out))
    else:
        gp2lib.make_file_copy(tmp_bed, processed_in_sites_bed)

    """
    Extract site sequences + filter
    ===============================

    - For transcript sites, extract from sequence dictionary.
    - For genomic sites, extract from --gen in_2bit.
    - For genomic sites, delete sites with lengths != extracted sequence lengths.
    - Remove N containing sequences / sites.

    """

    # Get region lengths.
    test_bed_len_dic = gp2lib.bed_get_region_lengths(processed_in_sites_bed)

    if transcript_regions:
        # Get sequences for transcript sites.
        print("Extract sequences for transcript sites ... ")
        test_seqs_dic = gp2lib.extract_transcript_sequences(id2row_dic, tr_seqs_dic)
    else:
        # Get sequences for genomic sites.
        print("Extract sequences for genomic sites ... ")
        # Extract positive sequences from .2bit.
        gp2lib.bed_extract_sequences_from_2bit(processed_in_sites_bed, tmp_fasta, args.in_2bit,
                                               lc_repeats=args.rep_reg_annot)
        # Read in sequences.
        test_seqs_dic = gp2lib.read_fasta_into_dic(tmp_fasta)

    # Compare region lengths with sequence lengths, delete mismatches.
    print("Compare extracted sequence lengths to site lengths ... ")

    # IDs to delete lists.
    test_del_list = []
    for seq_id in test_seqs_dic:
        seq_l = len(test_seqs_dic[seq_id])
        reg_l = test_bed_len_dic[seq_id]
        if seq_l != reg_l:
            test_del_list.append(seq_id)

    # Delete mismatches.
    for seq_id in test_del_list:
        del test_seqs_dic[seq_id]
    # Deleted and remaining counts.
    c_test_del = len(test_del_list)
    c_test_rem = len(test_seqs_dic)

    assert test_seqs_dic, "no sites remaining after sequence vs region length comparison"
    if c_test_del:
        print("# sites removed due to region vs sequence length difference:  %i" %(c_test_del))

    # Remove N containing sequences.
    print("Remove N containing sequences ... ")
    test_del_list = []
    neg_del_list = []
    for seq_id in test_seqs_dic:
        if re.search("N", test_seqs_dic[seq_id], re.I):
            test_del_list.append(seq_id)

    # Delete mismatches.
    for seq_id in test_del_list:
        del test_seqs_dic[seq_id]

    # Check / summarize.
    c_test_del = len(test_del_list)
    if c_test_del:
        print("# N containing sites removed:  %i" %(c_test_del))
    assert test_seqs_dic, "no sites remaining after removing N containing ones"

    """
    Create lowercase sequence context
    =================================

    - Make upstream + downstream sequence context lowercase.
      Upstream lowercase, uppercase, and downstream lowercase region lengths
      for each site ID are stored in id2pl_dic.
      For negatives, use new2oldid_dic for new random negative ID mapping
      to old associated positive ID, from which to get uppercase+lowercase
      lengths.
    - Output sequences + sites to finish FASTA+BED extraction part.

    """

    # Dictionaries for storing original lowercase+uppercase information (if --rep-reg).
    test_seqs_rep_lc_dic = {}
    if args.rep_reg_annot:
        # Store lowercase info in dics, then make sequences uppercase again.
        for seq_id in test_seqs_dic:
            seq = test_seqs_dic[seq_id]
            test_seqs_rep_lc_dic[seq_id] = seq
            test_seqs_dic[seq_id] = seq.upper()

    # Make sequences lowercase-uppercase-lowercase.
    id2ucr_dic = {}
    if args.con_ext:
        print("Make site sequence context (--con-ext) lowercase ... ")
        # Process positive sequences.
        for seq_id in test_seqs_dic:
            assert seq_id in id2pl_dic, "site ID \"%s\" missing in id2pl_dic" %(seq_id)
            uslc_len = id2pl_dic[seq_id][0]
            uc_len = id2pl_dic[seq_id][1]
            dslc_len = id2pl_dic[seq_id][2]
            seq = test_seqs_dic[seq_id]
            uc_s = uslc_len + 1
            uc_e = uslc_len + uc_len
            new_seq = gp2lib.add_lowercase_context_to_sequences(seq, uc_s, uc_e,
                                                      convert_to_rna=True)
            test_seqs_dic[seq_id] = new_seq
            id2ucr_dic[seq_id] = [uc_s, uc_e]

    # Length statistics for final positives + negatives.
    final_len_list = []
    final_len_list = gp2lib.get_seq_len_list_from_dic(test_seqs_dic)
    c_final_test = len(final_len_list)

    # Output full-length (+ context) sequences.
    print("Output selected sites to BED and FASTA ... ")
    gp2lib.fasta_output_dic(test_seqs_dic, test_fasta_out)

    # Output BED regions.
    gp2lib.bed_write_row_dic_into_file(id2row_dic, test_bed_out,
                                       id2out_dic=test_seqs_dic)
    print("# prediction sites output:  %i" %(c_final_test))

    """
    Calculate additional features
    =============================

    Currently these are:
    structure (for sequence, genomic regions, and transcript regions)
    transcript region annotation (for genomic and transcript regions)
    exon-intron annotation (for genomic regions)
    conservation scores (for genomic and transcript regions)
    repeat region information (for genomic and transcript regions)

    1) structure (--str):
    To calculate probabilities of base pairs + structural elements.
    .bpp.str: base pairs + probabilities file
    .str_elem.str: unpaired + structural elements probabilities
    Note that structure calculation yields same results for uppercase
    and lowercase sequences (mixed ones too), so no separate upper-
    lowercase sequence files necessary.

    2) conservation scores (--phastcons, --phylop):
    Position-wise phyloP and / or phastCons scores to add for each
    site position.
    .pc.con : phastCons position-wise conservation scores file
    .pp.con : phyloP position-wise conservation scores file

    3) transcript region annotation (--tra, --tra-codons, --tra-borders)
    Add transcript region label annotations from --gtf to genomic or
    transcript sites.
    --tra: add 5'UTR (F), CDS (C), and 3'UTR (T) annotation to transcript
    or genomic sites.
    --tra-codons: add start (S) and stop (E) codon annotation to transcript
    or genomic sites.
    --tra-borders: add transcript start (A), transcript end (Z) nt, and
    exon border (B) annotation to transcript sites.

    4) exon-intron annotation (--eia, --eia-ib, --eia-n):
    For genomic sites only.
    Position-wise exon + intron annotation, for each genomic site position
    .eia : stores E or I labels for each site position
    If --eia-n, additional N label for regions not overlapping with
    transcripts.
    If --eia-ib, intron border labels (F: 5' intron start, T: 3' intron end)

    5) repeat region annotation (--rra)
    Add position-wise R (repeat) or N (no repeat) annotations to label
    repeat and non-repeat region sequences.
    Repeat region information is stored in --gen .2bit, with lower-case
    nucleotides belonging to repeat regions annotated by RepeatMasker and
    Tandem Repeats Finder (with period of 12 or less).
    """

    # Additional feature stats dictionaries.
    test_str_stats_dic = None
    test_phastcons_stats_dic = None
    test_phylop_stats_dic = None
    test_eia_stats_dic = None
    test_tra_stats_dic = None
    test_rra_stats_dic = None
    if args.report and (args.add_str or args.bp_in):
        test_str_stats_dic = {}
    if args.report and args.pc_bw:
        test_phastcons_stats_dic = {}
    if args.report and args.pp_bw:
        test_phylop_stats_dic = {}
    if args.report and args.exon_intron_annot:
        test_eia_stats_dic = {}
    if args.report and args.tr_reg_annot:
        test_tra_stats_dic = {}
    if args.report and args.rep_reg_annot:
        test_rra_stats_dic = {}

    # Store feature infos.
    if args.con_ext:
        FEATOUT.write("fa\tC\tA,C,G,U,a,c,g,u\t-\n")
    else:
        FEATOUT.write("fa\tC\tA,C,G,U\t-\n")
    if args.add_str:
        FEATOUT.write("elem_p.str\tN\tp_u,p_e,p_h,p_i,p_m,p_s\tprob\n")
    if args.add_str or args.bp_in:
        FEATOUT.write("bpp.str\tN\tbp_prob\tprob\n")
    if args.pc_bw:
        FEATOUT.write("pc.con\tN\tphastcons_score\tprob\n")
    if args.pp_bw:
        FEATOUT.write("pp.con\tN\tphylop_score\tminmax2\n")
    if args.tr_reg_annot:
        ll = ["F", "C", "N", "T"] # label list.
        if args.tr_reg_border_annot:
            ll.append("A")
            ll.append("Z")
            ll.append("B")
        if args.tr_reg_codon_annot:
            ll.append("S")
            ll.append("E")
        ll.sort()
        lls = ",".join(ll)
        FEATOUT.write("tra\tC\t%s\t-\n" %(lls))
    if args.exon_intron_annot:
        ll = ["E", "I"]
        if args.intron_border_annot:
            ll.append("T")
            ll.append("F")
        if args.exon_intron_n:
            ll.append("N")
        ll.sort()
        lls = ",".join(ll)
        FEATOUT.write("eia\tC\t%s\t-\n" %(lls))
    if args.rep_reg_annot:
        FEATOUT.write("rra\tC\tN,R\t-\n")
    FEATOUT.close()

    """
    1) Secondary structure features.

    """
    # Secondary structure stats dictionaries.

    if args.add_str:

        # Calculate structure features for prediction set.
        #print("Get secondary structure information ... ")
        #gp2lib.calc_str_elem_up_bpp(test_fasta_out, test_bpp_out, test_str_elem_p_out,
        #                            stats_dic=test_str_stats_dic,
        #                            plfold_u=args.plfold_u,
        #                            plfold_l=args.plfold_l,
        #                            plfold_w=args.plfold_w)
        # Calculate structure features for positives.
        print("Get secondary structure information ... ")
        gp2lib.calc_ext_str_features(id2row_dic, chr_len_dic,
                                     test_bpp_out, test_str_elem_p_out, args,
                                     id2ucr_dic=id2ucr_dic,
                                     stats_dic=test_str_stats_dic,
                                     bp_check_seqs_dic=test_seqs_dic,
                                     tr_regions=transcript_regions,
                                     tr_seqs_dic=tr_seqs_dic)

    # If custom base pair information given.
    if args.bp_in:
        # Sanity check, process, and output custom base pair information file.
        print("--bp-in file supplied. Check, process + overwrite calculated base pairs ... ")
        gp2lib.process_custom_bp_file(args.bp_in, test_bpp_out, test_seqs_dic,
                                      stats_dic=test_str_stats_dic)

    """
    2) Conservation scores.

    """
    # More preprocessing if transcript_regions.
    if transcript_regions:
        if args.pc_bw or args.pp_bw or args.tr_reg_annot:
            print("Additional annotations for transcript sites require mapping to genome ... ")
            print("Mapping transcript sites to genome ... ")
            id2hitc_dic = {}
            gp2lib.bed_convert_transcript_to_genomic_sites(test_bed_out, args.in_gtf,
                                                           test_transcript_to_genome_bed,
                                                           site2hitc_dic=id2hitc_dic)

    # If phastCons conservation scores (--pc-bw) are given.
    if args.pc_bw:
        if transcript_regions:
            print("Extracting phastCons conservation scores for transcript sites ... ")
            # For transcript sites.
            gp2lib.extract_conservation_scores(test_transcript_to_genome_bed,
                                               test_pc_con_out, args.pc_bw,
                                               stats_dic=test_phastcons_stats_dic,
                                               merge_split_regions=True,
                                               report=False)
        else:
            print("Extracting phastCons conservation scores for genomic sites ... ")
            # For genomic sites.
            gp2lib.extract_conservation_scores(test_bed_out,
                                               test_pc_con_out, args.pc_bw,
                                               stats_dic=test_phastcons_stats_dic,
                                               merge_split_regions=True,
                                               report=False)

    # If phastCons conservation scores (--pp-bw) are given.
    if args.pp_bw:
        if transcript_regions:
            print("Extracting phyloP conservation scores for transcript sites ... ")
            # For transcript sites.
            gp2lib.extract_conservation_scores(test_transcript_to_genome_bed,
                                               test_pp_con_out, args.pp_bw,
                                               stats_dic=test_phylop_stats_dic,
                                               merge_split_regions=True,
                                               report=False)
        else:
            print("Extracting phyloP conservation scores for genomic sites ... ")
            # For genomic sites.
            gp2lib.extract_conservation_scores(test_bed_out,
                                               test_pp_con_out, args.pp_bw,
                                               stats_dic=test_phylop_stats_dic,
                                               merge_split_regions=True,
                                               report=False)
        print("Normalizing phyloP scores ... ")
        gp2lib.phylop_norm_test_scores(test_pp_con_out)

    """
    3) Transcript region annotations for transcript sites.

    """
    if args.tr_reg_annot and transcript_regions:
        print("Extracting transcript region annotations for transcript sites ... ")
        if args.tr_reg_codon_annot:
            print("Start + stop codon annotations enabled ... ")
        if args.tr_reg_border_annot:
            print("Transcript + exon border annotations enabled ... ")
        tr_ids_dic = chr_len_dic
        gp2lib.bed_get_transcript_annotations_from_gtf(tr_ids_dic,
                                            test_transcript_to_genome_bed,
                                            args.in_gtf, test_tra_out,
                                            stats_dic=test_tra_stats_dic,
                                            codon_annot=args.tr_reg_codon_annot,
                                            border_annot=args.tr_reg_border_annot,
                                            merge_split_regions=True)

    """
    4) Exon-intron + transcript region annotations for genomic sites.

    """
    if (args.exon_intron_annot or args.tr_reg_annot) and not transcript_regions:
        print("Get transcript annotations for genomic sites ... ")
        # Get transcript IDs to use for exon-intron or region annotation.
        tr_ids_dic = {}
        if args.tr_list:
            print("Using transcript list from --tr-list to define transcript regions ... ")
            # Read in transcript IDs from --tr-list.
            tr_ids_dic = gp2lib.read_ids_into_dic(args.tr_list)
            # Read in all transcript IDs in --gtf.
            all_tr_ids_dic = gp2lib.gtf_get_transcript_ids(args.in_gtf)
            # Check if given --tr list IDs are in --gtf.
            for tr_id in tr_ids_dic:
                if tr_id not in all_tr_ids_dic:
                    print("WARNING: transcript ID \"%s\" from --tr-list not found in --gtf. Ignoring transcript ... " %(tr_id))
        else:
            # Create exon BED file from most prominent transcripts.
            strict=False
            if strict:
                print("Get most prominent transcript for each gene from --gtf (TSL1-5 only)... ")
            else:
                print("Get most prominent transcript for each gene from --gtf ... ")
            # Don't be strict (!= only TSL1-5 allowed).
            tr_ids_dic = gp2lib.gtf_extract_most_prominent_transcripts(args.in_gtf,
                                            "dummy",
                                            strict=strict,
                                            return_ids_dic=tr_ids_dic)

        # Exon-intron annotations for genomic sites.
        if args.exon_intron_annot:
            print("Extract exon-intron annotations for genomic sites ... ")
            if args.tr_list:
                print("Use exon regions of --tr-list transcripts from --gtf ... ")
            else:
                print("Use exon regions of most prominent transcripts from --gtf ... ")
            if args.intron_border_annot:
                print("Intron border annotation enabled ... ")
            if args.exon_intron_n:
                print("Non-intron non-exon region annotation enabled ... ")
            gp2lib.bed_get_exon_intron_annotations_from_gtf(tr_ids_dic, test_bed_out,
                                        args.in_gtf, test_eia_out,
                                        stats_dic=test_eia_stats_dic,
                                        n_labels=args.exon_intron_n,
                                        intron_border_labels=args.intron_border_annot)

        # Transcript region annotations for genomic sites.
        if args.tr_reg_annot:
            print("Extracting transcript region annotations for genomic sites ... ")
            if args.tr_reg_codon_annot:
                print("Start + stop codon annotations enabled ... ")
            gp2lib.bed_get_transcript_annotations_from_gtf(tr_ids_dic,
                                                test_bed_out,
                                                args.in_gtf, test_tra_out,
                                                stats_dic=test_tra_stats_dic,
                                                codon_annot=args.tr_reg_codon_annot,
                                                border_annot=args.tr_reg_border_annot,
                                                merge_split_regions=True)

    """
    5) Repeat region annotations for genomic and transcript sites.

    """
    if args.rep_reg_annot:
        print("Extracting repeat region annotations ... ")
        gp2lib.fasta_get_repeat_region_annotations(test_seqs_rep_lc_dic, test_rra_out,
                                                   stats_dic=test_rra_stats_dic)

    """
    Generate HTML report including various training dataset statistics.

    """
    dataset_type =  "g"
    if transcript_regions:
        dataset_type =  "t"

    if args.report:
        # Gene biotype statistics for target genes.
        gbt_stats = True
        # All gene biotype counts.
        all_gbtc_dic = {}
        # Target gene biotype counts.
        target_gbtc_dic = {}
        # Target region to hit count (transcript or gene ID -> hit count).
        t2hc_dic = {}
        # Target region to info.
        t2i_dic = {}
        if gbt_stats:
            # Calculate additional stats for report.
            print("Extract additional statistics for report ... ")
            # Get count stats for gene biotypes.
            region_ids_dic = gp2lib.bed_get_region_ids(regions_with_sites_bed)
            # Get gene biotype counts for target genes.
            if transcript_regions:
                target_gbtc_dic = gp2lib.gtf_get_gene_biotypes_from_transcript_ids(
                                            region_ids_dic, args.in_gtf,
                                            all_gbtc_dic=all_gbtc_dic)

                # Get transcript ID to hit count dictionary.
                t2hc_dic = gp2lib.bed_get_chromosome_ids(test_bed_out)
                # Get transcript infos.
                t2i_dic = gp2lib.gtf_get_transcript_infos(region_ids_dic, args.in_gtf)
            else:
                target_gbtc_dic = gp2lib.gtf_get_gene_biotypes(
                                            region_ids_dic, args.in_gtf,
                                            all_gbtc_dic=all_gbtc_dic)
                # Get overlapping site counts for each gene.
                t2hc_dic = gp2lib.bed_intersect_count_region_overlaps(gene_regions_gtf_tmp_bed, test_bed_out)
                # Get gene infos.
                t2i_dic = gp2lib.gtf_get_gene_infos(region_ids_dic, args.in_gtf)

        # HTML report output file.
        html_report_out = args.out_folder + "/" + "report.graphprot2_gp.html"
        # Plots subfolder.
        plots_subfolder = "plots_graphprot2_gp"
        # Get library and logo path.
        gp2lib_path = os.path.dirname(gp2lib.__file__)
        # Generate report.
        gp2lib.gp2_gp_generate_html_report(test_seqs_dic,
                                           args.out_folder, dataset_type, gp2lib_path,
                                           html_report_out=html_report_out,
                                           id2ucr_dic=id2ucr_dic,
                                           plots_subfolder=plots_subfolder,
                                           test_str_stats_dic=test_str_stats_dic,
                                           test_phastcons_stats_dic=test_phastcons_stats_dic,
                                           test_phylop_stats_dic=test_phylop_stats_dic,
                                           test_eia_stats_dic=test_eia_stats_dic,
                                           test_tra_stats_dic=test_tra_stats_dic,
                                           test_rra_stats_dic=test_rra_stats_dic,
                                           target_gbtc_dic=target_gbtc_dic,
                                           all_gbtc_dic=all_gbtc_dic,
                                           t2hc_dic=t2hc_dic,
                                           t2i_dic=t2i_dic,
                                           kmer_top=10,
                                           target_top=20,
                                           theme=args.theme,
                                           rna=True)

    """
    Take out the trash.

    """
    clean_up = True
    if clean_up:
        print("Removing temporary files ... ")
        # Remove tmp files.
        if os.path.exists(tmp_bed):
            os.remove(tmp_bed)
        if os.path.exists(gene_regions_gtf_tmp_bed):
            os.remove(gene_regions_gtf_tmp_bed)
        if os.path.exists(processed_in_sites_bed):
            os.remove(processed_in_sites_bed)
        if os.path.exists(tmp_fasta):
            os.remove(tmp_fasta)
        if os.path.exists(test_transcript_to_genome_bed):
            os.remove(test_transcript_to_genome_bed)

    """
    Output file summary.

    """
    print("")
    print("PREDICTION SET OUTPUT FILES")
    print("===========================")
    print("Sequences .fa:\n%s" %(test_fasta_out))
    print("Sites .bed:\n%s" %(test_bed_out))
    if args.add_str:
        print("Base pair probabilities .bpp.str:\n%s" %(test_bpp_out))
        print("Position-wise structural elements probabilities .elem_p.str:\n%s" %(test_str_elem_p_out))
    if args.bp_in and not args.add_str:
        print("Base pair probabilities .bpp.str (from --bp-in):\n%s" %(test_bpp_out))
    if args.pc_bw:
        print("phastCons conservation scores .pc.con:\n%s" %(test_pc_con_out))
    if args.pp_bw:
        print("phyloP conservation scores .pp.con:\n%s" %(test_pp_con_out))
    if args.exon_intron_annot:
        print("Exon intron region annotations .eia:\n%s" %(test_eia_out))
    if args.tr_reg_annot:
        print("Transcript region annotations .tra:\n%s" %(test_tra_out))
    if args.rep_reg_annot:
        print("Repeat region annotations .rra:\n%s" %(test_rra_out))
    if args.report:
        print("Prediction set generation report .html:\n%s" %(html_report_out))
    print("")


################################################################################

if __name__ == '__main__':
    # Setup argparse.
    parser = setup_argument_parser()
    # Print help if no parameter is set.
    if len(sys.argv) < 2:
        parser.print_help()
        sys.exit()
    # Read in command line arguments.
    args = parser.parse_args()

    # Show some banner.
    print("")
    print("    ______                 __    ____             __   ")
    print("   / ____/________ _____  / /_  / __ \_________  / /_ 2")
    print("  / / __/ ___/ __ `/ __ \/ __ \/ /_/ / ___/ __ \/ __/  ")
    print(" / /_/ / /  / /_/ / /_/ / / / / ____/ /  / /_/ / /_    ")
    print(" \____/_/   \__,_/ .___/_/ /_/_/   /_/   \____/\__/    ")
    print("                /_/                                    ")
    print("")
    print("       \"Of all the RBP-BSP tools in the world, ")
    print("           this is definitely one of them.\"   ")
    print("")

    # Are my tools ready?
    assert gp2lib.is_tool("bedtools"), "bedtools not in PATH"
    assert gp2lib.is_tool("twoBitToFa"), "twoBitToFa not in PATH"
    assert gp2lib.is_tool("twoBitInfo"), "twoBitInfo not in PATH"
    assert gp2lib.is_tool("bigWigAverageOverBed"), "bigWigAverageOverBed not in PATH"

    # Run selected mode.
    if args.which == 'train':
        main_train(args)
    elif args.which == 'eval':
        main_eval(args)
    elif args.which == 'predict':
        main_predict(args)
    elif args.which == 'gt':
        main_gt(args)
    elif args.which == 'gp':
        main_gp(args)
